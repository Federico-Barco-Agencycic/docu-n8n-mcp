[
  {
    "node_type": "nodes-langchain.agent",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "AI Agent",
    "description": "Generates an action plan and executes it. Can use external tools.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "2",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: AI Agent node documentation\ndescription: Learn how to use the AI Agent node in n8n. Follow technical documentation to integrate AI Agent node into your workflows.\ncontentType: [integration, reference]\npriority: critical\n---\n\n# AI Agent node\n\nAn [AI agent](/glossary.md#ai-agent) is an autonomous system that receives data, makes rational decisions, and acts within its environment to achieve specific goals. The AI agent's environment is everything the agent can access that isn't the agent itself. This agent uses external [tools](/glossary.md#ai-tool) and APIs to perform actions and retrieve information. It can understand the capabilities of different tools and determine which tool to use depending on the task. \n\n/// note | Connect a tool\nYou must connect at least one tool [sub-node](/integrations/builtin/cluster-nodes/sub-nodes/index.md) to an AI Agent node.\n///\n\n/// note | Agent type\nPrior to version 1.82.0, the AI Agent had a setting for working as different agent types. This has now been removed and all AI Agent nodes work as a `Tools Agent` which was the recommended and most frequently used setting. If you're working with older versions of the AI Agent in workflows or templates, as long as they were set to 'Tools Agent', they should continue to behave as intended with the updated node.\n///\n\n\n## Templates and examples\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'agent') ]]\n\n## Related resources\n\nRefer to [LangChain's documentation on agents](https://js.langchain.com/docs/concepts/agents/){:target=_blank .external-link} for more information about the service.\n\nNew to AI Agents? Read the [n8n blog introduction to AI agents](https://blog.n8n.io/ai-agents/){:target=_blank .external-link}.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common errors or issues and suggested resolution steps, refer to [Common Issues](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for agents with our quick <a href=\\\"https://docs.n8n.io/advanced-ai/intro-tutorial/\\\" target=\\\"_blank\\\">tutorial</a> or see an <a href=\\\"/workflows/templates/1954\\\" target=\\\"_blank\\\">example</a> of how this node works\",\n    \"name\": \"aiAgentStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Source for Prompt (User Message)\",\n    \"name\": \"promptType\",\n    \"type\": \"options\",\n    \"default\": \"auto\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"auto\",\n        \"description\": \"Looks for an input field called 'chatInput' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"define\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"auto\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"define\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Require Specific Output Format\",\n    \"name\": \"hasOutputParser\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Connect an <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_outputParser'>output parser</a> on the canvas to specify the output format you require\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"hasOutputParser\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"System Message\",\n        \"name\": \"systemMessage\",\n        \"type\": \"string\",\n        \"default\": \"You are a helpful assistant\",\n        \"description\": \"The message that will be sent to the agent before the conversation starts\",\n        \"typeOptions\": {\n          \"rows\": 6\n        }\n      },\n      {\n        \"displayName\": \"Max Iterations\",\n        \"name\": \"maxIterations\",\n        \"type\": \"number\",\n        \"default\": 10,\n        \"description\": \"The maximum number of iterations the agent will run before stopping\"\n      },\n      {\n        \"displayName\": \"Return Intermediate Steps\",\n        \"name\": \"returnIntermediateSteps\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether or not the output should include intermediate steps the agent took\"\n      },\n      {\n        \"displayName\": \"Automatically Passthrough Binary Images\",\n        \"name\": \"passthroughBinaryImages\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether or not binary images should be automatically passed through to the agent as image type messages\"\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 1,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.chainLlm",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Basic LLM Chain",
    "description": "A simple chain to prompt a large language model",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.7",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Basic LLM Chain node documentation\ndescription: Learn how to use the Basic LLM Chain node in n8n. Follow technical documentation to integrate Basic LLM Chain node into your workflows.\ncontentType: [integration, reference]\npriority: critical\n---\n\n# Basic LLM Chain node\n\nUse the Basic LLM Chain node to set the prompt that the model will use along with setting an optional parser for the response.\n\nOn this page, you'll find the node parameters for the Basic LLM Chain node and links to more resources.\n\n/// note | Examples and templates\nFor usage examples and templates to help you get started, refer to n8n's [Basic LLM Chain integrations](https://n8n.io/integrations/basic-llm-chain/){:target=_blank .external-link} page.\n///\t\n\n## Node parameters\n\n### Prompt\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/prompt.md\"\n\n### Require Specific Output Format\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/output-format.md\"\n\n## Chat Messages\n\nUse **Chat Messages** when you're using a chat model to set a message.\n\nn8n ignores these options if you don't connect a chat model. Select the **Type Name or ID** you want the node to use:\n\n#### AI\n\nEnter a sample expected response in the **Message** field. The model will try to respond in the same way in its messages.\n\n#### System\n\nEnter a system **Message** to include with the user input to help guide the model in what it should do.\n\nUse this option for things like defining tone, for example: `Always respond talking like a pirate`.\n\n#### User\n\nEnter a sample user input. Using this with the AI option can help improve the output of the agent. Using both together provides a sample of an input and expected response (the **AI Message**) for the model to follow.\n\nSelect one of these input types:\n\n* **Text**: Enter a sample user input as a text **Message**.\n* **Image (Binary)**: Select a binary input from a previous node. Enter the **Image Data Field Name** to identify which binary field from the previous node contains the image data.\n* **Image (URL)**: Use this option to feed an image in from a URL. Enter the **Image URL**.\n\nFor both the **Image** types, select the **Image Details** to control how the model processes the image and generates its textual understanding. Choose from:\n\n* **Auto**: The model uses the auto setting, which looks at the image input size and decide if it should use the Low or High setting.\n* **Low**: The model receives a low-resolution 512px x 512px version of the image and represents the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens. Use this option for use cases that don't require high detail.\n* **High**: The model can access the low-resolution image and then creates detailed crops of input images as 512px squares based on the input image size. Each of the detailed crops uses twice the token budget (65 tokens) for a total of 129 tokens. Use this option for use cases that require high detail.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'basic-llm-chain') ]]\n\n## Related resources\n\nRefer to [LangChain's documentation on Basic LLM Chains](https://js.langchain.com/docs/tutorials/llm_chain/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n\n## Common issues\n\nHere are some common errors and issues with the Basic LLM Chain node and steps to resolve or troubleshoot them.\n\n### No prompt specified error\n\nThis error displays when the **Prompt** is empty or invalid.\n\nYou might see this error in one of two scenarios:\n\n1. When you've set the **Prompt** to **Define below** and haven't entered anything in the **Text** field.\n    * To resolve, enter a valid prompt in the **Text** field.\n2. When you've set the **Prompt** to **Connected Chat Trigger Node** and the incoming data has no field called `chatInput`. \n    * The node expects the `chatInput` field. If your previous node doesn't have this field, add an [Edit Fields (Set)](/integrations/builtin/core-nodes/n8n-nodes-base.set.md) node to edit an incoming field name to `chatInput`.\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Save time with an <a href=\\\"/templates/1978\\\" target=\\\"_blank\\\">example</a> of how this node works\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.input }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chat_input }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1,\n          1.2\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.3\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Source for Prompt (User Message)\",\n    \"name\": \"promptType\",\n    \"type\": \"options\",\n    \"default\": \"auto\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"auto\",\n        \"description\": \"Looks for an input field called 'chatInput' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"define\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          1,\n          1.1,\n          1.2,\n          1.3\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"auto\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.5\n            }\n          }\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"define\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Require Specific Output Format\",\n    \"name\": \"hasOutputParser\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          1,\n          1.1,\n          1.3\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Chat Messages (if Using a Chat Model)\",\n    \"name\": \"messages\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"name\": \"messageValues\",\n        \"displayName\": \"Prompt\",\n        \"values\": [\n          {\n            \"displayName\": \"Type Name or ID\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"AI\",\n                \"value\": \"AIMessagePromptTemplate\"\n              },\n              {\n                \"name\": \"System\",\n                \"value\": \"SystemMessagePromptTemplate\"\n              },\n              {\n                \"name\": \"User\",\n                \"value\": \"HumanMessagePromptTemplate\"\n              }\n            ],\n            \"default\": \"SystemMessagePromptTemplate\"\n          },\n          {\n            \"displayName\": \"Message Type\",\n            \"name\": \"messageType\",\n            \"type\": \"options\",\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"HumanMessagePromptTemplate\"\n                ]\n              }\n            },\n            \"options\": [\n              {\n                \"name\": \"Text\",\n                \"value\": \"text\",\n                \"description\": \"Simple text message\"\n              },\n              {\n                \"name\": \"Image (Binary)\",\n                \"value\": \"imageBinary\",\n                \"description\": \"Process the binary input from the previous node\"\n              },\n              {\n                \"name\": \"Image (URL)\",\n                \"value\": \"imageUrl\",\n                \"description\": \"Process the image from the specified URL\"\n              }\n            ],\n            \"default\": \"text\"\n          },\n          {\n            \"displayName\": \"Image Data Field Name\",\n            \"name\": \"binaryImageDataKey\",\n            \"type\": \"string\",\n            \"default\": \"data\",\n            \"required\": true,\n            \"description\": \"The name of the field in the chain's input that contains the binary image file to be processed\",\n            \"displayOptions\": {\n              \"show\": {\n                \"messageType\": [\n                  \"imageBinary\"\n                ]\n              }\n            }\n          },\n          {\n            \"displayName\": \"Image URL\",\n            \"name\": \"imageUrl\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"required\": true,\n            \"description\": \"URL to the image to be processed\",\n            \"displayOptions\": {\n              \"show\": {\n                \"messageType\": [\n                  \"imageUrl\"\n                ]\n              }\n            }\n          },\n          {\n            \"displayName\": \"Image Details\",\n            \"description\": \"Control how the model processes the image and generates its textual understanding\",\n            \"name\": \"imageDetail\",\n            \"type\": \"options\",\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"HumanMessagePromptTemplate\"\n                ],\n                \"messageType\": [\n                  \"imageBinary\",\n                  \"imageUrl\"\n                ]\n              }\n            },\n            \"options\": [\n              {\n                \"name\": \"Auto\",\n                \"value\": \"auto\",\n                \"description\": \"Model will use the auto setting which will look at the image input size and decide if it should use the low or high setting\"\n              },\n              {\n                \"name\": \"Low\",\n                \"value\": \"low\",\n                \"description\": \"The model will receive a low-res 512px x 512px version of the image, and represent the image with a budget of 65 tokens. This allows the API to return faster responses and consume fewer input tokens for use cases that do not require high detail.\"\n              },\n              {\n                \"name\": \"High\",\n                \"value\": \"high\",\n                \"description\": \"Allows the model to see the low res image and then creates detailed crops of input images as 512px squares based on the input image size. Each of the detailed crops uses twice the token budget (65 tokens) for a total of 129 tokens.\"\n              }\n            ],\n            \"default\": \"auto\"\n          },\n          {\n            \"displayName\": \"Message\",\n            \"name\": \"message\",\n            \"type\": \"string\",\n            \"required\": true,\n            \"displayOptions\": {\n              \"hide\": {\n                \"messageType\": [\n                  \"imageBinary\",\n                  \"imageUrl\"\n                ]\n              }\n            },\n            \"default\": \"\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Batch Processing\",\n    \"name\": \"batching\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Batch processing options for rate limiting\",\n    \"options\": [\n      {\n        \"displayName\": \"Batch Size\",\n        \"name\": \"batchSize\",\n        \"default\": 5,\n        \"type\": \"number\",\n        \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n      },\n      {\n        \"displayName\": \"Delay Between Batches\",\n        \"name\": \"delayBetweenBatches\",\n        \"default\": 0,\n        \"type\": \"number\",\n        \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.7\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Connect an <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_outputParser'>output parser</a> on the canvas to specify the output format you require\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"hasOutputParser\": [\n          true\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.chainRetrievalQa",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Question and Answer Chain",
    "description": "Answer questions about retrieved documents",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.6",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Question and Answer Chain node documentation\ndescription: Learn how to use the Question and Answer Chain node in n8n. Follow technical documentation to integrate Question and Answer Chain node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Question and Answer Chain node\n\nUse the Question and Answer Chain node to use a [vector store](/glossary.md#ai-vector-store) as a retriever.\n\nOn this page, you'll find the node parameters for the Question and Answer Chain node, and links to more resources.\n\n## Node parameters\n\n### Query\n\nThe question you want to ask.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'retrieval-qanda-chain') ]]\n\n## Related resources\n\nRefer to [LangChain's documentation on retrieval chains](https://js.langchain.com/docs/tutorials/rag/){:target=_blank .external-link} for examples of how LangChain can use a vector store as a retriever.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common errors or issues and suggested resolution steps, refer to [Common Issues](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Save time with an <a href=\\\"/templates/1960\\\" target=\\\"_blank\\\">example</a> of how this node works\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Query\",\n    \"name\": \"query\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.input }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Query\",\n    \"name\": \"query\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chat_input }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Query\",\n    \"name\": \"query\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.2\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Source for Prompt (User Message)\",\n    \"name\": \"promptType\",\n    \"type\": \"options\",\n    \"default\": \"auto\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"auto\",\n        \"description\": \"Looks for an input field called 'chatInput' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"define\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"auto\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.4\n            }\n          }\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"promptType\": [\n          \"define\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"System Prompt Template\",\n        \"name\": \"systemPromptTemplate\",\n        \"type\": \"string\",\n        \"default\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nContext: {context}\",\n        \"typeOptions\": {\n          \"rows\": 6\n        },\n        \"description\": \"Template string used for the system prompt. This should include the variable `{context}` for the provided context. For text completion models, you should also include the variable `{question}` for the user’s query.\",\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"lt\": 1.5\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"System Prompt Template\",\n        \"name\": \"systemPromptTemplate\",\n        \"type\": \"string\",\n        \"default\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nContext: {context}\",\n        \"typeOptions\": {\n          \"rows\": 6\n        },\n        \"description\": \"Template string used for the system prompt. This should include the variable `{context}` for the provided context. For text completion models, you should also include the variable `{input}` for the user’s query.\",\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.5\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 5,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.6\n                }\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.chainSummarization",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Summarization Chain",
    "description": "Transforms text into a concise summary",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "2.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Summarization Chain node documentation\ndescription: Learn how to use the Summarize Chain node in n8n. Follow technical documentation to integrate Summarize Chain node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Summarization Chain node\n\nUse the Summarization Chain node to summarize multiple documents.\n\nOn this page, you'll find the node parameters for the Summarization Chain node, and links to more resources.\n\n## Node parameters\n\nChoose the type of data you need to summarize in **Data to Summarize**. The data type you choose determines the other node parameters.\n\n* **Use Node Input (JSON)** and **Use Node Input (Binary)**: summarize the data coming into the node from the workflow. \n\t* You can configure the **Chunking Strategy**: choose what strategy to use to define the data chunk sizes.\n\t\t* If you choose **Simple (Define Below)** you can then set **Characters Per Chunk** and **Chunk Overlap (Characters)**.\n\t\t* Choose **Advanced** if you want to connect a splitter sub-node that provides more configuration options.\n* **Use Document Loader**: summarize data provided by a document loader sub-node.\n\n## Node Options\n\nYou can configure the summarization method and prompts. Select **Add Option** > **Summarization Method and Prompts**.\n\nOptions in **Summarization Method**:\n\n* **Map Reduce**: this is the recommended option. Learn more about [Map Reduce](https://js.langchain.com/v0.1/docs/modules/chains/document/map_reduce/){:target=_blank .external-link} in the LangChain documentation.\n* **Refine**: learn more about [Refine](https://js.langchain.com/v0.1/docs/modules/chains/document/refine/){:target=_blank .external-link} in the LangChain documentation.\n* **Stuff**: learn more about [Stuff](https://js.langchain.com/v0.1/docs/modules/chains/document/stuff/){:target=_blank .external-link} in the LangChain documentation.\n\nYou can customize the **Individual Summary Prompts** and the **Final Prompt to Combine**. There are examples in the node. You must include the `\"{text}\"` placeholder.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'summarization-chain') ]]\n\n## Related resources\n\nRefer to [LangChain's documentation on summarization](https://js.langchain.com/docs/tutorials/summarization/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Save time with an <a href=\\\"/templates/1951\\\" target=\\\"_blank\\\">example</a> of how this node works\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Data to Summarize\",\n    \"name\": \"operationMode\",\n    \"type\": \"options\",\n    \"default\": \"nodeInputJson\",\n    \"description\": \"How to pass data into the summarization chain\",\n    \"options\": [\n      {\n        \"name\": \"Use Node Input (JSON)\",\n        \"value\": \"nodeInputJson\",\n        \"description\": \"Summarize the JSON data coming into this node from the previous one\"\n      },\n      {\n        \"name\": \"Use Node Input (Binary)\",\n        \"value\": \"nodeInputBinary\",\n        \"description\": \"Summarize the binary data coming into this node from the previous one\"\n      },\n      {\n        \"name\": \"Use Document Loader\",\n        \"value\": \"documentLoader\",\n        \"description\": \"Use a loader sub-node with more configuration options\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Chunking Strategy\",\n    \"name\": \"chunkingMode\",\n    \"type\": \"options\",\n    \"default\": \"simple\",\n    \"description\": \"Chunk splitting strategy\",\n    \"options\": [\n      {\n        \"name\": \"Simple (Define Below)\",\n        \"value\": \"simple\"\n      },\n      {\n        \"name\": \"Advanced\",\n        \"value\": \"advanced\",\n        \"description\": \"Use a splitter sub-node with more configuration options\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"/operationMode\": [\n          \"nodeInputJson\",\n          \"nodeInputBinary\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Characters Per Chunk\",\n    \"name\": \"chunkSize\",\n    \"type\": \"number\",\n    \"default\": 1000,\n    \"description\": \"Controls the max size (in terms of number of characters) of the final document chunk\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/chunkingMode\": [\n          \"simple\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Chunk Overlap (Characters)\",\n    \"name\": \"chunkOverlap\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Specifies how much characters overlap there should be between chunks\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/chunkingMode\": [\n          \"simple\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Input Data Field Name\",\n        \"name\": \"binaryDataKey\",\n        \"type\": \"string\",\n        \"default\": \"data\",\n        \"description\": \"The name of the field in the agent or chain’s input that contains the binary file to be processed\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/operationMode\": [\n              \"nodeInputBinary\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Summarization Method and Prompts\",\n        \"name\": \"summarizationMethodAndPrompts\",\n        \"type\": \"fixedCollection\",\n        \"default\": {\n          \"values\": {\n            \"summarizationMethod\": \"map_reduce\",\n            \"prompt\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\",\n            \"combineMapPrompt\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\"\n          }\n        },\n        \"placeholder\": \"Add Option\",\n        \"typeOptions\": {},\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Values\",\n            \"values\": [\n              {\n                \"displayName\": \"Summarization Method\",\n                \"name\": \"summarizationMethod\",\n                \"type\": \"options\",\n                \"description\": \"The type of summarization to run\",\n                \"default\": \"map_reduce\",\n                \"options\": [\n                  {\n                    \"name\": \"Map Reduce (Recommended)\",\n                    \"value\": \"map_reduce\",\n                    \"description\": \"Summarize each document (or chunk) individually, then summarize those summaries\"\n                  },\n                  {\n                    \"name\": \"Refine\",\n                    \"value\": \"refine\",\n                    \"description\": \"Summarize the first document (or chunk). Then update that summary based on the next document (or chunk), and repeat.\"\n                  },\n                  {\n                    \"name\": \"Stuff\",\n                    \"value\": \"stuff\",\n                    \"description\": \"Pass all documents (or chunks) at once. Ideal for small datasets.\"\n                  }\n                ]\n              },\n              {\n                \"displayName\": \"Individual Summary Prompt\",\n                \"name\": \"combineMapPrompt\",\n                \"type\": \"string\",\n                \"hint\": \"The prompt to summarize an individual document (or chunk)\",\n                \"displayOptions\": {\n                  \"hide\": {\n                    \"/options.summarizationMethodAndPrompts.values.summarizationMethod\": [\n                      \"stuff\",\n                      \"refine\"\n                    ]\n                  }\n                },\n                \"default\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\",\n                \"typeOptions\": {\n                  \"rows\": 9\n                }\n              },\n              {\n                \"displayName\": \"Final Prompt to Combine\",\n                \"name\": \"prompt\",\n                \"type\": \"string\",\n                \"default\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\",\n                \"hint\": \"The prompt to combine individual summaries\",\n                \"displayOptions\": {\n                  \"hide\": {\n                    \"/options.summarizationMethodAndPrompts.values.summarizationMethod\": [\n                      \"stuff\",\n                      \"refine\"\n                    ]\n                  }\n                },\n                \"typeOptions\": {\n                  \"rows\": 9\n                }\n              },\n              {\n                \"displayName\": \"Prompt\",\n                \"name\": \"prompt\",\n                \"type\": \"string\",\n                \"default\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\",\n                \"displayOptions\": {\n                  \"hide\": {\n                    \"/options.summarizationMethodAndPrompts.values.summarizationMethod\": [\n                      \"refine\",\n                      \"map_reduce\"\n                    ]\n                  }\n                },\n                \"typeOptions\": {\n                  \"rows\": 9\n                }\n              },\n              {\n                \"displayName\": \"Subsequent (Refine) Prompt\",\n                \"name\": \"refinePrompt\",\n                \"type\": \"string\",\n                \"displayOptions\": {\n                  \"hide\": {\n                    \"/options.summarizationMethodAndPrompts.values.summarizationMethod\": [\n                      \"stuff\",\n                      \"map_reduce\"\n                    ]\n                  }\n                },\n                \"default\": \"Your job is to produce a final summary\\nWe have provided an existing summary up to a certain point: \\\"{existing_answer}\\\"\\nWe have the opportunity to refine the existing summary\\n(only if needed) with some more context below.\\n------------\\n\\\"{text}\\\"\\n------------\\n\\nGiven the new context, refine the original summary\\nIf the context isn't useful, return the original summary.\\n\\nREFINED SUMMARY:\",\n                \"hint\": \"The prompt to refine the summary based on the next document (or chunk)\",\n                \"typeOptions\": {\n                  \"rows\": 9\n                }\n              },\n              {\n                \"displayName\": \"Initial Prompt\",\n                \"name\": \"refineQuestionPrompt\",\n                \"type\": \"string\",\n                \"displayOptions\": {\n                  \"hide\": {\n                    \"/options.summarizationMethodAndPrompts.values.summarizationMethod\": [\n                      \"stuff\",\n                      \"map_reduce\"\n                    ]\n                  }\n                },\n                \"default\": \"Write a concise summary of the following:\\n\\n\\n\\\"{text}\\\"\\n\\n\\nCONCISE SUMMARY:\",\n                \"hint\": \"The prompt for the first document (or chunk)\",\n                \"typeOptions\": {\n                  \"rows\": 9\n                }\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 5,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 2.1\n                }\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.chatTrigger",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Chat Trigger",
    "description": "Runs the workflow when an n8n generated webchat is submitted",
    "category": "trigger",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 1,
    "is_webhook": 1,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Make Chat Publicly Available\",\n    \"name\": \"public\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether the chat should be publicly available or only accessible through the manual chat interface\"\n  },\n  {\n    \"displayName\": \"Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"hostedChat\",\n    \"options\": [\n      {\n        \"name\": \"Hosted Chat\",\n        \"value\": \"hostedChat\",\n        \"description\": \"Chat on a page served by n8n\"\n      },\n      {\n        \"name\": \"Embedded Chat\",\n        \"value\": \"webhook\",\n        \"description\": \"Chat through a widget embedded in another page, or by calling a webhook\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"public\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Chat will be live at the URL above once you activate this workflow. Live executions will show up in the ‘executions’ tab\",\n    \"name\": \"hostedChatNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"hostedChat\"\n        ],\n        \"public\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Follow the instructions <a href=\\\"https://www.npmjs.com/package/@n8n/chat\\\" target=\\\"_blank\\\">here</a> to embed chat in a webpage (or just call the webhook URL at the top of this section). Chat will be live once you activate this workflow\",\n    \"name\": \"embeddedChatNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"webhook\"\n        ],\n        \"public\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Authentication\",\n    \"name\": \"authentication\",\n    \"type\": \"options\",\n    \"default\": \"none\",\n    \"description\": \"The way to authenticate\",\n    \"options\": [\n      {\n        \"name\": \"Basic Auth\",\n        \"value\": \"basicAuth\",\n        \"description\": \"Simple username and password (the same one for all users)\"\n      },\n      {\n        \"name\": \"n8n User Auth\",\n        \"value\": \"n8nUserAuth\",\n        \"description\": \"Require user to be logged in with their n8n account\"\n      },\n      {\n        \"name\": \"None\",\n        \"value\": \"none\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"public\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Initial Message(s)\",\n    \"name\": \"initialMessages\",\n    \"type\": \"string\",\n    \"default\": \"Hi there! 👋\\nMy name is Nathan. How can I assist you today?\",\n    \"description\": \"Default messages shown at the start of the chat, one per line\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"hostedChat\"\n        ],\n        \"public\": [\n          true\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Allow File Uploads\",\n        \"name\": \"allowFileUploads\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to allow file uploads in the chat\"\n      },\n      {\n        \"displayName\": \"Allowed File Mime Types\",\n        \"name\": \"allowedFilesMimeTypes\",\n        \"type\": \"string\",\n        \"default\": \"*\",\n        \"placeholder\": \"e.g. image/*, text/*, application/pdf\",\n        \"description\": \"Allowed file types for upload. Comma-separated list of <a href=\\\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types\\\" target=\\\"_blank\\\">MIME types</a>.\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"public\": [\n          false\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Allowed Origins (CORS)\",\n        \"name\": \"allowedOrigins\",\n        \"type\": \"string\",\n        \"default\": \"*\",\n        \"description\": \"Comma-separated list of URLs allowed for cross-origin non-preflight requests. Use * (default) to allow all origins.\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\",\n              \"webhook\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Allow File Uploads\",\n        \"name\": \"allowFileUploads\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to allow file uploads in the chat\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Allowed File Mime Types\",\n        \"name\": \"allowedFilesMimeTypes\",\n        \"type\": \"string\",\n        \"default\": \"*\",\n        \"placeholder\": \"e.g. image/*, text/*, application/pdf\",\n        \"description\": \"Allowed file types for upload. Comma-separated list of <a href=\\\"https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types\\\" target=\\\"_blank\\\">MIME types</a>.\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Input Placeholder\",\n        \"name\": \"inputPlaceholder\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": \"Type your question..\",\n        \"placeholder\": \"e.g. Type your message here\",\n        \"description\": \"Shown as placeholder text in the chat input field\"\n      },\n      {\n        \"displayName\": \"Load Previous Session\",\n        \"name\": \"loadPreviousSession\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Off\",\n            \"value\": \"notSupported\",\n            \"description\": \"Loading messages of previous session is turned off\"\n          },\n          {\n            \"name\": \"From Memory\",\n            \"value\": \"memory\",\n            \"description\": \"Load session messages from memory\"\n          },\n          {\n            \"name\": \"Manually\",\n            \"value\": \"manually\",\n            \"description\": \"Manually return messages of session\"\n          }\n        ],\n        \"default\": \"notSupported\",\n        \"description\": \"If loading messages of a previous session should be enabled\"\n      },\n      {\n        \"displayName\": \"Response Mode\",\n        \"name\": \"responseMode\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"When Last Node Finishes\",\n            \"value\": \"lastNode\",\n            \"description\": \"Returns data of the last-executed node\"\n          },\n          {\n            \"name\": \"Using 'Respond to Webhook' Node\",\n            \"value\": \"responseNode\",\n            \"description\": \"Response defined in that node\"\n          }\n        ],\n        \"default\": \"lastNode\",\n        \"description\": \"When and how to respond to the webhook\"\n      },\n      {\n        \"displayName\": \"Require Button Click to Start Chat\",\n        \"name\": \"showWelcomeScreen\",\n        \"type\": \"boolean\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": false,\n        \"description\": \"Whether to show the welcome screen at the start of the chat\"\n      },\n      {\n        \"displayName\": \"Start Conversation Button Text\",\n        \"name\": \"getStarted\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"show\": {\n            \"showWelcomeScreen\": [\n              true\n            ],\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": \"New Conversation\",\n        \"placeholder\": \"e.g. New Conversation\",\n        \"description\": \"Shown as part of the welcome screen, in the middle of the chat window\"\n      },\n      {\n        \"displayName\": \"Subtitle\",\n        \"name\": \"subtitle\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": \"Start a chat. We're here to help you 24/7.\",\n        \"placeholder\": \"e.g. We're here for you\",\n        \"description\": \"Shown at the top of the chat, under the title\"\n      },\n      {\n        \"displayName\": \"Title\",\n        \"name\": \"title\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": \"Hi there! 👋\",\n        \"placeholder\": \"e.g. Welcome\",\n        \"description\": \"Shown at the top of the chat\"\n      },\n      {\n        \"displayName\": \"Custom Chat Styling\",\n        \"name\": \"customCss\",\n        \"type\": \"string\",\n        \"typeOptions\": {\n          \"rows\": 10,\n          \"editor\": \"cssEditor\"\n        },\n        \"displayOptions\": {\n          \"show\": {\n            \"/mode\": [\n              \"hostedChat\"\n            ]\n          }\n        },\n        \"default\": \":root {\\n  /* Colors */\\n  --chat--color-primary: #e74266;\\n  --chat--color-primary-shade-50: #db4061;\\n  --chat--color-primary-shade-100: #cf3c5c;\\n  --chat--color-secondary: #20b69e;\\n  --chat--color-secondary-shade-50: #1ca08a;\\n  --chat--color-white: #ffffff;\\n  --chat--color-light: #f2f4f8;\\n  --chat--color-light-shade-50: #e6e9f1;\\n  --chat--color-light-shade-100: #c2c5cc;\\n  --chat--color-medium: #d2d4d9;\\n  --chat--color-dark: #101330;\\n  --chat--color-disabled: #777980;\\n  --chat--color-typing: #404040;\\n\\n  /* Base Layout */\\n  --chat--spacing: 1rem;\\n  --chat--border-radius: 0.25rem;\\n  --chat--transition-duration: 0.15s;\\n  --chat--font-family: (\\n    -apple-system,\\n    BlinkMacSystemFont,\\n    'Segoe UI',\\n    Roboto,\\n    Oxygen-Sans,\\n    Ubuntu,\\n    Cantarell,\\n    'Helvetica Neue',\\n    sans-serif\\n  );\\n\\n  /* Window Dimensions */\\n  --chat--window--width: 400px;\\n  --chat--window--height: 600px;\\n  --chat--window--bottom: var(--chat--spacing);\\n  --chat--window--right: var(--chat--spacing);\\n  --chat--window--z-index: 9999;\\n  --chat--window--border: 1px solid var(--chat--color-light-shade-50);\\n  --chat--window--border-radius: var(--chat--border-radius);\\n  --chat--window--margin-bottom: var(--chat--spacing);\\n\\n  /* Header Styles */\\n  --chat--header-height: auto;\\n  --chat--header--padding: var(--chat--spacing);\\n  --chat--header--background: var(--chat--color-dark);\\n  --chat--header--color: var(--chat--color-light);\\n  --chat--header--border-top: none;\\n  --chat--header--border-bottom: none;\\n  --chat--header--border-left: none;\\n  --chat--header--border-right: none;\\n  --chat--heading--font-size: 2em;\\n  --chat--subtitle--font-size: inherit;\\n  --chat--subtitle--line-height: 1.8;\\n\\n  /* Message Styles */\\n  --chat--message--font-size: 1rem;\\n  --chat--message--padding: var(--chat--spacing);\\n  --chat--message--border-radius: var(--chat--border-radius);\\n  --chat--message-line-height: 1.5;\\n  --chat--message--margin-bottom: calc(var(--chat--spacing) * 1);\\n  --chat--message--bot--background: var(--chat--color-white);\\n  --chat--message--bot--color: var(--chat--color-dark);\\n  --chat--message--bot--border: none;\\n  --chat--message--user--background: var(--chat--color-secondary);\\n  --chat--message--user--color: var(--chat--color-white);\\n  --chat--message--user--border: none;\\n  --chat--message--pre--background: rgba(0, 0, 0, 0.05);\\n  --chat--messages-list--padding: var(--chat--spacing);\\n\\n  /* Toggle Button */\\n  --chat--toggle--size: 64px;\\n  --chat--toggle--width: var(--chat--toggle--size);\\n  --chat--toggle--height: var(--chat--toggle--size);\\n  --chat--toggle--border-radius: 50%;\\n  --chat--toggle--background: var(--chat--color-primary);\\n  --chat--toggle--hover--background: var(--chat--color-primary-shade-50);\\n  --chat--toggle--active--background: var(--chat--color-primary-shade-100);\\n  --chat--toggle--color: var(--chat--color-white);\\n\\n  /* Input Area */\\n  --chat--textarea--height: 50px;\\n  --chat--textarea--max-height: 30rem;\\n  --chat--input--font-size: inherit;\\n  --chat--input--border: 0;\\n  --chat--input--border-radius: 0;\\n  --chat--input--padding: 0.8rem;\\n  --chat--input--background: var(--chat--color-white);\\n  --chat--input--text-color: initial;\\n  --chat--input--line-height: 1.5;\\n  --chat--input--placeholder--font-size: var(--chat--input--font-size);\\n  --chat--input--border-active: 0;\\n  --chat--input--left--panel--width: 2rem;\\n\\n  /* Button Styles */\\n  --chat--button--color: var(--chat--color-light);\\n  --chat--button--background: var(--chat--color-primary);\\n  --chat--button--padding: calc(var(--chat--spacing) * 1 / 2) var(--chat--spacing);\\n  --chat--button--border-radius: var(--chat--border-radius);\\n  --chat--button--hover--color: var(--chat--color-light);\\n  --chat--button--hover--background: var(--chat--color-primary-shade-50);\\n  --chat--close--button--color-hover: var(--chat--color-primary);\\n\\n  /* Send and File Buttons */\\n  --chat--input--send--button--background: var(--chat--color-white);\\n  --chat--input--send--button--color: var(--chat--color-light);\\n  --chat--input--send--button--background-hover: var(--chat--color-primary-shade-50);\\n  --chat--input--send--button--color-hover: var(--chat--color-secondary-shade-50);\\n  --chat--input--file--button--background: var(--chat--color-white);\\n  --chat--input--file--button--color: var(--chat--color-secondary);\\n  --chat--input--file--button--background-hover: var(--chat--input--file--button--background);\\n  --chat--input--file--button--color-hover: var(--chat--color-secondary-shade-50);\\n  --chat--files-spacing: 0.25rem;\\n\\n  /* Body and Footer */\\n  --chat--body--background: var(--chat--color-light);\\n  --chat--footer--background: var(--chat--color-light);\\n  --chat--footer--color: var(--chat--color-dark);\\n}\\n\\n\\n/* You can override any class styles, too. Right-click inspect in Chat UI to find class to override. */\\n.chat-message {\\n\\tmax-width: 50%;\\n}\",\n        \"description\": \"Override default styling of the public chat interface with CSS\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"hostedChat\",\n          \"webhook\"\n        ],\n        \"public\": [\n          true\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"httpBasicAuth\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"basicAuth\"\n        ]\n      }\n    }\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.code",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "LangChain Code",
    "description": "LangChain Code Node",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: LangChain Code node documentation\ndescription: Learn how to use the LangChain Code node in n8n. Follow technical documentation to integrate LangChain Code node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# LangChain Code node\n\nUse the LangChain Code node to import LangChain. This means if there is functionality you need that n8n hasn't created a node for, you can still use it. By configuring the LangChain Code node connectors you can use it as a normal node, root node or sub-node.\n\nOn this page, you'll find the node parameters, guidance on configuring the node, and links to more resources.\n\n/// note | Not available on Cloud\nThis node is only available on self-hosted n8n.\n///\n\n## Node parameters\n\n### Add Code\n\nAdd your custom code. Choose either **Execute** or **Supply Data** mode. You can only use one mode.\n\nUnlike the [Code node](/integrations/builtin/core-nodes/n8n-nodes-base.code/index.md), the LangChain Code node doesn't support Python.\n\n* **Execute**: use the LangChain Code node like n8n's own Code node. This takes input data from the workflow, processes it, and returns it as the node output. This mode requires a main input and output. You must create these connections in **Inputs** and **Outputs**.\n* **Supply Data**: use the LangChain Code node as a sub-node, sending data to a root node. This uses an output other than main.\n\nBy default, you can't load built-in or external modules in this node. Self-hosted users can [enable built-in and external modules](/hosting/configuration/configuration-methods.md).\n\n### Inputs\n\nChoose the input types. \n\nThe main input is the normal connector found in all n8n workflows. If you have a main input and output set in the node, **Execute** code is required.\n\n### Outputs\n\nChoose the output types. \n\nThe main output is the normal connector found in all n8n workflows. If you have a main input and output set in the node, **Execute** code is required.\n\n## Node inputs and outputs configuration\n\nBy configuring the LangChain Code node connectors (inputs and outputs) you can use it as an app node, root node or sub-node.\n\n![Screenshot of a workflow with four LangChain nodes, configured as different node types](/_images/integrations/builtin/cluster-nodes/langchaincode/create-node-types.png)\n\n| Node type | Inputs | Outputs | Code mode |\n| --------- | ------ | ------- | --------- |\n| App node. Similar to the [Code node](/integrations/builtin/core-nodes/n8n-nodes-base.code/index.md). | Main | Main | Execute |\n| Root node | Main; at least one other type | Main | Execute |\n| Sub-node | - | A type other than main. Must match the input type you want to connect to. | Supply Data |\n| Sub-node with sub-nodes | A type other than main |A type other than main. Must match the input type you want to connect to. | Supply Data |\n\n## Built-in methods\n\nn8n provides these methods to make it easier to perform common tasks in the LangChain Code node.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/langchaincode/builtin-methods.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'langchain-code') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Code\",\n    \"name\": \"code\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"name\": \"execute\",\n        \"displayName\": \"Execute\",\n        \"values\": [\n          {\n            \"displayName\": \"JavaScript - Execute\",\n            \"name\": \"code\",\n            \"type\": \"string\",\n            \"typeOptions\": {\n              \"editor\": \"jsEditor\"\n            },\n            \"default\": \"const { PromptTemplate } = require('@langchain/core/prompts');\\n\\nconst query = 'Tell me a joke';\\nconst prompt = PromptTemplate.fromTemplate(query);\\n\\n// If you are allowing more than one language model input connection (-1 or\\n// anything greater than 1), getInputConnectionData returns an array, so you\\n// will have to change the code below it to deal with that. For example, use\\n// llm[0] in the chain definition\\n\\nconst llm = await this.getInputConnectionData('ai_languageModel', 0);\\nlet chain = prompt.pipe(llm);\\nconst output = await chain.invoke();\\nreturn [ {json: { output } } ];\",\n            \"hint\": \"This code will only run and return data if a \\\"Main\\\" input & output got created.\",\n            \"noDataExpression\": true\n          }\n        ]\n      },\n      {\n        \"name\": \"supplyData\",\n        \"displayName\": \"Supply Data\",\n        \"values\": [\n          {\n            \"displayName\": \"JavaScript - Supply Data\",\n            \"name\": \"code\",\n            \"type\": \"string\",\n            \"typeOptions\": {\n              \"editor\": \"jsEditor\"\n            },\n            \"default\": \"const { WikipediaQueryRun } = require( '@langchain/community/tools/wikipedia_query_run');\\nreturn new WikipediaQueryRun();\",\n            \"hint\": \"This code will only run and return data if an output got created which is not \\\"Main\\\".\",\n            \"noDataExpression\": true\n          }\n        ]\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"You can import LangChain and use all available functionality. Debug by using <code>console.log()</code> statements and viewing their output in the browser console.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Inputs\",\n    \"name\": \"inputs\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"description\": \"The input to add\",\n    \"options\": [\n      {\n        \"name\": \"input\",\n        \"displayName\": \"Input\",\n        \"values\": [\n          {\n            \"displayName\": \"Type\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"Chain\",\n                \"value\": \"ai_chain\"\n              },\n              {\n                \"name\": \"Document\",\n                \"value\": \"ai_document\"\n              },\n              {\n                \"name\": \"Embedding\",\n                \"value\": \"ai_embedding\"\n              },\n              {\n                \"name\": \"Language Model\",\n                \"value\": \"ai_languageModel\"\n              },\n              {\n                \"name\": \"Memory\",\n                \"value\": \"ai_memory\"\n              },\n              {\n                \"name\": \"Output Parser\",\n                \"value\": \"ai_outputParser\"\n              },\n              {\n                \"name\": \"Text Splitter\",\n                \"value\": \"ai_textSplitter\"\n              },\n              {\n                \"name\": \"Tool\",\n                \"value\": \"ai_tool\"\n              },\n              {\n                \"name\": \"Vector Store\",\n                \"value\": \"ai_vectorStore\"\n              },\n              {\n                \"name\": \"Main\",\n                \"value\": \"main\"\n              }\n            ],\n            \"noDataExpression\": true,\n            \"default\": \"\",\n            \"required\": true,\n            \"description\": \"The type of the input\"\n          },\n          {\n            \"displayName\": \"Max Connections\",\n            \"name\": \"maxConnections\",\n            \"type\": \"number\",\n            \"noDataExpression\": true,\n            \"default\": -1,\n            \"required\": true,\n            \"description\": \"How many nodes of this type are allowed to be connected. Set it to -1 for unlimited.\"\n          },\n          {\n            \"displayName\": \"Required\",\n            \"name\": \"required\",\n            \"type\": \"boolean\",\n            \"noDataExpression\": true,\n            \"default\": false,\n            \"required\": true,\n            \"description\": \"Whether the input needs a connection\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true,\n      \"sortable\": true\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Outputs\",\n    \"name\": \"outputs\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"description\": \"The output to add\",\n    \"options\": [\n      {\n        \"name\": \"output\",\n        \"displayName\": \"Output\",\n        \"values\": [\n          {\n            \"displayName\": \"Type\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"Chain\",\n                \"value\": \"ai_chain\"\n              },\n              {\n                \"name\": \"Document\",\n                \"value\": \"ai_document\"\n              },\n              {\n                \"name\": \"Embedding\",\n                \"value\": \"ai_embedding\"\n              },\n              {\n                \"name\": \"Language Model\",\n                \"value\": \"ai_languageModel\"\n              },\n              {\n                \"name\": \"Memory\",\n                \"value\": \"ai_memory\"\n              },\n              {\n                \"name\": \"Output Parser\",\n                \"value\": \"ai_outputParser\"\n              },\n              {\n                \"name\": \"Text Splitter\",\n                \"value\": \"ai_textSplitter\"\n              },\n              {\n                \"name\": \"Tool\",\n                \"value\": \"ai_tool\"\n              },\n              {\n                \"name\": \"Vector Store\",\n                \"value\": \"ai_vectorStore\"\n              },\n              {\n                \"name\": \"Main\",\n                \"value\": \"main\"\n              }\n            ],\n            \"noDataExpression\": true,\n            \"default\": \"\",\n            \"required\": true,\n            \"description\": \"The type of the input\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true,\n      \"sortable\": true\n    },\n    \"noDataExpression\": true\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.documentBinaryInputLoader",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Binary Input Loader",
    "description": "Use binary data from a previous step in the workflow",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Loader Type\",\n    \"name\": \"loader\",\n    \"type\": \"options\",\n    \"default\": \"jsonLoader\",\n    \"options\": [\n      {\n        \"name\": \"CSV Loader\",\n        \"value\": \"csvLoader\",\n        \"description\": \"Load CSV files\"\n      },\n      {\n        \"name\": \"Docx Loader\",\n        \"value\": \"docxLoader\",\n        \"description\": \"Load Docx documents\"\n      },\n      {\n        \"name\": \"EPub Loader\",\n        \"value\": \"epubLoader\",\n        \"description\": \"Load EPub files\"\n      },\n      {\n        \"name\": \"JSON Loader\",\n        \"value\": \"jsonLoader\",\n        \"description\": \"Load JSON files\"\n      },\n      {\n        \"name\": \"PDF Loader\",\n        \"value\": \"pdfLoader\",\n        \"description\": \"Load PDF documents\"\n      },\n      {\n        \"name\": \"Text Loader\",\n        \"value\": \"textLoader\",\n        \"description\": \"Load plain text files\"\n      }\n    ],\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Binary Data Key\",\n    \"name\": \"binaryDataKey\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"Name of the binary property from which to read the file buffer\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Split Pages\",\n    \"name\": \"splitPages\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"loader\": [\n          \"pdfLoader\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Column\",\n    \"name\": \"column\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Column to extract from CSV\",\n    \"displayOptions\": {\n      \"show\": {\n        \"loader\": [\n          \"csvLoader\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Separator\",\n    \"name\": \"separator\",\n    \"type\": \"string\",\n    \"default\": \",\",\n    \"description\": \"Separator to use for CSV\",\n    \"displayOptions\": {\n      \"show\": {\n        \"loader\": [\n          \"csvLoader\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Pointers\",\n    \"name\": \"pointers\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Pointers to extract from JSON, e.g. \\\"/text\\\" or \\\"/text, /meta/title\\\"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"loader\": [\n          \"jsonLoader\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Metadata\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to add to each document. Could be used for filtering during retrieval\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add property\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.documentDefaultDataLoader",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Default Data Loader",
    "description": "Load data from previous step in the workflow",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Default Data Loader node documentation\ndescription: Learn how to use the Default Data Loader node in n8n. Follow technical documentation to integrate Default Data Loader node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Default Data Loader node\n\nUse the Default Data Loader node to load binary data files or JSON data for [vector stores](/glossary.md#ai-vector-store) or summarization.\n\nOn this page, you'll find a list of parameters the Default Data Loader node supports, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Text Splitting**: Choose from:\n\t* **Simple**: Uses the [Recursive Character Text Splitter](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter.md) with a chunk size of 1000 and an overlap of 200.\n\t* **Custom**: Allows you to connect a text splitter of your choice.\n* **Type of Data**: Select **Binary** or **JSON**.\n* **Mode**: Choose from:\n\t* **Load All Input Data**: Use all the node's input data.\n\t* **Load Specific Data**: Use [expressions](/code/expressions.md) to define the data you want to load. You can add text as well as expressions. This means you can create a custom document from a mix of text and expressions.\n* **Data Format**: Displays when you set **Type of Data** to **Binary**. Select the file MIME type for your binary data. Set to **Automatically Detect by MIME Type** if you want n8n to set the data format for you. If you set a specific data format and the incoming file MIME type doesn't match it, the node errors. If you use **Automatically Detect by MIME Type**, the node falls back to text format if it can't match the file MIME type to a supported data format.\n\n## Node options\n\n* **Metadata**: Set the metadata that should accompany the document in the vector store. This is what you match to using the **Metadata Filter** option when retrieving data using the vector store nodes.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'default-data-loader') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-doc-loaders-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This will load data from a previous step in the workflow. <a href=\\\"/templates/1962\\\" target=\\\"_blank\\\">Example</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Type of Data\",\n    \"name\": \"dataType\",\n    \"type\": \"options\",\n    \"default\": \"json\",\n    \"options\": [\n      {\n        \"name\": \"JSON\",\n        \"value\": \"json\",\n        \"description\": \"Process JSON data from previous step in the workflow\"\n      },\n      {\n        \"name\": \"Binary\",\n        \"value\": \"binary\",\n        \"description\": \"Process binary data from previous step in the workflow\"\n      }\n    ],\n    \"required\": true,\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Mode\",\n    \"name\": \"jsonMode\",\n    \"type\": \"options\",\n    \"default\": \"allInputData\",\n    \"options\": [\n      {\n        \"name\": \"Load All Input Data\",\n        \"value\": \"allInputData\",\n        \"description\": \"Use all JSON data that flows into the parent agent or chain\"\n      },\n      {\n        \"name\": \"Load Specific Data\",\n        \"value\": \"expressionData\",\n        \"description\": \"Load a subset of data, and/or data from any previous step in the workflow\"\n      }\n    ],\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"dataType\": [\n          \"json\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Mode\",\n    \"name\": \"binaryMode\",\n    \"type\": \"options\",\n    \"default\": \"allInputData\",\n    \"options\": [\n      {\n        \"name\": \"Load All Input Data\",\n        \"value\": \"allInputData\",\n        \"description\": \"Use all Binary data that flows into the parent agent or chain\"\n      },\n      {\n        \"name\": \"Load Specific Data\",\n        \"value\": \"specificField\",\n        \"description\": \"Load data from a specific field in the parent agent or chain\"\n      }\n    ],\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"dataType\": [\n          \"binary\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Data Format\",\n    \"name\": \"loader\",\n    \"type\": \"options\",\n    \"default\": \"auto\",\n    \"options\": [\n      {\n        \"name\": \"Automatically Detect by Mime Type\",\n        \"value\": \"auto\",\n        \"description\": \"Uses the mime type to detect the format\"\n      },\n      {\n        \"name\": \"CSV\",\n        \"value\": \"csvLoader\",\n        \"description\": \"Load CSV files\"\n      },\n      {\n        \"name\": \"Docx\",\n        \"value\": \"docxLoader\",\n        \"description\": \"Load Docx documents\"\n      },\n      {\n        \"name\": \"EPub\",\n        \"value\": \"epubLoader\",\n        \"description\": \"Load EPub files\"\n      },\n      {\n        \"name\": \"JSON\",\n        \"value\": \"jsonLoader\",\n        \"description\": \"Load JSON files\"\n      },\n      {\n        \"name\": \"PDF\",\n        \"value\": \"pdfLoader\",\n        \"description\": \"Load PDF documents\"\n      },\n      {\n        \"name\": \"Text\",\n        \"value\": \"textLoader\",\n        \"description\": \"Load plain text files\"\n      }\n    ],\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"dataType\": [\n          \"binary\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Data\",\n    \"name\": \"jsonData\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Drag and drop fields from the input pane, or use an expression\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"dataType\": [\n          \"json\"\n        ],\n        \"jsonMode\": [\n          \"expressionData\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 6\n    }\n  },\n  {\n    \"displayName\": \"Input Data Field Name\",\n    \"name\": \"binaryDataKey\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"The name of the field in the agent or chain’s input that contains the binary file to be processed\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"dataType\": [\n          \"binary\"\n        ]\n      },\n      \"hide\": {\n        \"binaryMode\": [\n          \"allInputData\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Text Splitting\",\n    \"name\": \"textSplittingMode\",\n    \"type\": \"options\",\n    \"default\": \"simple\",\n    \"options\": [\n      {\n        \"name\": \"Simple\",\n        \"value\": \"simple\",\n        \"description\": \"Splits every 1000 characters with a 200 character overlap\"\n      },\n      {\n        \"name\": \"Custom\",\n        \"value\": \"custom\",\n        \"description\": \"Connect a custom text-splitting sub-node\"\n      }\n    ],\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"JSON Pointers\",\n        \"name\": \"pointers\",\n        \"type\": \"string\",\n        \"default\": \"\",\n        \"description\": \"Pointers to extract from JSON, e.g. \\\"/text\\\" or \\\"/text, /meta/title\\\"\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/loader\": [\n              \"jsonLoader\",\n              \"auto\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"CSV Separator\",\n        \"name\": \"separator\",\n        \"type\": \"string\",\n        \"description\": \"Separator to use for CSV\",\n        \"default\": \",\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/loader\": [\n              \"csvLoader\",\n              \"auto\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"CSV Column\",\n        \"name\": \"column\",\n        \"type\": \"string\",\n        \"default\": \"\",\n        \"description\": \"Column to extract from CSV\",\n        \"displayOptions\": {\n          \"show\": {\n            \"/loader\": [\n              \"csvLoader\",\n              \"auto\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Split Pages in PDF\",\n        \"description\": \"Whether to split PDF pages into separate documents\",\n        \"name\": \"splitPages\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"displayOptions\": {\n          \"show\": {\n            \"/loader\": [\n              \"pdfLoader\",\n              \"auto\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Metadata\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to add to each document. Could be used for filtering during retrieval\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add property\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.documentGithubLoader",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "GitHub Document Loader",
    "description": "Use GitHub data as input to this chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: GitHub Document Loader node documentation\ndescription: Learn how to use the GitHub Document Loader node in n8n. Follow technical documentation to integrate GitHub Document Loader node into your workflows.\ncontentType: [integration, reference]\n---\n\n# GitHub Document Loader node\n\nUse the GitHub Document Loader node to load data from a GitHub repository for [vector stores](/glossary.md#ai-vector-store) or summarization.\n\nOn this page, you'll find the node parameters for the GitHub Document Loader node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/github.md). This node doesn't support OAuth for authentication.\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Text Splitting**: Choose from:\n\t* **Simple**: Uses the [Recursive Character Text Splitter](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.textsplitterrecursivecharactertextsplitter.md) with a chunk size of 1000 and an overlap of 200.\n    * **Custom**: Allows you to connect a text splitter of your choice.\n* **Repository Link**: Enter the URL of your GitHub repository.\n* **Branch**: Enter the branch name to use.\n\n## Node options\n\n* **Recursive**: Select whether to include sub-folders and files (turned on) or not (turned off).\n* **Ignore Paths**: Enter directories to ignore.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'github-document-loader') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-doc-loaders-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Repository Link\",\n    \"name\": \"repository\",\n    \"type\": \"string\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Branch\",\n    \"name\": \"branch\",\n    \"type\": \"string\",\n    \"default\": \"main\"\n  },\n  {\n    \"displayName\": \"Text Splitting\",\n    \"name\": \"textSplittingMode\",\n    \"type\": \"options\",\n    \"default\": \"simple\",\n    \"options\": [\n      {\n        \"name\": \"Simple\",\n        \"value\": \"simple\",\n        \"description\": \"Splits every 1000 characters with a 200 character overlap\"\n      },\n      {\n        \"name\": \"Custom\",\n        \"value\": \"custom\",\n        \"description\": \"Connect a custom text-splitting sub-node\"\n      }\n    ],\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"additionalOptions\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Recursive\",\n        \"name\": \"recursive\",\n        \"type\": \"boolean\",\n        \"default\": false\n      },\n      {\n        \"displayName\": \"Ignore Paths\",\n        \"name\": \"ignorePaths\",\n        \"type\": \"string\",\n        \"description\": \"Comma-separated list of paths to ignore, e.g. \\\"docs, src/tests\",\n        \"default\": \"\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"githubApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.documentJsonInputLoader",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "JSON Input Loader",
    "description": "Use JSON data from a previous step in the workflow",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Pointers\",\n    \"name\": \"pointers\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Pointers to extract from JSON, e.g. \\\"/text\\\" or \\\"/text, /meta/title\\\"\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Metadata\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to add to each document. Could be used for filtering during retrieval\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add property\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsAwsBedrock",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings AWS Bedrock",
    "description": "Use Embeddings AWS Bedrock",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings AWS Bedrock node documentation\ndescription: Learn how to use the Embeddings AWS Bedrock node in n8n. Follow technical documentation to integrate Embeddings AWS Bedrock node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Embeddings AWS Bedrock node\n\nUse the Embeddings AWS Bedrock node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings AWS Bedrock node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/aws.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding.\n\nLearn more about available models in the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/what-is-bedrock.html){:target=_blank .external-link}. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-aws-bedrock') ]]\n\n## Related resources\n\nRefer to [LangChains's AWS Bedrock embeddings documentation](https://js.langchain.com/docs/integrations/platforms/aws/#text-embedding-models){:target=_blank .external-link} and the [AWS Bedrock documentation](https://docs.aws.amazon.com/bedrock/){:target=_blank .external-link} for more information about AWS Bedrock.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/foundation-models?byInferenceType=ON_DEMAND&byOutputModality=EMBEDDING\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"modelSummaries\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.modelName}}\",\n                  \"description\": \"={{$responseItem.modelArn}}\",\n                  \"value\": \"={{$responseItem.modelId}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"aws\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsAzureOpenAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Azure OpenAI",
    "description": "Use Embeddings Azure OpenAI",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Azure OpenAI node documentation\ndescription: Learn how to use the Embeddings Azure OpenAI node in n8n. Follow technical documentation to integrate Embeddings Azure OpenAI node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Embeddings Azure OpenAI node\n\nUse the Embeddings Azure OpenAI node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Azure OpenAI node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/azureopenai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n\n## Node options\n\n* **Model (Deployment) Name**: Select the model (deployment) to use for generating embeddings.\n* **Batch Size**: Enter the maximum number of documents to send in each request.\n* **Strip New Lines**: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\n* **Timeout**: Enter the maximum amount of time a request can take in seconds. Set to `-1` for no timeout.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-azure-openai') ]]\n\n## Related resources\n\nRefer to [LangChains's OpenAI embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/azure_openai/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model (Deployment) Name\",\n    \"name\": \"model\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the model(deployment) to use\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Batch Size\",\n        \"name\": \"batchSize\",\n        \"default\": 512,\n        \"typeOptions\": {\n          \"maxValue\": 2048\n        },\n        \"description\": \"Maximum number of documents to send in each request\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Strip New Lines\",\n        \"name\": \"stripNewLines\",\n        \"default\": true,\n        \"description\": \"Whether to strip new lines from the input text\",\n        \"type\": \"boolean\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": -1,\n        \"description\": \"Maximum amount of time a request is allowed to take in seconds. Set to -1 for no timeout.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Dimensions\",\n        \"name\": \"dimensions\",\n        \"description\": \"The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"256\",\n            \"value\": 256\n          },\n          {\n            \"name\": \"512\",\n            \"value\": 512\n          },\n          {\n            \"name\": \"1024\",\n            \"value\": 1024\n          },\n          {\n            \"name\": \"1536\",\n            \"value\": 1536\n          },\n          {\n            \"name\": \"3072\",\n            \"value\": 3072\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"azureOpenAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsCohere",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Cohere",
    "description": "Use Cohere Embeddings",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Cohere node documentation\ndescription: Learn how to use the Embeddings Cohere node in n8n. Follow technical documentation to integrate Embeddings Cohere node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Embeddings Cohere node\n\nUse the Embeddings Cohere node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Cohere node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/cohere.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding. Choose from:\n    * **Embed-English-v2.0(4096 Dimensions)**\n\t* **Embed-English-Light-v2.0(1024 Dimensions)**\n\t* **Embed-Multilingual-v2.0(768 Dimensions)**\n\nLearn more about available models in [Cohere's models documentation](https://docs.cohere.com/docs/models){:target=_blank .external-link}.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-cohere') ]]\n\n## Related resources\n\nRefer to [Langchain's Cohere embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/cohere/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelName\",\n    \"type\": \"options\",\n    \"default\": \"embed-english-v2.0\",\n    \"description\": \"The model which will generate the embeddings. <a href=\\\"https://docs.cohere.com/docs/models\\\">Learn more</a>.\",\n    \"options\": [\n      {\n        \"name\": \"Embed-English-Light-v2.0 (1024 Dimensions)\",\n        \"value\": \"embed-english-light-v2.0\"\n      },\n      {\n        \"name\": \"Embed-English-Light-v3.0 (384 Dimensions)\",\n        \"value\": \"embed-english-light-v3.0\"\n      },\n      {\n        \"name\": \"Embed-English-v2.0 (4096 Dimensions)\",\n        \"value\": \"embed-english-v2.0\"\n      },\n      {\n        \"name\": \"Embed-English-v3.0 (1024 Dimensions)\",\n        \"value\": \"embed-english-v3.0\"\n      },\n      {\n        \"name\": \"Embed-Multilingual-Light-v3.0 (384 Dimensions)\",\n        \"value\": \"embed-multilingual-light-v3.0\"\n      },\n      {\n        \"name\": \"Embed-Multilingual-v2.0 (768 Dimensions)\",\n        \"value\": \"embed-multilingual-v2.0\"\n      },\n      {\n        \"name\": \"Embed-Multilingual-v3.0 (1024 Dimensions)\",\n        \"value\": \"embed-multilingual-v3.0\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"cohereApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsGoogleGemini",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Google Gemini",
    "description": "Use Google Gemini Embeddings",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Google Gemini node documentation\ndescription: Learn how to use the Embeddings Google Gemini node in n8n. Follow technical documentation to integrate Embeddings Google Gemini node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Embeddings Google Gemini node\n\nUse the Embeddings Google Gemini node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Google Gemini node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/googleai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding.\n\nLearn more about available models in [Google Gemini's models documentation](https://ai.google.dev/models/gemini){:target=_blank .external-link}.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-google-gemini') ]]\n\n## Related resources\n\nRefer to [Langchain's Google Generative AI embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/google_generativeai){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelName\",\n    \"type\": \"options\",\n    \"default\": \"models/text-embedding-004\",\n    \"description\": \"The model which will generate the embeddings. <a href=\\\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/v1beta/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"models\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.name.includes('embedding') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.name}}\",\n                  \"description\": \"={{$responseItem.description}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"googlePalmApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsGoogleVertex",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Google Vertex",
    "description": "Use Google Vertex Embeddings",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Google Vertex node documentation\ndescription: Learn how to use the Embeddings Google Vertex node in n8n. Follow technical documentation to integrate Embeddings Google Gemini node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Embeddings Google Vertex node\n\nUse the Embeddings Google Vertex node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Google Vertex node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/google/service-account.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n- **Model**: Select the model to use to generate the embedding.\n\nLearn more about available embedding models in [Google VertexAI embeddings API documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api).\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n\n[[templatesWidget(page.title, 'embeddings-google-vertex')]]\n\n## Related resources\n\nRefer to [LangChain's Google Generative AI embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/google_generativeai) for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings. You can find available models <a href=\\\"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api\\\">here</a>.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Project ID\",\n    \"name\": \"projectId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"description\": \"Select or enter your Google Cloud project ID\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Model Name\",\n    \"name\": \"modelName\",\n    \"type\": \"string\",\n    \"default\": \"text-embedding-005\",\n    \"description\": \"The model which will generate the embeddings. <a href=\\\"https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings-api\\\">Learn more</a>.\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"googleApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsHuggingFaceInference",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Hugging Face Inference",
    "description": "Use HuggingFace Inference Embeddings",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings HuggingFace Inference node documentation\ndescription: Learn how to use the Embeddings HuggingFace Inference node in n8n. Follow technical documentation to integrate Embeddings HuggingFace Inference node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Embeddings HuggingFace Inference node\n\nUse the Embeddings HuggingFace Inference node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings HuggingFace Inference, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/huggingface.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding.\n\nRefer to the [Hugging Face models documentation](https://huggingface.co/models?other=embeddings){:target=_blank .external-link} for available models.\n\n## Node options\n\n* **Custom Inference Endpoint**: Enter the URL of your deployed model, hosted by HuggingFace. If you set this, n8n ignores the **Model Name**.\n\nRefer to [HuggingFace's guide to inference](https://huggingface.co/inference-endpoints){:target=_blank .external-link} for more information.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-hugging-face-inference') ]]\n\n## Related resources\n\nRefer to [Langchain's HuggingFace Inference embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/hugging_face_inference/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Each model is using different dimensional density for embeddings. Please make sure to use the same dimensionality for your vector store. The default model is using 768-dimensional embeddings.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Model Name\",\n    \"name\": \"modelName\",\n    \"type\": \"string\",\n    \"default\": \"sentence-transformers/distilbert-base-nli-mean-tokens\",\n    \"description\": \"The model name to use from HuggingFace library\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Custom Inference Endpoint\",\n        \"name\": \"endpointUrl\",\n        \"default\": \"\",\n        \"description\": \"Custom endpoint URL\",\n        \"type\": \"string\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"huggingFaceApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsMistralCloud",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Mistral Cloud",
    "description": "Use Embeddings Mistral Cloud",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Mistral Cloud node documentation\ndescription: Learn how to use the Embeddings Mistral Cloud node in n8n. Follow technical documentation to integrate Embeddings Mistral Cloud node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Embeddings Mistral Cloud node\n\nUse the Embeddings Mistral Cloud node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Mistral Cloud node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/mistral.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding.\n\nLearn more about available models in [Mistral's models documentation](https://docs.mistral.ai/platform/pricing/){:target=_blank .external-link}.\n\n## Node options\n\n* **Batch Size**: Enter the maximum number of documents to send in each request.\n* **Strip New Lines**: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-mistral-cloud') ]]\n\n## Related resources\n\nRefer to [Langchain's Mistral embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/mistralai){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"mistral-embed\",\n    \"description\": \"The model which will compute the embeddings. <a href=\\\"https://docs.mistral.ai/platform/endpoints/\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.id.includes('embed') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{ $responseItem.id }}\",\n                  \"value\": \"={{ $responseItem.id }}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Batch Size\",\n        \"name\": \"batchSize\",\n        \"default\": 512,\n        \"typeOptions\": {\n          \"maxValue\": 2048\n        },\n        \"description\": \"Maximum number of documents to send in each request\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Strip New Lines\",\n        \"name\": \"stripNewLines\",\n        \"default\": true,\n        \"description\": \"Whether to strip new lines from the input text\",\n        \"type\": \"boolean\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"mistralCloudApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsOllama",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings Ollama",
    "description": "Use Ollama Embeddings",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings Ollama node documentation\ndescription: Learn how to use the Embeddings Ollama node in n8n. Follow technical documentation to integrate Embeddings Ollama node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Embeddings Ollama node\n\nUse the Embeddings Ollama node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings Ollama node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/ollama.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the embedding. Choose from:\n    * [all-minilm](https://ollama.com/library/all-minilm) (384 Dimensions)\n    * [nomic-embed-text](https://ollama.com/library/nomic-embed-text) (768 Dimensions)\n\nLearn more about available models in [Ollama's models documentation](https://ollama.ai/library){:target=_blank .external-link}.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-ollama') ]]\n\n## Related resources\n\nRefer to [Langchain's Ollama embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/ollama/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"llama3.2\",\n    \"description\": \"The model which will generate the completion. To download models, visit <a href=\\\"https://ollama.ai/library\\\">Ollama Models Library</a>.\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/api/tags\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"models\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.name}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"ollamaApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.embeddingsOpenAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Embeddings OpenAI",
    "description": "Use Embeddings OpenAI",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.2",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Embeddings OpenAI node documentation\ndescription: Learn how to use the Embeddings OpenAI node in n8n. Follow technical documentation to integrate Embeddings OpenAI node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Embeddings OpenAI node\n\nUse the Embeddings OpenAI node to generate [embeddings](/glossary.md#ai-embedding) for a given text.\n\nOn this page, you'll find the node parameters for the Embeddings OpenAI node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/openai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n\n## Node options\n\n* **Model**: Select the model to use for generating embeddings.\n* **Base URL**: Enter the URL to send the request to. Use this if you are using a self-hosted OpenAI-like model. \n* **Batch Size**: Enter the maximum number of documents to send in each request.\n* **Strip New Lines**: Select whether to remove new line characters from input text (turned on) or not (turned off). n8n enables this by default.\n* **Timeout**: Enter the maximum amount of time a request can take in seconds. Set to `-1` for no timeout.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'embeddings-openai') ]]\n\n## Related resources\n\nRefer to [LangChains's OpenAI embeddings documentation](https://js.langchain.com/docs/integrations/text_embedding/openai/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a vector store. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_vectorStore'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"text-embedding-ada-002\",\n    \"description\": \"The model which will generate the embeddings. <a href=\\\"https://platform.openai.com/docs/models/overview\\\">Learn more</a>.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"={{ $parameter.options?.baseURL?.split(\\\"/\\\").slice(-1).pop() || $credentials?.url?.split(\\\"/\\\").slice(-1).pop() || \\\"v1\\\" }}/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.id.includes('embed') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"text-embedding-3-small\",\n    \"description\": \"The model which will generate the embeddings. <a href=\\\"https://platform.openai.com/docs/models/overview\\\">Learn more</a>.\",\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          1\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"={{ $parameter.options?.baseURL?.split(\\\"/\\\").slice(-1).pop() || $credentials?.url?.split(\\\"/\\\").slice(-1).pop() || \\\"v1\\\" }}/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.id.includes('embed') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Dimensions\",\n        \"name\": \"dimensions\",\n        \"description\": \"The number of dimensions the resulting output embeddings should have. Only supported in text-embedding-3 and later models.\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"256\",\n            \"value\": 256\n          },\n          {\n            \"name\": \"512\",\n            \"value\": 512\n          },\n          {\n            \"name\": \"1024\",\n            \"value\": 1024\n          },\n          {\n            \"name\": \"1536\",\n            \"value\": 1536\n          },\n          {\n            \"name\": \"3072\",\n            \"value\": 3072\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Base URL\",\n        \"name\": \"baseURL\",\n        \"default\": \"https://api.openai.com/v1\",\n        \"description\": \"Override the default base URL for the API\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.2\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Batch Size\",\n        \"name\": \"batchSize\",\n        \"default\": 512,\n        \"typeOptions\": {\n          \"maxValue\": 2048\n        },\n        \"description\": \"Maximum number of documents to send in each request\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Strip New Lines\",\n        \"name\": \"stripNewLines\",\n        \"default\": true,\n        \"description\": \"Whether to strip new lines from the input text\",\n        \"type\": \"boolean\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": -1,\n        \"description\": \"Maximum amount of time a request is allowed to take in seconds. Set to -1 for no timeout.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"openAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.informationExtractor",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Information Extractor",
    "description": "Extract information from text in a structured format",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.2",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Text\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The text to extract information from\",\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Schema Type\",\n    \"name\": \"schemaType\",\n    \"type\": \"options\",\n    \"default\": \"fromAttributes\",\n    \"description\": \"How to specify the schema for the desired output\",\n    \"options\": [\n      {\n        \"name\": \"From Attribute Descriptions\",\n        \"value\": \"fromAttributes\",\n        \"description\": \"Extract specific attributes from the text based on types and descriptions\"\n      },\n      {\n        \"name\": \"Generate From JSON Example\",\n        \"value\": \"fromJson\",\n        \"description\": \"Generate a schema from an example JSON object\"\n      },\n      {\n        \"name\": \"Define using JSON Schema\",\n        \"value\": \"manual\",\n        \"description\": \"Define the JSON schema manually\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"JSON Example\",\n    \"name\": \"jsonSchemaExample\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\t\\\"state\\\": \\\"California\\\",\\n\\t\\\"cities\\\": [\\\"Los Angeles\\\", \\\"San Francisco\\\", \\\"San Diego\\\"]\\n}\",\n    \"description\": \"Example JSON object to use to generate the schema\",\n    \"displayOptions\": {\n      \"show\": {\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"All properties will be required. To make them optional, use the 'JSON Schema' schema type instead\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ],\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Schema\",\n    \"name\": \"inputSchema\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\t\\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n\\t\\t\\\"state\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t},\\n\\t\\t\\\"cities\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"array\\\",\\n\\t\\t\\t\\\"items\\\": {\\n\\t\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\",\n    \"description\": \"Schema to use for the function\",\n    \"displayOptions\": {\n      \"show\": {\n        \"schemaType\": [\n          \"manual\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Attributes\",\n    \"name\": \"attributes\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"name\": \"attributes\",\n        \"displayName\": \"Attribute List\",\n        \"values\": [\n          {\n            \"displayName\": \"Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"description\": \"Attribute to extract\",\n            \"placeholder\": \"e.g. company_name\",\n            \"required\": true\n          },\n          {\n            \"displayName\": \"Type\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"description\": \"Data type of the attribute\",\n            \"required\": true,\n            \"options\": [\n              {\n                \"name\": \"Boolean\",\n                \"value\": \"boolean\"\n              },\n              {\n                \"name\": \"Date\",\n                \"value\": \"date\"\n              },\n              {\n                \"name\": \"Number\",\n                \"value\": \"number\"\n              },\n              {\n                \"name\": \"String\",\n                \"value\": \"string\"\n              }\n            ],\n            \"default\": \"string\"\n          },\n          {\n            \"displayName\": \"Description\",\n            \"name\": \"description\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"description\": \"Describe your attribute\",\n            \"placeholder\": \"Add description for the attribute\",\n            \"required\": true\n          },\n          {\n            \"displayName\": \"Required\",\n            \"name\": \"required\",\n            \"type\": \"boolean\",\n            \"default\": false,\n            \"description\": \"Whether attribute is required\",\n            \"required\": true\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"schemaType\": [\n          \"fromAttributes\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"System Prompt Template\",\n        \"name\": \"systemPromptTemplate\",\n        \"type\": \"string\",\n        \"default\": \"You are an expert extraction algorithm.\\nOnly extract relevant information from the text.\\nIf you do not know the value of an attribute asked to extract, you may omit the attribute's value.\",\n        \"description\": \"String to use directly as the system prompt template\",\n        \"typeOptions\": {\n          \"rows\": 6\n        }\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 5,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.1\n                }\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatAnthropic",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Anthropic Chat Model",
    "description": "Language Model Anthropic",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Anthropic Chat Model node documentation\ndescription: Learn how to use the Anthropic Chat Model node in n8n. Follow technical documentation to integrate Anthropic Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Anthropic Chat Model node\n\nUse the Anthropic Chat Model node to use Anthropic's Claude family of chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Anthropic Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/anthropic.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model that generates the completion. Choose from:\n\t* **Claude**\n\t* **Claude Instant**\n\nLearn more in the [Anthropic model documentation](https://docs.anthropic.com/claude/reference/selecting-a-model){:target=_blank .external-link}.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'anthropic-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Anthropic documentation](https://js.langchain.com/docs/integrations/chat/anthropic/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"claude-2\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.anthropic.com/claude/docs/models-overview\\\">Learn more</a>.\",\n    \"options\": [\n      {\n        \"name\": \"Claude 3.5 Sonnet(20241022)\",\n        \"value\": \"claude-3-5-sonnet-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Opus(20240229)\",\n        \"value\": \"claude-3-opus-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Sonnet(20240620)\",\n        \"value\": \"claude-3-5-sonnet-20240620\"\n      },\n      {\n        \"name\": \"Claude 3 Sonnet(20240229)\",\n        \"value\": \"claude-3-sonnet-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Haiku(20241022)\",\n        \"value\": \"claude-3-5-haiku-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Haiku(20240307)\",\n        \"value\": \"claude-3-haiku-20240307\"\n      },\n      {\n        \"name\": \"LEGACY: Claude 2\",\n        \"value\": \"claude-2\"\n      },\n      {\n        \"name\": \"LEGACY: Claude 2.1\",\n        \"value\": \"claude-2.1\"\n      },\n      {\n        \"name\": \"LEGACY: Claude Instant 1.2\",\n        \"value\": \"claude-instant-1.2\"\n      },\n      {\n        \"name\": \"LEGACY: Claude Instant 1\",\n        \"value\": \"claude-instant-1\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"claude-3-sonnet-20240229\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.anthropic.com/claude/docs/models-overview\\\">Learn more</a>.\",\n    \"options\": [\n      {\n        \"name\": \"Claude 3.5 Sonnet(20241022)\",\n        \"value\": \"claude-3-5-sonnet-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Opus(20240229)\",\n        \"value\": \"claude-3-opus-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Sonnet(20240620)\",\n        \"value\": \"claude-3-5-sonnet-20240620\"\n      },\n      {\n        \"name\": \"Claude 3 Sonnet(20240229)\",\n        \"value\": \"claude-3-sonnet-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Haiku(20241022)\",\n        \"value\": \"claude-3-5-haiku-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Haiku(20240307)\",\n        \"value\": \"claude-3-haiku-20240307\"\n      },\n      {\n        \"name\": \"LEGACY: Claude 2\",\n        \"value\": \"claude-2\"\n      },\n      {\n        \"name\": \"LEGACY: Claude 2.1\",\n        \"value\": \"claude-2.1\"\n      },\n      {\n        \"name\": \"LEGACY: Claude Instant 1.2\",\n        \"value\": \"claude-instant-1.2\"\n      },\n      {\n        \"name\": \"LEGACY: Claude Instant 1\",\n        \"value\": \"claude-instant-1\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"claude-3-5-sonnet-20240620\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.anthropic.com/claude/docs/models-overview\\\">Learn more</a>.\",\n    \"options\": [\n      {\n        \"name\": \"Claude 3.5 Sonnet(20241022)\",\n        \"value\": \"claude-3-5-sonnet-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Opus(20240229)\",\n        \"value\": \"claude-3-opus-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Sonnet(20240620)\",\n        \"value\": \"claude-3-5-sonnet-20240620\"\n      },\n      {\n        \"name\": \"Claude 3 Sonnet(20240229)\",\n        \"value\": \"claude-3-sonnet-20240229\"\n      },\n      {\n        \"name\": \"Claude 3.5 Haiku(20241022)\",\n        \"value\": \"claude-3-5-haiku-20241022\"\n      },\n      {\n        \"name\": \"Claude 3 Haiku(20240307)\",\n        \"value\": \"claude-3-haiku-20240307\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"claude-sonnet-4-20250514\",\n      \"cachedResultName\": \"Claude 4 Sonnet\"\n    },\n    \"description\": \"The model. Choose from the list, or specify an ID. <a href=\\\"https://docs.anthropic.com/claude/docs/models-overview\\\">Learn more</a>.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokensToSample\",\n        \"default\": 4096,\n        \"description\": \"The maximum number of tokens to generate in the completion\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"thinking\": [\n              true\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": -1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": -1,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Used to remove \\\"long tail\\\" low probability responses. Defaults to -1, which disables it.\",\n        \"type\": \"number\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"thinking\": [\n              true\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"thinking\": [\n              true\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Enable Thinking\",\n        \"name\": \"thinking\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to enable thinking mode for the model\"\n      },\n      {\n        \"displayName\": \"Thinking Budget (Tokens)\",\n        \"name\": \"thinkingBudget\",\n        \"type\": \"number\",\n        \"default\": 1024,\n        \"description\": \"The maximum number of tokens to use for thinking\",\n        \"displayOptions\": {\n          \"show\": {\n            \"thinking\": [\n              true\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"anthropicApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatAwsBedrock",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "AWS Bedrock Chat Model",
    "description": "Language Model AWS Bedrock",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: AWS Bedrock Chat Model node documentation\ndescription: Learn how to use the AWS Bedrock Chat Model node in n8n. Follow technical documentation to integrate AWS Bedrock Chat Model node into your workflows.\ncontentType: [integration, reference]\n---\n\n# AWS Bedrock Chat Model node\n\nThe AWS Bedrock Chat Model node allows you use LLM models utilising AWS Bedrock platform.\n\nOn this page, you'll find the node parameters for the AWS Bedrock Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/aws.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\t\n## Node parameters\n\n* **Model**: Select the model that generates the completion.\n\nLearn more about available models in the [Amazon Bedrock model documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html){:target=_blank .external-link}.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'aws-bedrock-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's AWS Bedrock Chat Model documentation](https://js.langchain.com/docs/integrations/chat/bedrock/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.aws.amazon.com/bedrock/latest/userguide/foundation-models.html\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/foundation-models?&byOutputModality=TEXT&byInferenceType=ON_DEMAND\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"modelSummaries\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.modelName}}\",\n                  \"description\": \"={{$responseItem.modelArn}}\",\n                  \"value\": \"={{$responseItem.modelId}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokensToSample\",\n        \"default\": 2000,\n        \"description\": \"The maximum number of tokens to generate in the completion\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"aws\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatAzureOpenAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Azure OpenAI Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Azure OpenAI Chat Model node documentation\ndescription: Learn how to use the Azure OpenAI Chat Model node in n8n. Follow technical documentation to integrate Azure OpenAI Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Azure OpenAI Chat Model node\n\nUse the Azure OpenAI Chat Model node to use OpenAI's chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Azure OpenAI Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/azureopenai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the completion.\n\n## Node options\n\n* **Frequency Penalty**: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Response Format**: Choose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n* **Presence Penalty**: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Timeout**: Enter the maximum request time in milliseconds.\n* **Max Retries**: Enter the maximum number of times to retry a request.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'azure-openai-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Azure OpenAI documentation](https://js.langchain.com/docs/integrations/chat/azure){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Authentication\",\n    \"name\": \"authentication\",\n    \"type\": \"options\",\n    \"default\": \"azureOpenAiApi\",\n    \"options\": [\n      {\n        \"name\": \"API Key\",\n        \"value\": \"azureOpenAiApi\"\n      },\n      {\n        \"name\": \"Azure Entra ID (OAuth2)\",\n        \"value\": \"azureEntraCognitiveServicesOAuth2Api\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"If using JSON response format, you must include word \\\"json\\\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.responseFormat\": [\n          \"json_object\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model (Deployment) Name\",\n    \"name\": \"model\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the model(deployment) to use (e.g., gpt-4, gpt-35-turbo)\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768). Use -1 for default.\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 128000\n        }\n      },\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"responseFormat\",\n        \"default\": \"text\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Text\",\n            \"value\": \"text\",\n            \"description\": \"Regular text response\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json_object\",\n            \"description\": \"Enables JSON mode, which should guarantee the message the model generates is valid JSON\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout (Ms)\",\n        \"name\": \"timeout\",\n        \"default\": 60000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt on failure\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"azureOpenAiApi\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"azureOpenAiApi\"\n        ]\n      }\n    }\n  },\n  {\n    \"name\": \"azureEntraCognitiveServicesOAuth2Api\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"azureEntraCognitiveServicesOAuth2Api\"\n        ]\n      }\n    }\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatDeepSeek",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "DeepSeek Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: DeepSeek Chat Model node documentation\ndescription: Learn how to use the DeepSeek Chat Model node in n8n. Follow technical documentation to integrate DeepSeek Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# DeepSeek Chat Model node\n\nUse the DeepSeek Chat Model node to use DeepSeek's chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the DeepSeek Chat Model node and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/deepseek.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Model\n\nSelect the model to use to generate the completion.\n\nn8n dynamically loads models from DeepSeek and you'll only see the models available to your account.\n\n## Node options\n\nUse these options to further refine the node's behavior.\n\n### Base URL\n\nEnter a URL here to override the default URL for the API.\n\n### Frequency Penalty\n\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n\n### Maximum Number of Tokens\n\nEnter the maximum number of tokens used, which sets the completion length.\n\n### Response Format\n\nChoose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n\n### Presence Penalty\n\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n\n### Sampling Temperature\n\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n### Timeout\n\nEnter the maximum request time in milliseconds.\n\n### Max Retries\n\nEnter the maximum number of times to retry a request.\n\n### Top P\n\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'deepseek-chat-model') ]]\n\n## Related resources\n\nAs DeepSeek is API-compatible with OpenAI, you can refer to [LangChains's OpenAI documentation](https://js.langchain.com/docs/integrations/chat/openai/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"If using JSON response format, you must include word \\\"json\\\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.responseFormat\": [\n          \"json_object\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"deepseek-chat\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://api-docs.deepseek.com/quick_start/pricing\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"responseFormat\",\n        \"default\": \"text\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Text\",\n            \"value\": \"text\",\n            \"description\": \"Regular text response\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json_object\",\n            \"description\": \"Enables JSON mode, which should guarantee the message the model generates is valid JSON\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 360000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"deepSeekApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatGoogleGemini",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Google Gemini Chat Model",
    "description": "Chat Model Google Gemini",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Google Gemini Chat Model node documentation\ndescription: Learn how to use the Google Gemini Chat Model node in n8n. Follow technical documentation to integrate Google Gemini Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Google Gemini Chat Model node\n\nUse the Google Gemini Chat Model node to use Google's Gemini chat models with conversational agents.\n\nOn this page, you'll find the node parameters for the Google Gemini Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/googleai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the completion.\n\nn8n dynamically loads models from the Google Gemini API and you'll only see the models available to your account.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n* **Safety Settings**: Gemini supports adjustable safety settings. Refer to Google's [Gemini API safety settings](https://ai.google.dev/docs/safety_setting_gemini){:target=_blank .external-link} for information on the available filters and levels.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'google-gemini-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChain's Google Gemini documentation](https://js.langchain.com/docs/integrations/chat/google_generativeai){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelName\",\n    \"type\": \"options\",\n    \"default\": \"models/gemini-1.0-pro\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://developers.generativeai.google/api/rest/generativelanguage/models/list\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/v1beta/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"models\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ !$responseItem.name.includes('embedding') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.name}}\",\n                  \"description\": \"={{$responseItem.description}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxOutputTokens\",\n        \"default\": 2048,\n        \"description\": \"The maximum number of tokens to generate in the completion\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.4,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": 32,\n        \"typeOptions\": {\n          \"maxValue\": 40,\n          \"minValue\": -1,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Used to remove \\\"long tail\\\" low probability responses. Defaults to -1, which disables it.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Safety Settings\",\n        \"name\": \"safetySettings\",\n        \"type\": \"fixedCollection\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {\n          \"values\": {\n            \"category\": \"HARM_CATEGORY_HARASSMENT\",\n            \"threshold\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n          }\n        },\n        \"placeholder\": \"Add Option\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Values\",\n            \"values\": [\n              {\n                \"displayName\": \"Safety Category\",\n                \"name\": \"category\",\n                \"type\": \"options\",\n                \"description\": \"The category of harmful content to block\",\n                \"default\": \"HARM_CATEGORY_UNSPECIFIED\",\n                \"options\": [\n                  {\n                    \"value\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Harassment content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Hate speech and content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Sexually explicit content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Dangerous content\"\n                  }\n                ]\n              },\n              {\n                \"displayName\": \"Safety Threshold\",\n                \"name\": \"threshold\",\n                \"type\": \"options\",\n                \"description\": \"The threshold of harmful content to block\",\n                \"default\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                \"options\": [\n                  {\n                    \"value\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified\"\n                  },\n                  {\n                    \"value\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Content with NEGLIGIBLE will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Content with NEGLIGIBLE and LOW will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_ONLY_HIGH\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_NONE\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"All content will be allowed\"\n                  }\n                ]\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"googlePalmApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatGoogleVertex",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Google Vertex Chat Model",
    "description": "Chat Model Google Vertex",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Google Vertex Chat Model node documentation\ndescription: Learn how to use the Google Vertex Chat Model node in n8n. Follow technical documentation to integrate Google Vertex Chat Model node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Google Vertex Chat Model node\n\nUse the Google Vertex AI Chat Model node to use Google's Vertex AI chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Google Vertex AI Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/google/service-account.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Project ID**: Select the project ID from your Google Cloud account to use. n8n dynamically loads projects from the Google Cloud account, but you can also enter it manually.\n* **Model Name**: Select the name of the model to use to generate the completion, for example `gemini-1.5-flash-001`, `gemini-1.5-pro-001`, etc. Refer to [Google models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models){:target=_blank .external-link} for a list of available models.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n* **Safety Settings**: Gemini supports adjustable safety settings. Refer to Google's [Gemini API safety settings](https://ai.google.dev/docs/safety_setting_gemini){:target=_blank .external-link} for information on the available filters and levels.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'google-vertex-chat-model') ]]\n\n## Related resources\n\n\nRefer to [LangChain's Google Vertex AI documentation](https://js.langchain.com/docs/integrations/chat/google_vertex_ai/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Project ID\",\n    \"name\": \"projectId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"description\": \"Select or enter your Google Cloud project ID\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Model Name\",\n    \"name\": \"modelName\",\n    \"type\": \"string\",\n    \"default\": \"gemini-1.5-flash\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models\\\">Learn more</a>.\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxOutputTokens\",\n        \"default\": 2048,\n        \"description\": \"The maximum number of tokens to generate in the completion\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.4,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": 32,\n        \"typeOptions\": {\n          \"maxValue\": 40,\n          \"minValue\": -1,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Used to remove \\\"long tail\\\" low probability responses. Defaults to -1, which disables it.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Safety Settings\",\n        \"name\": \"safetySettings\",\n        \"type\": \"fixedCollection\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {\n          \"values\": {\n            \"category\": \"HARM_CATEGORY_HARASSMENT\",\n            \"threshold\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\"\n          }\n        },\n        \"placeholder\": \"Add Option\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Values\",\n            \"values\": [\n              {\n                \"displayName\": \"Safety Category\",\n                \"name\": \"category\",\n                \"type\": \"options\",\n                \"description\": \"The category of harmful content to block\",\n                \"default\": \"HARM_CATEGORY_UNSPECIFIED\",\n                \"options\": [\n                  {\n                    \"value\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"name\": \"HARM_CATEGORY_HARASSMENT\",\n                    \"description\": \"Harassment content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"name\": \"HARM_CATEGORY_HATE_SPEECH\",\n                    \"description\": \"Hate speech and content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"name\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n                    \"description\": \"Sexually explicit content\"\n                  },\n                  {\n                    \"value\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"name\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n                    \"description\": \"Dangerous content\"\n                  }\n                ]\n              },\n              {\n                \"displayName\": \"Safety Threshold\",\n                \"name\": \"threshold\",\n                \"type\": \"options\",\n                \"description\": \"The threshold of harmful content to block\",\n                \"default\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                \"options\": [\n                  {\n                    \"value\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"name\": \"HARM_BLOCK_THRESHOLD_UNSPECIFIED\",\n                    \"description\": \"Threshold is unspecified\"\n                  },\n                  {\n                    \"value\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"name\": \"BLOCK_LOW_AND_ABOVE\",\n                    \"description\": \"Content with NEGLIGIBLE will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"name\": \"BLOCK_MEDIUM_AND_ABOVE\",\n                    \"description\": \"Content with NEGLIGIBLE and LOW will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_ONLY_HIGH\",\n                    \"name\": \"BLOCK_ONLY_HIGH\",\n                    \"description\": \"Content with NEGLIGIBLE, LOW, and MEDIUM will be allowed\"\n                  },\n                  {\n                    \"value\": \"BLOCK_NONE\",\n                    \"name\": \"BLOCK_NONE\",\n                    \"description\": \"All content will be allowed\"\n                  }\n                ]\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"googleApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatGroq",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Groq Chat Model",
    "description": "Language Model Groq",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Groq Chat Model node documentation\ndescription: Learn how to use the Groq Chat Model node in n8n. Follow technical documentation to integrate Groq Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Groq Chat Model node\n\nUse the Groq Chat Model node to access Groq's large language models for conversational AI and text generation tasks.\n\nOn this page, you'll find the node parameters for the Groq Chat Model node, and links to more resources.\n\n/// note | Credentials \nYou can find authentication information for this node [here](/integrations/builtin/credentials/groq.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model which will generate the completion. n8n dynamically loads available models from the Groq API. Learn more in the [Groq model documentation](https://console.groq.com/docs/models){:target=_blank .external-link}.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'groq-chat-model') ]]\n\n## Related resources\n\nRefer to [Groq's API documentation](https://console.groq.com/docs/quickstart){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"llama3-8b-8192\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://console.groq.com/docs/models\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.active === true && $responseItem.object === \\\"model\\\" }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokensToSample\",\n        \"default\": 4096,\n        \"description\": \"The maximum number of tokens to generate in the completion\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"groqApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatMistralCloud",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Mistral Cloud Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Mistral Cloud Chat Model node documentation\ndescription: Learn how to use the Mistral Cloud Chat Model node in n8n. Follow technical documentation to integrate Mistral Cloud Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Mistral Cloud Chat Model node\n\nUse the Mistral Cloud Chat Model node to combine Mistral Cloud's chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Mistral Cloud Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/mistral.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the completion. n8n dynamically loads models from Mistral Cloud and you'll only see the models available to your account.\n\n## Node options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Timeout**: Enter the maximum request time in milliseconds.\n* **Max Retries**: Enter the maximum number of times to retry a request.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n* **Enable Safe Mode**: Enable safe mode by injecting a safety prompt at the beginning of the completion. This helps prevent the model from generating offensive content.\n* **Random Seed**: Enter a seed to use for random sampling. If set, different calls will generate deterministic results.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'mistral-cloud-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Mistral documentation](https://js.langchain.com/docs/integrations/chat/mistral){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"mistral-small\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.mistral.ai/platform/endpoints/\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ !$responseItem.id.includes('embed') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{ $responseItem.id }}\",\n                  \"value\": \"={{ $responseItem.id }}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Enable Safe Mode\",\n        \"name\": \"safeMode\",\n        \"default\": false,\n        \"type\": \"boolean\",\n        \"description\": \"Whether to inject a safety prompt before all conversations\"\n      },\n      {\n        \"displayName\": \"Random Seed\",\n        \"name\": \"randomSeed\",\n        \"type\": \"number\",\n        \"description\": \"The seed to use for random sampling. If set, different calls will generate deterministic results.\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"mistralCloudApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatOllama",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Ollama Chat Model",
    "description": "Language Model Ollama",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Ollama Chat Model node documentation\ndescription: Learn how to use the Ollama Chat Model node in n8n. Follow technical documentation to integrate Ollama Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Ollama Chat Model node\n\nThe Ollama Chat Model node allows you use local Llama 2 models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Ollama Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/ollama.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model that generates the completion. Choose from:\n\t* **Llama2**\n\t* **Llama2 13B**\n\t* **Llama2 70B**\n\t* **Llama2 Uncensored**\n\nRefer to the Ollama [Models Library documentation](https://ollama.com/library){:target=_blank .external-link} for more information about available models.\n\n## Node options\n\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'ollama-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Ollama Chat Model documentation](https://js.langchain.com/docs/integrations/chat/ollama/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"llama3.2\",\n    \"description\": \"The model which will generate the completion. To download models, visit <a href=\\\"https://ollama.ai/library\\\">Ollama Models Library</a>.\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/api/tags\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"models\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.name}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": -1,\n        \"typeOptions\": {\n          \"maxValue\": 100,\n          \"minValue\": -1,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"minValue\": 0\n        },\n        \"description\": \"Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition.\"\n      },\n      {\n        \"displayName\": \"Keep Alive\",\n        \"name\": \"keepAlive\",\n        \"type\": \"string\",\n        \"default\": \"5m\",\n        \"description\": \"Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes).\"\n      },\n      {\n        \"displayName\": \"Low VRAM Mode\",\n        \"name\": \"lowVram\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory.\"\n      },\n      {\n        \"displayName\": \"Main GPU ID\",\n        \"name\": \"mainGpu\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs.\"\n      },\n      {\n        \"displayName\": \"Context Batch Size\",\n        \"name\": \"numBatch\",\n        \"type\": \"number\",\n        \"default\": 512,\n        \"description\": \"Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage.\"\n      },\n      {\n        \"displayName\": \"Context Length\",\n        \"name\": \"numCtx\",\n        \"type\": \"number\",\n        \"default\": 2048,\n        \"description\": \"The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model.\"\n      },\n      {\n        \"displayName\": \"Number of GPUs\",\n        \"name\": \"numGpu\",\n        \"type\": \"number\",\n        \"default\": -1,\n        \"description\": \"Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection.\"\n      },\n      {\n        \"displayName\": \"Max Tokens to Generate\",\n        \"name\": \"numPredict\",\n        \"type\": \"number\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs.\"\n      },\n      {\n        \"displayName\": \"Number of CPU Threads\",\n        \"name\": \"numThread\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection.\"\n      },\n      {\n        \"displayName\": \"Penalize Newlines\",\n        \"name\": \"penalizeNewline\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text\"\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity.\"\n      },\n      {\n        \"displayName\": \"Repetition Penalty\",\n        \"name\": \"repeatPenalty\",\n        \"type\": \"number\",\n        \"default\": 1,\n        \"description\": \"Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty.\"\n      },\n      {\n        \"displayName\": \"Use Memory Locking\",\n        \"name\": \"useMLock\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory.\"\n      },\n      {\n        \"displayName\": \"Use Memory Mapping\",\n        \"name\": \"useMMap\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled.\"\n      },\n      {\n        \"displayName\": \"Load Vocabulary Only\",\n        \"name\": \"vocabOnly\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization.\"\n      },\n      {\n        \"displayName\": \"Output Format\",\n        \"name\": \"format\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Default\",\n            \"value\": \"default\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json\"\n          }\n        ],\n        \"default\": \"default\",\n        \"description\": \"Specifies the format of the API response\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"ollamaApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatOpenAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "OpenAI Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.2",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: OpenAI Chat Model node documentation\ndescription: Learn how to use the OpenAI Chat Model node in n8n. Follow technical documentation to integrate OpenAI Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# OpenAI Chat Model node\n\nUse the OpenAI Chat Model node to use OpenAI's chat models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the OpenAI Chat Model node and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/openai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Model\n\nSelect the model to use to generate the completion.\n\nn8n dynamically loads models from OpenAI and you'll only see the models available to your account.\n\n## Node options\n\nUse these options to further refine the node's behavior.\n\n### Base URL\n\nEnter a URL here to override the default URL for the API.\n\n### Frequency Penalty\n\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n\n### Maximum Number of Tokens\n\nEnter the maximum number of tokens used, which sets the completion length.\n\n### Response Format\n\nChoose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n\n### Presence Penalty\n\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n\n### Sampling Temperature\n\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n### Timeout\n\nEnter the maximum request time in milliseconds.\n\n### Max Retries\n\nEnter the maximum number of times to retry a request.\n\n### Top P\n\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'openai-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's OpenAI documentation](https://js.langchain.com/docs/integrations/chat/openai/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatopenai/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"If using JSON response format, you must include word \\\"json\\\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.responseFormat\": [\n          \"json_object\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"gpt-4o-mini\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://beta.openai.com/docs/models/overview\\\">Learn more</a>.\",\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"={{ $parameter.options?.baseURL?.split(\\\"/\\\").slice(-1).pop() || $credentials?.url?.split(\\\"/\\\").slice(-1).pop() || \\\"v1\\\" }}/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t($parameter.options?.baseURL && !$parameter.options?.baseURL?.startsWith('https://api.openai.com/')) ||\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t($credentials?.url && !$credentials.url.startsWith('https://api.openai.com/')) ||\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t$responseItem.id.startsWith('ft:') ||\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t$responseItem.id.startsWith('o1') ||\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t$responseItem.id.startsWith('o3') ||\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t($responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct'))\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t}}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"gpt-4.1-mini\"\n    },\n    \"description\": \"The model. Choose from the list, or specify an ID.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"When using non-OpenAI models via \\\"Base URL\\\" override, not all models might be chat-compatible or support other features, like tools calling or JSON response format\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.baseURL\": [\n          {\n            \"_cnd\": {\n              \"exists\": true\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Base URL\",\n        \"name\": \"baseURL\",\n        \"default\": \"https://api.openai.com/v1\",\n        \"description\": \"Override the default base URL for the API\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.1\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"responseFormat\",\n        \"default\": \"text\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Text\",\n            \"value\": \"text\",\n            \"description\": \"Regular text response\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json_object\",\n            \"description\": \"Enables JSON mode, which should guarantee the message the model generates is valid JSON\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Reasoning Effort\",\n        \"name\": \"reasoningEffort\",\n        \"default\": \"medium\",\n        \"description\": \"Controls the amount of reasoning tokens to use. A value of \\\"low\\\" will favor speed and economical token usage, \\\"high\\\" will favor more complete reasoning at the cost of more tokens generated and slower responses.\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Low\",\n            \"value\": \"low\",\n            \"description\": \"Favors speed and economical token usage\"\n          },\n          {\n            \"name\": \"Medium\",\n            \"value\": \"medium\",\n            \"description\": \"Balance between speed and reasoning accuracy\"\n          },\n          {\n            \"name\": \"High\",\n            \"value\": \"high\",\n            \"description\": \"Favors more complete reasoning at the cost of more tokens generated and slower responses\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              {\n                \"_cnd\": {\n                  \"regex\": \"(^o1([-\\\\d]+)?$)|(^o[3-9].*)\"\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 60000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"openAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatOpenRouter",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "OpenRouter Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: OpenRouter Chat Model node documentation\ndescription: Learn how to use the OpenRouter Chat Model node in n8n. Follow technical documentation to integrate OpenRouter Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# OpenRouter Chat Model node\n\nUse the OpenRouter Chat Model node to use OpenRouter's chat models with conversational agents.\n\nOn this page, you'll find the node parameters for the OpenRouter Chat Model node and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/openrouter.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Model\n\nSelect the model to use to generate the completion.\n\nn8n dynamically loads models from OpenRouter and you'll only see the models available to your account.\n\n## Node options\n\nUse these options to further refine the node's behavior.\n\n### Frequency Penalty\n\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n\n### Maximum Number of Tokens\n\nEnter the maximum number of tokens used, which sets the completion length.\n\n### Response Format\n\nChoose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n\n### Presence Penalty\n\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n\n### Sampling Temperature\n\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n### Timeout\n\nEnter the maximum request time in milliseconds.\n\n### Max Retries\n\nEnter the maximum number of times to retry a request.\n\n### Top P\n\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'openrouter-chat-model') ]]\n\n## Related resources\n\nAs OpenRouter is API-compatible with OpenAI, you can refer to [LangChains's OpenAI documentation](https://js.langchain.com/docs/integrations/chat/openai/) for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"If using JSON response format, you must include word \\\"json\\\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.responseFormat\": [\n          \"json_object\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"openai/gpt-4.1-mini\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://openrouter.ai/docs/models\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"responseFormat\",\n        \"default\": \"text\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Text\",\n            \"value\": \"text\",\n            \"description\": \"Regular text response\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json_object\",\n            \"description\": \"Enables JSON mode, which should guarantee the message the model generates is valid JSON\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 360000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"openRouterApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmChatXAiGrok",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "xAI Grok Chat Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: xAI Grok Chat Model node documentation\ndescription: Learn how to use the xAI Grok Chat Model node in n8n. Follow technical documentation to integrate xAI Grok Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# xAI Grok Chat Model node\n\nUse the xAI Grok Chat Model node to access xAI Grok's large language models for conversational AI and text generation tasks.\n\nOn this page, you'll find the node parameters for the xAI Grok Chat Model node, and links to more resources.\n\n/// note | Credentials \nYou can find authentication information for this node [here](/integrations/builtin/credentials/xai.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model which will generate the completion. n8n dynamically loads available models from the xAI Grok API. Learn more in the [xAI Grok model documentation](https://docs.x.ai/docs/models).\n\n## Node options\n\n* **Frequency Penalty**: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length. Most models have a context length of 2048 tokens with the newest models supporting up to 32,768 tokens. \n* **Response Format**: Choose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n* **Presence Penalty**: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Timeout**: Enter the maximum request time in milliseconds.\n* **Max Retries**: Enter the maximum number of times to retry a request.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'xai-grok-chat-model') ]]\n\n## Related resources\n\nRefer to [xAI Grok's API documentation](https://docs.x.ai/docs/api-reference) for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"If using JSON response format, you must include word \\\"json\\\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.responseFormat\": [\n          \"json_object\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"grok-2-vision-1212\",\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://docs.x.ai/docs/models\\\">Learn more</a>.\",\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"responseFormat\",\n        \"default\": \"text\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Text\",\n            \"value\": \"text\",\n            \"description\": \"Regular text response\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json_object\",\n            \"description\": \"Enables JSON mode, which should guarantee the message the model generates is valid JSON\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 360000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"xAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmCohere",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Cohere Model",
    "description": "Language Model Cohere",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Cohere Model node documentation\ndescription: Learn how to use the Cohere Model node in n8n. Follow technical documentation to integrate Cohere Model node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Cohere Model node\n\nUse the Cohere Model node to use Cohere's models.\n\nOn this page, you'll find the node parameters for the Cohere Model node, and links to more resources.\n\nThis node lacks tools support, so it won't work with the [AI Agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) node. Instead, connect it with the [Basic LLM Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm.md) node.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/cohere.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node Options\n\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'cohere-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Cohere documentation](https://js.langchain.com/docs/integrations/llms/cohere/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": 250,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Model\",\n        \"name\": \"model\",\n        \"type\": \"string\",\n        \"description\": \"The name of the model to use\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"cohereApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmOllama",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Ollama Model",
    "description": "Language Model Ollama",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Ollama Model node documentation\ndescription: Learn how to use the Ollama Model node in n8n. Follow technical documentation to integrate Ollama Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Ollama Model node\n\nThe Ollama Model node allows you use local Llama 2 models.\n\nOn this page, you'll find the node parameters for the Ollama Model node, and links to more resources.\n\nThis node lacks tools support, so it won't work with the [AI Agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) node. Instead, connect it with the [Basic LLM Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm.md) node.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/ollama.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model that generates the completion. Choose from:\n\t* **Llama2**\n\t* **Llama2 13B**\n\t* **Llama2 70B**\n\t* **Llama2 Uncensored**\n\nRefer to the Ollama [Models Library documentation](https://ollama.com/library){:target=_blank .external-link} for more information about available models.\n\n## Node options\n\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'ollama-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Ollama documentation](https://js.langchain.com/docs/integrations/llms/ollama/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmollama/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"llama3.2\",\n    \"description\": \"The model which will generate the completion. To download models, visit <a href=\\\"https://ollama.ai/library\\\">Ollama Models Library</a>.\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"/api/tags\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"models\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.name}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": -1,\n        \"typeOptions\": {\n          \"maxValue\": 100,\n          \"minValue\": -1,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"minValue\": 0\n        },\n        \"description\": \"Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition.\"\n      },\n      {\n        \"displayName\": \"Keep Alive\",\n        \"name\": \"keepAlive\",\n        \"type\": \"string\",\n        \"default\": \"5m\",\n        \"description\": \"Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes).\"\n      },\n      {\n        \"displayName\": \"Low VRAM Mode\",\n        \"name\": \"lowVram\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory.\"\n      },\n      {\n        \"displayName\": \"Main GPU ID\",\n        \"name\": \"mainGpu\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs.\"\n      },\n      {\n        \"displayName\": \"Context Batch Size\",\n        \"name\": \"numBatch\",\n        \"type\": \"number\",\n        \"default\": 512,\n        \"description\": \"Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage.\"\n      },\n      {\n        \"displayName\": \"Context Length\",\n        \"name\": \"numCtx\",\n        \"type\": \"number\",\n        \"default\": 2048,\n        \"description\": \"The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model.\"\n      },\n      {\n        \"displayName\": \"Number of GPUs\",\n        \"name\": \"numGpu\",\n        \"type\": \"number\",\n        \"default\": -1,\n        \"description\": \"Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection.\"\n      },\n      {\n        \"displayName\": \"Max Tokens to Generate\",\n        \"name\": \"numPredict\",\n        \"type\": \"number\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs.\"\n      },\n      {\n        \"displayName\": \"Number of CPU Threads\",\n        \"name\": \"numThread\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection.\"\n      },\n      {\n        \"displayName\": \"Penalize Newlines\",\n        \"name\": \"penalizeNewline\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text\"\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"description\": \"Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity.\"\n      },\n      {\n        \"displayName\": \"Repetition Penalty\",\n        \"name\": \"repeatPenalty\",\n        \"type\": \"number\",\n        \"default\": 1,\n        \"description\": \"Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty.\"\n      },\n      {\n        \"displayName\": \"Use Memory Locking\",\n        \"name\": \"useMLock\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory.\"\n      },\n      {\n        \"displayName\": \"Use Memory Mapping\",\n        \"name\": \"useMMap\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled.\"\n      },\n      {\n        \"displayName\": \"Load Vocabulary Only\",\n        \"name\": \"vocabOnly\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization.\"\n      },\n      {\n        \"displayName\": \"Output Format\",\n        \"name\": \"format\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Default\",\n            \"value\": \"default\"\n          },\n          {\n            \"name\": \"JSON\",\n            \"value\": \"json\"\n          }\n        ],\n        \"default\": \"default\",\n        \"description\": \"Specifies the format of the API response\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"ollamaApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmOpenAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "OpenAI Model",
    "description": "For advanced usage with an AI chain",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node is using OpenAI completions which are now deprecated. Please use the OpenAI Chat Model node instead.\",\n    \"name\": \"deprecated\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"gpt-3.5-turbo-instruct\"\n    },\n    \"description\": \"The model which will generate the completion. <a href=\\\"https://beta.openai.com/docs/models/overview\\\">Learn more</a>.\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"When using non OpenAI models via Base URL override, not all models might be chat-compatible or support other features, like tools calling or JSON response format.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.baseURL\": [\n          {\n            \"_cnd\": {\n              \"exists\": true\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Base URL\",\n        \"name\": \"baseURL\",\n        \"default\": \"https://api.openai.com/v1\",\n        \"description\": \"Override the default base URL for the API\",\n        \"type\": \"string\"\n      },\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": -1,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 0.7,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 60000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"openAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.lmOpenHuggingFaceInference",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Hugging Face Inference Model",
    "description": "Language Model HuggingFaceInference",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Hugging Face Inference Model node documentation\ndescription: Learn how to use the Hugging Face Inference Model node in n8n. Follow technical documentation to integrate Hugging Face Inference Model node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Hugging Face Inference Model node\n\nUse the Hugging Face Inference Model node to use Hugging Face's models.\n\nOn this page, you'll find the node parameters for the Hugging Face Inference Model node, and links to more resources.\n\nThis node lacks tools support, so it won't work with the [AI Agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) node. Instead, connect it with the [Basic LLM Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainllm.md) node.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/huggingface.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model to use to generate the completion.\n\n## Node options\n\n* **Custom Inference Endpoint**: Enter a custom inference endpoint URL.\n* **Frequency Penalty**: Use this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n* **Maximum Number of Tokens**: Enter the maximum number of tokens used, which sets the completion length.\n* **Presence Penalty**: Use this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'hugging-face-inference-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Hugging Face Inference Model documentation](https://js.langchain.com/docs/integrations/llms/huggingface_inference/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"string\",\n    \"default\": \"gpt2\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Custom Inference Endpoint\",\n        \"name\": \"endpointUrl\",\n        \"default\": \"\",\n        \"description\": \"Custom endpoint URL\",\n        \"type\": \"string\"\n      },\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequencyPenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": 128,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presencePenalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Sampling Temperature\",\n        \"name\": \"temperature\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top K\",\n        \"name\": \"topK\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls the top tokens to consider within the sample operation to create new text\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Top P\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"huggingFaceApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.manualChatTrigger",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Manual Chat Trigger",
    "description": "Runs the flow on new manual chat message",
    "category": "trigger",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 1,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node is where a manual chat workflow execution starts. To make one, go back to the canvas and click ‘Chat’\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Chat and execute workflow\",\n    \"name\": \"openChat\",\n    \"type\": \"button\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"buttonConfig\": {\n        \"action\": \"openChat\"\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.mcpClientTool",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "MCP Client Tool",
    "description": "Connect tools from an MCP Server",
    "category": "output",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"SSE Endpoint\",\n    \"name\": \"sseEndpoint\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"SSE Endpoint of your MCP server\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Authentication\",\n    \"name\": \"authentication\",\n    \"type\": \"options\",\n    \"default\": \"none\",\n    \"description\": \"The way to authenticate with your SSE endpoint\",\n    \"options\": [\n      {\n        \"name\": \"Bearer Auth\",\n        \"value\": \"bearerAuth\"\n      },\n      {\n        \"name\": \"Header Auth\",\n        \"value\": \"headerAuth\"\n      },\n      {\n        \"name\": \"None\",\n        \"value\": \"none\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Credentials\",\n    \"name\": \"credentials\",\n    \"type\": \"credentials\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"headerAuth\",\n          \"bearerAuth\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Tools to Include\",\n    \"name\": \"include\",\n    \"type\": \"options\",\n    \"default\": \"all\",\n    \"description\": \"How to select the tools you want to be exposed to the AI Agent\",\n    \"options\": [\n      {\n        \"name\": \"All\",\n        \"value\": \"all\",\n        \"description\": \"Also include all unchanged fields from the input\"\n      },\n      {\n        \"name\": \"Selected\",\n        \"value\": \"selected\",\n        \"description\": \"Also include the tools listed in the parameter \\\"Tools to Include\\\"\"\n      },\n      {\n        \"name\": \"All Except\",\n        \"value\": \"except\",\n        \"description\": \"Exclude the tools listed in the parameter \\\"Tools to Exclude\\\"\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Tools to Include\",\n    \"name\": \"includeTools\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"description\": \"Choose from the list, or specify IDs using an <a href=\\\"https://docs.n8n.io/code/expressions/\\\">expression</a>\",\n    \"displayOptions\": {\n      \"show\": {\n        \"include\": [\n          \"selected\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsMethod\": \"getTools\",\n      \"loadOptionsDependsOn\": [\n        \"sseEndpoint\"\n      ]\n    }\n  },\n  {\n    \"displayName\": \"Tools to Exclude\",\n    \"name\": \"excludeTools\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"description\": \"Choose from the list, or specify IDs using an <a href=\\\"https://docs.n8n.io/code/expressions/\\\">expression</a>\",\n    \"displayOptions\": {\n      \"show\": {\n        \"include\": [\n          \"except\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsMethod\": \"getTools\"\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"httpBearerAuth\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"bearerAuth\"\n        ]\n      }\n    }\n  },\n  {\n    \"name\": \"httpHeaderAuth\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"headerAuth\"\n        ]\n      }\n    }\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.mcpTrigger",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "MCP Server Trigger",
    "description": "Expose n8n tools as an MCP Server endpoint",
    "category": "trigger",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 1,
    "is_webhook": 1,
    "is_versioned": 1,
    "version": "2",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Authentication\",\n    \"name\": \"authentication\",\n    \"type\": \"options\",\n    \"default\": \"none\",\n    \"description\": \"The way to authenticate\",\n    \"options\": [\n      {\n        \"name\": \"None\",\n        \"value\": \"none\"\n      },\n      {\n        \"name\": \"Bearer Auth\",\n        \"value\": \"bearerAuth\"\n      },\n      {\n        \"name\": \"Header Auth\",\n        \"value\": \"headerAuth\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Path\",\n    \"name\": \"path\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The base path for this MCP server\",\n    \"required\": true\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"httpBearerAuth\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"bearerAuth\"\n        ]\n      }\n    }\n  },\n  {\n    \"name\": \"httpHeaderAuth\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"headerAuth\"\n        ]\n      }\n    }\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryBufferWindow",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Simple Memory",
    "description": "Stores in n8n memory, so no credentials required",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Simple Memory node documentation\ndescription: Learn how to use the Simple Memory node in n8n. Follow technical documentation to integrate Simple Memory node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Simple Memory node\n\nUse the Simple Memory node to [persist](/glossary.md#ai-memory) chat history in your workflow.\n\nOn this page, you'll find a list of operations the Simple Memory node supports, and links to more resources.\n\n/// warning | Don't use this node if running n8n in queue mode\nIf your n8n instance uses [queue mode](/hosting/scaling/queue-mode.md), this node doesn't work in an active production workflow. This is because n8n can't guarantee that every call to Simple Memory will go to the same worker.\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\nConfigure these parameters to configure the node:\n\n* **Session Key**: Enter the key to use to store the memory in the workflow data.\n* **Context Window Length**: Enter the number of previous interactions to consider for context.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'window-buffer-memory') ]]\n\n## Related resources\n\nRefer to [LangChain's Buffer Window Memory documentation](https://v03.api.js.langchain.com/classes/langchain.memory.BufferWindowMemory.html){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"chat_history\",\n    \"description\": \"The key to use to store the memory in the workflow data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"description\": \"The key to use to store the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Context Window Length\",\n    \"name\": \"contextWindowLength\",\n    \"type\": \"number\",\n    \"default\": 5\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryChatRetriever",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Chat Messages Retriever",
    "description": "Retrieve chat messages from memory and use them in the workflow",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node is deprecated. Use 'Chat Memory Manager' node instead.\",\n    \"name\": \"deprecatedNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplifyOutput\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether to simplify the output to only include the sender and the text\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryManager",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Chat Memory Manager",
    "description": "Manage chat messages memory and use it in the workflow",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Chat Memory Manager node documentation\ndescription: Learn how to use the Chat Memory Manager node in n8n. Follow technical documentation to integrate Chat Memory Manager node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Chat Memory Manager node\n\nThe Chat Memory Manager node manages chat message [memories](/glossary.md#ai-memory) within your workflows. Use this node to load, insert, and delete chat messages in an in-memory [vector store](/glossary.md#ai-vector-store).\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-sub-nodes/chat-memory-manager-purpose.md\"\n\nOn this page, you'll find a list of operations that the Chat Memory Manager node supports, along with links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Operation Mode**: Choose between **Get Many Messages**, **Insert Messages**, and **Delete Messages** operations.\n* **Insert Mode**: Available in **Insert Messages** mode. Choose from:\n    * **Insert Messages**: Insert messages alongside existing messages.\n    * **Override All Messages**: Replace current memory.\n* **Delete Mode**: available in **Delete Messages** mode. Choose from:\n    * **Last N**: Delete the last N messages.\n    * **All Messages**: Delete messages from memory.\n* **Chat Messages**: available in **Insert Messages** mode. Define the chat messages to insert into the memory, including:\n\t* **Type Name or ID**: Set the message type. Select one of:\n\t\t* **AI**: Use this for messages from the AI.\n\t\t* **System**: Add a message containing instructions for the AI.\n\t\t* **User**: Use this for messages from the user. This message type is sometimes called the 'human' message in other AI tools and guides.\n\t* **Message**: Enter the message contents.\n\t* **Hide Message in Chat**: Select whether n8n should display the message to the user in the chat UI (turned off) or not (turned on).\n* **Messages Count**: Available in **Delete Messages** mode when you select **Last N**. Enter the number of latest messages to delete.\n* **Simplify Output**: Available in **Get Many Messages** mode. Turn on to simplify the output to include only the sender (AI, user, or system) and the text.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'chat-memory-manager') ]]\n\n## Related resources\n\nRefer to [LangChain's Memory documentation](https://langchain-ai.github.io/langgraphjs/concepts/memory/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"load\",\n    \"options\": [\n      {\n        \"name\": \"Get Many Messages\",\n        \"description\": \"Retrieve chat messages from connected memory\",\n        \"value\": \"load\"\n      },\n      {\n        \"name\": \"Insert Messages\",\n        \"description\": \"Insert chat messages into connected memory\",\n        \"value\": \"insert\"\n      },\n      {\n        \"name\": \"Delete Messages\",\n        \"description\": \"Delete chat messages from connected memory\",\n        \"value\": \"delete\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Insert Mode\",\n    \"name\": \"insertMode\",\n    \"type\": \"options\",\n    \"default\": \"insert\",\n    \"description\": \"Choose how new messages are inserted into the memory\",\n    \"options\": [\n      {\n        \"name\": \"Insert Messages\",\n        \"value\": \"insert\",\n        \"description\": \"Add messages alongside existing ones\"\n      },\n      {\n        \"name\": \"Override All Messages\",\n        \"value\": \"override\",\n        \"description\": \"Replace the current memory with new messages\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Delete Mode\",\n    \"name\": \"deleteMode\",\n    \"type\": \"options\",\n    \"default\": \"lastN\",\n    \"description\": \"How messages are deleted from memory\",\n    \"options\": [\n      {\n        \"name\": \"Last N\",\n        \"value\": \"lastN\",\n        \"description\": \"Delete the last N messages\"\n      },\n      {\n        \"name\": \"All Messages\",\n        \"value\": \"all\",\n        \"description\": \"Clear all messages from memory\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"delete\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Chat Messages\",\n    \"name\": \"messages\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"description\": \"Chat messages to insert into memory\",\n    \"options\": [\n      {\n        \"name\": \"messageValues\",\n        \"displayName\": \"Message\",\n        \"values\": [\n          {\n            \"displayName\": \"Type Name or ID\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"AI\",\n                \"value\": \"ai\"\n              },\n              {\n                \"name\": \"System\",\n                \"value\": \"system\"\n              },\n              {\n                \"name\": \"User\",\n                \"value\": \"user\"\n              }\n            ],\n            \"default\": \"system\"\n          },\n          {\n            \"displayName\": \"Message\",\n            \"name\": \"message\",\n            \"type\": \"string\",\n            \"required\": true,\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Hide Message in Chat\",\n            \"name\": \"hideFromUI\",\n            \"type\": \"boolean\",\n            \"required\": true,\n            \"default\": false,\n            \"description\": \"Whether to hide the message from the chat UI\"\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Messages Count\",\n    \"name\": \"lastMessagesCount\",\n    \"type\": \"number\",\n    \"default\": 2,\n    \"description\": \"The amount of last messages to delete\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"delete\"\n        ],\n        \"deleteMode\": [\n          \"lastN\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplifyOutput\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether to simplify the output to only include the sender and the text\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Group Messages\",\n        \"name\": \"groupMessages\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to group messages into a single item or return each message as a separate item\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryMongoDbChat",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "MongoDB Chat Memory",
    "description": "Stores the chat history in MongoDB collection.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Collection Name\",\n    \"name\": \"collectionName\",\n    \"type\": \"string\",\n    \"default\": \"n8n_chat_histories\",\n    \"description\": \"The collection name to store the chat history in. If collection does not exist, it will be created.\"\n  },\n  {\n    \"displayName\": \"Database Name\",\n    \"name\": \"databaseName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The database name to store the chat history in. If not provided, the database from credentials will be used.\"\n  },\n  {\n    \"displayName\": \"Context Window Length\",\n    \"name\": \"contextWindowLength\",\n    \"type\": \"number\",\n    \"default\": 5\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"mongoDb\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryMotorhead",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Motorhead",
    "description": "Use Motorhead Memory",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Motorhead node documentation\ndescription: Learn how to use the Motorhead node in n8n. Follow technical documentation to integrate Motorhead node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Motorhead node\n\nUse the Motorhead node to use Motorhead as a [memory](/glossary.md#ai-memory) server.\n\nOn this page, you'll find a list of operations the Motorhead node supports, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/motorhead.md).\n///\n\n## Node parameters\n\n* **Session ID**: Enter the ID to use to store the memory in the workflow data.\n\n## Node reference\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'motorhead') ]]\n\n## Related resources\n\nRefer to [LangChain's Motorhead documentation](https://js.langchain.com/docs/integrations/memory/motorhead_memory){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Single memory instance\n\n[[% include \"_includes/integrations/cluster-nodes/memory-shared.html\" %]]\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"description\": \"The key to use to store the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"motorheadApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryPostgresChat",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Postgres Chat Memory",
    "description": "Stores the chat history in Postgres table.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Postgres Chat Memory node documentation\ndescription: Learn how to use the Postgres Chat Memory node in n8n. Follow technical documentation to integrate Postgres Chat Memory node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Postgres Chat Memory node\n\nUse the Postgres Chat Memory node to use Postgres as a [memory](/glossary.md#ai-memory) server for storing chat history.\n\nOn this page, you'll find a list of operations the Postgres Chat Memory node supports, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/postgres.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Session Key**: Enter the key to use to store the memory in the workflow data.\n* **Table Name**: Enter the name of the table to store the chat history in. The system will create the table if doesn't exist.\n* **Context Window Length**: Enter the number of previous interactions to consider for context.\n\n## Related resources\n\nRefer to [LangChain's Postgres Chat Message History documentation](https://js.langchain.com/docs/integrations/memory/postgres){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Single memory instance\n\n[[% include \"_includes/integrations/cluster-nodes/memory-shared.html\" %]]\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Table Name\",\n    \"name\": \"tableName\",\n    \"type\": \"string\",\n    \"default\": \"n8n_chat_histories\",\n    \"description\": \"The table name to store the chat history in. If table does not exist, it will be created.\"\n  },\n  {\n    \"displayName\": \"Context Window Length\",\n    \"name\": \"contextWindowLength\",\n    \"type\": \"number\",\n    \"default\": 5,\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lt\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"postgres\",\n    \"required\": true,\n    \"testedBy\": \"postgresConnectionTest\"\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryRedisChat",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Redis Chat Memory",
    "description": "Stores the chat history in Redis.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.5",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Redis Chat Memory node documentation\ndescription: Learn how to use the Redis Chat Memory node in n8n. Follow technical documentation to integrate Redis Chat Memory node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Redis Chat Memory node\n\nUse the Redis Chat Memory node to use Redis as a [memory](/glossary.md#ai-memory) server.\n\nOn this page, you'll find a list of operations the Redis Chat Memory node supports, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/redis.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Session Key**: Enter the key to use to store the memory in the workflow data.\n* **Session Time To Live**: Use this parameter to make the session expire after a given number of seconds.\n* **Context Window Length**: Enter the number of previous interactions to consider for context.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'redis-chat-memory') ]]\n\n## Related resources\n\nRefer to [LangChain's Redis Chat Memory documentation](https://js.langchain.com/docs/integrations/memory/redis){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Single memory instance\n\n[[% include \"_includes/integrations/cluster-nodes/memory-shared.html\" %]]\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"chat_history\",\n    \"description\": \"The key to use to store the memory in the workflow data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"description\": \"The key to use to store the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.4\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Time To Live\",\n    \"name\": \"sessionTTL\",\n    \"type\": \"number\",\n    \"default\": 0,\n    \"description\": \"For how long the session should be stored in seconds. If set to 0 it will not expire.\"\n  },\n  {\n    \"displayName\": \"Context Window Length\",\n    \"name\": \"contextWindowLength\",\n    \"type\": \"number\",\n    \"default\": 5,\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lt\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"redis\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryXata",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Xata",
    "description": "Use Xata Memory",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.4",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Xata node documentation\ndescription: Learn how to use the Xata node in n8n. Follow technical documentation to integrate Xata node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Xata node\n\nUse the Xata node to use Xata as a [memory](/glossary.md#ai-memory) server.\nOn this page, you'll find a list of operations the Xata node supports, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/xata.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n-   **Session ID**: Enter the ID to use to store the memory in the workflow data.\n-   **Context Window Length**: Enter the number of previous interactions to consider for context.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'xata') ]]\n\n## Related resources\n\nRefer to [LangChain's Xata documentation](https://js.langchain.com/docs/integrations/memory/xata){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Single memory instance\n\n[[% include \"_includes/integrations/cluster-nodes/memory-shared.html\" %]]\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"description\": \"The key to use to store the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.4\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Context Window Length\",\n    \"name\": \"contextWindowLength\",\n    \"type\": \"number\",\n    \"default\": 5,\n    \"displayOptions\": {\n      \"hide\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lt\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"xataApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.memoryZep",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Zep",
    "description": "Use Zep Memory",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Zep node documentation\ndescription: Learn how to use the Zep node in n8n. Follow technical documentation to integrate Zep node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Zep node\n\nUse the Zep node to use Zep as a [memory](/glossary.md#ai-memory) server.\n\nOn this page, you'll find a list of operations the Zep node supports, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/zep.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Session ID**: Enter the ID to use to store the memory in the workflow data.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'zep') ]]\n\n## Related resources\n\nRefer to [LangChain's Zep documentation](https://js.langchain.com/docs/integrations/memory/zep_memory){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Single memory instance\n\n[[% include \"_includes/integrations/cluster-nodes/memory-shared.html\" %]]\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Only works with Zep Cloud and Community edition <= v0.27.2\",\n    \"name\": \"supportedVersions\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionId\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"description\": \"The key to use to store the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session ID\",\n    \"name\": \"sessionIdType\",\n    \"type\": \"options\",\n    \"default\": \"fromInput\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"fromInput\",\n        \"description\": \"Looks for an input field called 'sessionId' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"customKey\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Session Key From Previous Node\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.sessionId }}\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"fromInput\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Key\",\n    \"name\": \"sessionKey\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The key to use to store session ID in the memory\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sessionIdType\": [\n          \"customKey\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"zepApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.modelSelector",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Model Selector",
    "description": "Use this node to select one of the connected models to this node based on workflow data",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Number of Inputs\",\n    \"name\": \"numberInputs\",\n    \"type\": \"options\",\n    \"default\": 2,\n    \"description\": \"The number of data inputs you want to merge. The node waits for all connected inputs to be executed.\",\n    \"options\": [\n      {\n        \"name\": \"2\",\n        \"value\": 2\n      },\n      {\n        \"name\": \"3\",\n        \"value\": 3\n      },\n      {\n        \"name\": \"4\",\n        \"value\": 4\n      },\n      {\n        \"name\": \"5\",\n        \"value\": 5\n      },\n      {\n        \"name\": \"6\",\n        \"value\": 6\n      },\n      {\n        \"name\": \"7\",\n        \"value\": 7\n      },\n      {\n        \"name\": \"8\",\n        \"value\": 8\n      },\n      {\n        \"name\": \"9\",\n        \"value\": 9\n      },\n      {\n        \"name\": \"10\",\n        \"value\": 10\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Rules\",\n    \"name\": \"rules\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"description\": \"Rules to map workflow data to specific models\",\n    \"options\": [\n      {\n        \"displayName\": \"Rule\",\n        \"name\": \"rule\",\n        \"values\": [\n          {\n            \"displayName\": \"Model\",\n            \"name\": \"modelIndex\",\n            \"type\": \"options\",\n            \"description\": \"Choose model input from the list\",\n            \"default\": 1,\n            \"required\": true,\n            \"placeholder\": \"Choose model input from the list\",\n            \"typeOptions\": {\n              \"loadOptionsMethod\": \"getModels\"\n            }\n          },\n          {\n            \"displayName\": \"Conditions\",\n            \"name\": \"conditions\",\n            \"placeholder\": \"Add Condition\",\n            \"type\": \"filter\",\n            \"default\": {},\n            \"typeOptions\": {\n              \"filter\": {\n                \"caseSensitive\": true,\n                \"typeValidation\": \"strict\",\n                \"version\": 2\n              }\n            },\n            \"description\": \"Conditions that must be met to select this model\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true,\n      \"sortable\": true\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.openAi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "OpenAI",
    "description": "Message an assistant or GPT, analyze images, generate audio, etc.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.8",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Resource\",\n    \"name\": \"resource\",\n    \"type\": \"options\",\n    \"default\": \"text\",\n    \"options\": [\n      {\n        \"name\": \"Assistant\",\n        \"value\": \"assistant\"\n      },\n      {\n        \"name\": \"Text\",\n        \"value\": \"text\"\n      },\n      {\n        \"name\": \"Image\",\n        \"value\": \"image\"\n      },\n      {\n        \"name\": \"Audio\",\n        \"value\": \"audio\"\n      },\n      {\n        \"name\": \"File\",\n        \"value\": \"file\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"operation\",\n    \"type\": \"options\",\n    \"default\": \"message\",\n    \"options\": [\n      {\n        \"name\": \"Create an Assistant\",\n        \"value\": \"create\",\n        \"action\": \"Create an assistant\",\n        \"description\": \"Create a new assistant\"\n      },\n      {\n        \"name\": \"Delete an Assistant\",\n        \"value\": \"deleteAssistant\",\n        \"action\": \"Delete an assistant\",\n        \"description\": \"Delete an assistant from the account\"\n      },\n      {\n        \"name\": \"List Assistants\",\n        \"value\": \"list\",\n        \"action\": \"List assistants\",\n        \"description\": \"List assistants in the organization\"\n      },\n      {\n        \"name\": \"Message an Assistant\",\n        \"value\": \"message\",\n        \"action\": \"Message an assistant\",\n        \"description\": \"Send messages to an assistant\"\n      },\n      {\n        \"name\": \"Update an Assistant\",\n        \"value\": \"update\",\n        \"action\": \"Update an assistant\",\n        \"description\": \"Update an existing assistant\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the assistant. The maximum length is 256 characters.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"description\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The description of the assistant. The maximum length is 512 characters.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Instructions\",\n    \"name\": \"instructions\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The system instructions that the assistant uses. The maximum length is 32768 characters.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Code Interpreter\",\n    \"name\": \"codeInterpreter\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to enable the code interpreter that allows the assistants to write and run Python code in a sandboxed execution environment, find more <a href=\\\"https://platform.openai.com/docs/assistants/tools/code-interpreter\\\" target=\\\"_blank\\\">here</a>\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Knowledge Retrieval\",\n    \"name\": \"knowledgeRetrieval\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to augments the assistant with knowledge from outside its model, such as proprietary product information or documents, find more <a href=\\\"https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\\\" target=\\\"_blank\\\">here</a>\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Files\",\n    \"name\": \"file_ids\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"description\": \"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant. You can use expression to pass file IDs as an array or comma-separated string.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"codeInterpreter\": [\n          true\n        ],\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      },\n      \"hide\": {\n        \"knowledgeRetrieval\": [\n          true\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsMethod\": \"getFiles\"\n    }\n  },\n  {\n    \"displayName\": \"Files\",\n    \"name\": \"file_ids\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"description\": \"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant\",\n    \"displayOptions\": {\n      \"show\": {\n        \"knowledgeRetrieval\": [\n          true\n        ],\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      },\n      \"hide\": {\n        \"codeInterpreter\": [\n          true\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsMethod\": \"getFiles\"\n    }\n  },\n  {\n    \"displayName\": \"Files\",\n    \"name\": \"file_ids\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"description\": \"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant\",\n    \"displayOptions\": {\n      \"show\": {\n        \"knowledgeRetrieval\": [\n          true\n        ],\n        \"codeInterpreter\": [\n          true\n        ],\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsMethod\": \"getFiles\"\n    }\n  },\n  {\n    \"displayName\": \"Add custom n8n tools when you <i>message</i> your assistant (rather than when creating it)\",\n    \"name\": \"noticeTools\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Output Randomness (Temperature)\",\n        \"name\": \"temperature\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Top P)\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Fail if Assistant Already Exists\",\n        \"name\": \"failIfExists\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to fail an operation if the assistant with the same name already exists\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"create\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Assistant\",\n    \"name\": \"assistantId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"description\": \"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\\\"https://platform.openai.com/playground?mode=assistant\\\" target=\\\"_blank\\\">playground</a>.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"deleteAssistant\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Assistant\",\n    \"name\": \"assistantId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"description\": \"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\\\"https://platform.openai.com/playground?mode=assistant\\\" target=\\\"_blank\\\">playground</a>.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Source for Prompt (User Message)\",\n    \"name\": \"prompt\",\n    \"type\": \"options\",\n    \"default\": \"auto\",\n    \"options\": [\n      {\n        \"name\": \"Connected Chat Trigger Node\",\n        \"value\": \"auto\",\n        \"description\": \"Looks for an input field called 'chatInput' that is coming from a directly connected Chat Trigger\"\n      },\n      {\n        \"name\": \"Define below\",\n        \"value\": \"define\",\n        \"description\": \"Use an expression to reference data in previous nodes or enter static text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt (User Message)\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"prompt\": [\n          \"define\"\n        ],\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Memory\",\n    \"name\": \"memory\",\n    \"type\": \"options\",\n    \"default\": \"connector\",\n    \"options\": [\n      {\n        \"name\": \"Use memory connector\",\n        \"value\": \"connector\",\n        \"description\": \"Connect one of the supported memory nodes\"\n      },\n      {\n        \"name\": \"Use thread ID\",\n        \"value\": \"threadId\",\n        \"description\": \"Specify the ID of the thread to continue\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.6\n            }\n          }\n        ],\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Thread ID\",\n    \"name\": \"threadId\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The ID of the thread to continue, a new thread will be created if not specified\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.6\n            }\n          }\n        ],\n        \"memory\": [\n          \"threadId\"\n        ],\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Connect your own custom n8n tools to this node on the canvas\",\n    \"name\": \"noticeTools\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Base URL\",\n        \"name\": \"baseURL\",\n        \"default\": \"https://api.openai.com/v1\",\n        \"description\": \"Override the default base URL for the API\",\n        \"type\": \"string\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.8\n                }\n              }\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 10000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Preserve Original Tools\",\n        \"name\": \"preserveOriginalTools\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to preserve the original tools of the assistant after the execution of this node, otherwise the tools will be replaced with the connected tools, if any, default is true\",\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.3\n                }\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplify\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether to return a simplified version of the response instead of the raw data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"list\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Assistant\",\n    \"name\": \"assistantId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"description\": \"Assistant to respond to the message. You can add, modify or remove assistants in the <a href=\\\"https://platform.openai.com/playground?mode=assistant\\\" target=\\\"_blank\\\">playground</a>.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"update\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Code Interpreter\",\n        \"name\": \"codeInterpreter\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to enable the code interpreter that allows the assistants to write and run Python code in a sandboxed execution environment, find more <a href=\\\"https://platform.openai.com/docs/assistants/tools/code-interpreter\\\" target=\\\"_blank\\\">here</a>\"\n      },\n      {\n        \"displayName\": \"Description\",\n        \"name\": \"description\",\n        \"type\": \"string\",\n        \"default\": \"\",\n        \"description\": \"The description of the assistant. The maximum length is 512 characters.\",\n        \"placeholder\": \"e.g. My personal assistant\"\n      },\n      {\n        \"displayName\": \"Files\",\n        \"name\": \"file_ids\",\n        \"type\": \"multiOptions\",\n        \"description\": \"The files to be used by the assistant, there can be a maximum of 20 files attached to the assistant. You can use expression to pass file IDs as an array or comma-separated string.\",\n        \"typeOptions\": {\n          \"loadOptionsMethod\": \"getFiles\"\n        },\n        \"default\": [],\n        \"hint\": \"Add more files by using the 'Upload a File' operation, any existing files not selected here will be removed.\"\n      },\n      {\n        \"displayName\": \"Instructions\",\n        \"name\": \"instructions\",\n        \"type\": \"string\",\n        \"description\": \"The system instructions that the assistant uses. The maximum length is 32768 characters.\",\n        \"default\": \"\",\n        \"typeOptions\": {\n          \"rows\": 2\n        }\n      },\n      {\n        \"displayName\": \"Knowledge Retrieval\",\n        \"name\": \"knowledgeRetrieval\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to augments the assistant with knowledge from outside its model, such as proprietary product information or documents, find more <a href=\\\"https://platform.openai.com/docs/assistants/tools/knowledge-retrieval\\\" target=\\\"_blank\\\">here</a>\"\n      },\n      {\n        \"displayName\": \"Model\",\n        \"name\": \"modelId\",\n        \"type\": \"resourceLocator\",\n        \"default\": {\n          \"mode\": \"list\",\n          \"value\": \"\"\n        },\n        \"required\": false,\n        \"modes\": [\n          {\n            \"displayName\": \"From List\",\n            \"name\": \"list\",\n            \"type\": \"list\",\n            \"typeOptions\": {\n              \"searchListMethod\": \"modelSearch\",\n              \"searchable\": true\n            }\n          },\n          {\n            \"displayName\": \"ID\",\n            \"name\": \"id\",\n            \"type\": \"string\",\n            \"placeholder\": \"e.g. gpt-4\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Name\",\n        \"name\": \"name\",\n        \"type\": \"string\",\n        \"default\": \"\",\n        \"description\": \"The name of the assistant. The maximum length is 256 characters.\",\n        \"placeholder\": \"e.g. My Assistant\"\n      },\n      {\n        \"displayName\": \"Remove All Custom Tools (Functions)\",\n        \"name\": \"removeCustomTools\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to remove all custom tools (functions) from the assistant\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Temperature)\",\n        \"name\": \"temperature\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Top P)\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"update\"\n        ],\n        \"resource\": [\n          \"assistant\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"operation\",\n    \"type\": \"options\",\n    \"default\": \"generate\",\n    \"options\": [\n      {\n        \"name\": \"Generate Audio\",\n        \"value\": \"generate\",\n        \"action\": \"Generate audio\",\n        \"description\": \"Creates audio from a text prompt\"\n      },\n      {\n        \"name\": \"Transcribe a Recording\",\n        \"value\": \"transcribe\",\n        \"action\": \"Transcribe a recording\",\n        \"description\": \"Transcribes audio into the text\"\n      },\n      {\n        \"name\": \"Translate a Recording\",\n        \"value\": \"translate\",\n        \"action\": \"Translate a recording\",\n        \"description\": \"Translate audio into the text in the english language\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"OpenAI API limits the size of the audio file to 25 MB\",\n    \"name\": \"fileSizeLimitNotice\",\n    \"type\": \"notice\",\n    \"default\": \" \",\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"audio\"\n        ],\n        \"operation\": [\n          \"translate\",\n          \"transcribe\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"tts-1\",\n    \"options\": [\n      {\n        \"name\": \"TTS-1\",\n        \"value\": \"tts-1\"\n      },\n      {\n        \"name\": \"TTS-1-HD\",\n        \"value\": \"tts-1-hd\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Text Input\",\n    \"name\": \"input\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The text to generate audio for. The maximum length is 4096 characters.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Voice\",\n    \"name\": \"voice\",\n    \"type\": \"options\",\n    \"default\": \"alloy\",\n    \"description\": \"The voice to use when generating the audio\",\n    \"options\": [\n      {\n        \"name\": \"Alloy\",\n        \"value\": \"alloy\"\n      },\n      {\n        \"name\": \"Echo\",\n        \"value\": \"echo\"\n      },\n      {\n        \"name\": \"Fable\",\n        \"value\": \"fable\"\n      },\n      {\n        \"name\": \"Nova\",\n        \"value\": \"nova\"\n      },\n      {\n        \"name\": \"Onyx\",\n        \"value\": \"onyx\"\n      },\n      {\n        \"name\": \"Shimmer\",\n        \"value\": \"shimmer\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Response Format\",\n        \"name\": \"response_format\",\n        \"type\": \"options\",\n        \"default\": \"mp3\",\n        \"options\": [\n          {\n            \"name\": \"MP3\",\n            \"value\": \"mp3\"\n          },\n          {\n            \"name\": \"OPUS\",\n            \"value\": \"opus\"\n          },\n          {\n            \"name\": \"AAC\",\n            \"value\": \"aac\"\n          },\n          {\n            \"name\": \"FLAC\",\n            \"value\": \"flac\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Audio Speed\",\n        \"name\": \"speed\",\n        \"type\": \"number\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"minValue\": 0.25,\n          \"maxValue\": 4,\n          \"numberPrecision\": 1\n        }\n      },\n      {\n        \"displayName\": \"Put Output in Field\",\n        \"name\": \"binaryPropertyOutput\",\n        \"type\": \"string\",\n        \"default\": \"data\",\n        \"hint\": \"The name of the output field to put the binary file data in\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Data Field Name\",\n    \"name\": \"binaryPropertyName\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"Name of the binary property which contains the audio file in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"transcribe\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Language of the Audio File\",\n        \"name\": \"language\",\n        \"type\": \"string\",\n        \"description\": \"The language of the input audio. Supplying the input language in <a href=\\\"https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes\\\" target=\\\"_blank\\\">ISO-639-1</a> format will improve accuracy and latency.\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Temperature)\",\n        \"name\": \"temperature\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"minValue\": 0,\n          \"maxValue\": 1,\n          \"numberPrecision\": 1\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"transcribe\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Data Field Name\",\n    \"name\": \"binaryPropertyName\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"Name of the binary property which contains the audio file in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"translate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Output Randomness (Temperature)\",\n        \"name\": \"temperature\",\n        \"type\": \"number\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"minValue\": 0,\n          \"maxValue\": 1,\n          \"numberPrecision\": 1\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"translate\"\n        ],\n        \"resource\": [\n          \"audio\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"operation\",\n    \"type\": \"options\",\n    \"default\": \"upload\",\n    \"options\": [\n      {\n        \"name\": \"Delete a File\",\n        \"value\": \"deleteFile\",\n        \"action\": \"Delete a file\",\n        \"description\": \"Delete a file from the server\"\n      },\n      {\n        \"name\": \"List Files\",\n        \"value\": \"list\",\n        \"action\": \"List files\",\n        \"description\": \"Returns a list of files that belong to the user's organization\"\n      },\n      {\n        \"name\": \"Upload a File\",\n        \"value\": \"upload\",\n        \"action\": \"Upload a file\",\n        \"description\": \"Upload a file that can be used across various endpoints\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"file\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Input Data Field Name\",\n    \"name\": \"binaryPropertyName\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"Name of the binary property which contains the file. The size of individual files can be a maximum of 512 MB or 2 million tokens for Assistants.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"upload\"\n        ],\n        \"resource\": [\n          \"file\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Purpose\",\n        \"name\": \"purpose\",\n        \"type\": \"options\",\n        \"default\": \"assistants\",\n        \"description\": \"The intended purpose of the uploaded file, the 'Fine-tuning' only supports .jsonl files\",\n        \"options\": [\n          {\n            \"name\": \"Assistants\",\n            \"value\": \"assistants\"\n          },\n          {\n            \"name\": \"Fine-Tune\",\n            \"value\": \"fine-tune\"\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"upload\"\n        ],\n        \"resource\": [\n          \"file\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"File\",\n    \"name\": \"fileId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"deleteFile\"\n        ],\n        \"resource\": [\n          \"file\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Purpose\",\n        \"name\": \"purpose\",\n        \"type\": \"options\",\n        \"default\": \"any\",\n        \"description\": \"Only return files with the given purpose\",\n        \"options\": [\n          {\n            \"name\": \"Any [Default]\",\n            \"value\": \"any\"\n          },\n          {\n            \"name\": \"Assistants\",\n            \"value\": \"assistants\"\n          },\n          {\n            \"name\": \"Fine-Tune\",\n            \"value\": \"fine-tune\"\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"list\"\n        ],\n        \"resource\": [\n          \"file\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"operation\",\n    \"type\": \"options\",\n    \"default\": \"generate\",\n    \"options\": [\n      {\n        \"name\": \"Analyze Image\",\n        \"value\": \"analyze\",\n        \"action\": \"Analyze image\",\n        \"description\": \"Take in images and answer questions about them\"\n      },\n      {\n        \"name\": \"Generate an Image\",\n        \"value\": \"generate\",\n        \"action\": \"Generate an image\",\n        \"description\": \"Creates an image from a text prompt\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"dall-e-3\",\n    \"description\": \"The model to use for image generation\",\n    \"options\": [\n      {\n        \"name\": \"DALL·E 2\",\n        \"value\": \"dall-e-2\"\n      },\n      {\n        \"name\": \"DALL·E 3\",\n        \"value\": \"dall-e-3\"\n      },\n      {\n        \"name\": \"GPT Image 1\",\n        \"value\": \"gpt-image-1\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"A text description of the desired image(s). The maximum length is 1000 characters for dall-e-2 and 4000 characters for dall-e-3.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Number of Images\",\n        \"name\": \"n\",\n        \"default\": 1,\n        \"description\": \"Number of images to generate\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"minValue\": 1,\n          \"maxValue\": 10\n        },\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"dall-e-2\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Quality\",\n        \"name\": \"dalleQuality\",\n        \"type\": \"options\",\n        \"description\": \"The quality of the image that will be generated, HD creates images with finer details and greater consistency across the image\",\n        \"options\": [\n          {\n            \"name\": \"HD\",\n            \"value\": \"hd\"\n          },\n          {\n            \"name\": \"Standard\",\n            \"value\": \"standard\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"dall-e-3\"\n            ]\n          }\n        },\n        \"default\": \"standard\"\n      },\n      {\n        \"displayName\": \"Quality\",\n        \"name\": \"quality\",\n        \"type\": \"options\",\n        \"description\": \"The quality of the image that will be generated, High creates images with finer details and greater consistency across the image\",\n        \"options\": [\n          {\n            \"name\": \"High\",\n            \"value\": \"high\"\n          },\n          {\n            \"name\": \"Medium\",\n            \"value\": \"medium\"\n          },\n          {\n            \"name\": \"Low\",\n            \"value\": \"low\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"gpt-image-1\"\n            ]\n          }\n        },\n        \"default\": \"medium\"\n      },\n      {\n        \"displayName\": \"Resolution\",\n        \"name\": \"size\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"256x256\",\n            \"value\": \"256x256\"\n          },\n          {\n            \"name\": \"512x512\",\n            \"value\": \"512x512\"\n          },\n          {\n            \"name\": \"1024x1024\",\n            \"value\": \"1024x1024\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"dall-e-2\"\n            ]\n          }\n        },\n        \"default\": \"1024x1024\"\n      },\n      {\n        \"displayName\": \"Resolution\",\n        \"name\": \"size\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"1024x1024\",\n            \"value\": \"1024x1024\"\n          },\n          {\n            \"name\": \"1792x1024\",\n            \"value\": \"1792x1024\"\n          },\n          {\n            \"name\": \"1024x1792\",\n            \"value\": \"1024x1792\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"dall-e-3\"\n            ]\n          }\n        },\n        \"default\": \"1024x1024\"\n      },\n      {\n        \"displayName\": \"Resolution\",\n        \"name\": \"size\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"1024x1024\",\n            \"value\": \"1024x1024\"\n          },\n          {\n            \"name\": \"1024x1536\",\n            \"value\": \"1024x1536\"\n          },\n          {\n            \"name\": \"1536x1024\",\n            \"value\": \"1536x1024\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"gpt-image-1\"\n            ]\n          }\n        },\n        \"default\": \"1024x1024\"\n      },\n      {\n        \"displayName\": \"Style\",\n        \"name\": \"style\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Natural\",\n            \"value\": \"natural\",\n            \"description\": \"Produce more natural looking images\"\n          },\n          {\n            \"name\": \"Vivid\",\n            \"value\": \"vivid\",\n            \"description\": \"Lean towards generating hyper-real and dramatic images\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"/model\": [\n              \"dall-e-3\"\n            ]\n          }\n        },\n        \"default\": \"vivid\"\n      },\n      {\n        \"displayName\": \"Respond with Image URL(s)\",\n        \"name\": \"returnImageUrls\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to return image URL(s) instead of binary file(s)\",\n        \"displayOptions\": {\n          \"hide\": {\n            \"/model\": [\n              \"gpt-image-1\"\n            ]\n          }\n        }\n      },\n      {\n        \"displayName\": \"Put Output in Field\",\n        \"name\": \"binaryPropertyOutput\",\n        \"type\": \"string\",\n        \"default\": \"data\",\n        \"hint\": \"The name of the output field to put the binary file data in\",\n        \"displayOptions\": {\n          \"show\": {\n            \"returnImageUrls\": [\n              false\n            ]\n          }\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"generate\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.4\n            }\n          }\n        ],\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Text Input\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"What's in this image?\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Input Type\",\n    \"name\": \"inputType\",\n    \"type\": \"options\",\n    \"default\": \"url\",\n    \"options\": [\n      {\n        \"name\": \"Image URL(s)\",\n        \"value\": \"url\"\n      },\n      {\n        \"name\": \"Binary File(s)\",\n        \"value\": \"base64\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"URL(s)\",\n    \"name\": \"imageUrls\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"URL(s) of the image(s) to analyze, multiple URLs can be added separated by comma\",\n    \"displayOptions\": {\n      \"show\": {\n        \"inputType\": [\n          \"url\"\n        ],\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Data Field Name\",\n    \"name\": \"binaryPropertyName\",\n    \"type\": \"string\",\n    \"default\": \"data\",\n    \"description\": \"Name of the binary property which contains the image(s)\",\n    \"displayOptions\": {\n      \"show\": {\n        \"inputType\": [\n          \"base64\"\n        ],\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplify\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether to simplify the response or not\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Detail\",\n        \"name\": \"detail\",\n        \"type\": \"options\",\n        \"default\": \"auto\",\n        \"options\": [\n          {\n            \"name\": \"Auto\",\n            \"value\": \"auto\",\n            \"description\": \"Model will look at the image input size and decide if it should use the low or high setting\"\n          },\n          {\n            \"name\": \"Low\",\n            \"value\": \"low\",\n            \"description\": \"Return faster responses and consume fewer tokens\"\n          },\n          {\n            \"name\": \"High\",\n            \"value\": \"high\",\n            \"description\": \"Return more detailed responses, consumes more tokens\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Length of Description (Max Tokens)\",\n        \"description\": \"Fewer tokens will result in shorter, less detailed image description\",\n        \"name\": \"maxTokens\",\n        \"type\": \"number\",\n        \"default\": 300,\n        \"typeOptions\": {\n          \"minValue\": 1\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"analyze\"\n        ],\n        \"resource\": [\n          \"image\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"operation\",\n    \"type\": \"options\",\n    \"default\": \"message\",\n    \"options\": [\n      {\n        \"name\": \"Message a Model\",\n        \"value\": \"message\",\n        \"action\": \"Message a model\",\n        \"description\": \"Create a completion with GPT 3, 4, etc.\"\n      },\n      {\n        \"name\": \"Classify Text for Violations\",\n        \"value\": \"classify\",\n        \"action\": \"Classify text for violations\",\n        \"description\": \"Check whether content complies with usage policies\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Text Input\",\n    \"name\": \"input\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The input text to classify if it is violates the moderation policy\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"classify\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplify\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to return a simplified version of the response instead of the raw data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"classify\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Use Stable Model\",\n        \"name\": \"useStableModel\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to use the stable version of the model instead of the latest version, accuracy may be slightly lower\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"classify\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelId\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Messages\",\n    \"name\": \"messages\",\n    \"type\": \"fixedCollection\",\n    \"default\": {\n      \"values\": [\n        {\n          \"content\": \"\"\n        }\n      ]\n    },\n    \"options\": [\n      {\n        \"displayName\": \"Values\",\n        \"name\": \"values\",\n        \"values\": [\n          {\n            \"displayName\": \"Prompt\",\n            \"name\": \"content\",\n            \"type\": \"string\",\n            \"description\": \"The content of the message to be send\",\n            \"default\": \"\",\n            \"placeholder\": \"e.g. Hello, how can you help me?\",\n            \"typeOptions\": {\n              \"rows\": 2\n            }\n          },\n          {\n            \"displayName\": \"Role\",\n            \"name\": \"role\",\n            \"type\": \"options\",\n            \"description\": \"Role in shaping the model's response, it tells the model how it should behave and interact with the user\",\n            \"options\": [\n              {\n                \"name\": \"User\",\n                \"value\": \"user\",\n                \"description\": \"Send a message as a user and get a response from the model\"\n              },\n              {\n                \"name\": \"Assistant\",\n                \"value\": \"assistant\",\n                \"description\": \"Tell the model to adopt a specific tone or personality\"\n              },\n              {\n                \"name\": \"System\",\n                \"value\": \"system\",\n                \"description\": \"Usually used to set the model's behavior or context for the next user message\"\n              }\n            ],\n            \"default\": \"user\"\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"sortable\": true,\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Simplify Output\",\n    \"name\": \"simplify\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether to return a simplified version of the response instead of the raw data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Output Content as JSON\",\n    \"name\": \"jsonOutput\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to attempt to return the response in JSON format. Compatible with GPT-4 Turbo and all GPT-3.5 Turbo models newer than gpt-3.5-turbo-1106.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Hide Tools\",\n    \"name\": \"hideTools\",\n    \"type\": \"hidden\",\n    \"default\": \"hide\",\n    \"displayOptions\": {\n      \"show\": {\n        \"modelId\": [\n          \"gpt-3.5-turbo-16k-0613\",\n          \"dall-e-3\",\n          \"text-embedding-3-large\",\n          \"dall-e-2\",\n          \"whisper-1\",\n          \"tts-1-hd-1106\",\n          \"tts-1-hd\",\n          \"gpt-4-0314\",\n          \"text-embedding-3-small\",\n          \"gpt-4-32k-0314\",\n          \"gpt-3.5-turbo-0301\",\n          \"gpt-4-vision-preview\",\n          \"gpt-3.5-turbo-16k\",\n          \"gpt-3.5-turbo-instruct-0914\",\n          \"tts-1\",\n          \"davinci-002\",\n          \"gpt-3.5-turbo-instruct\",\n          \"babbage-002\",\n          \"tts-1-1106\",\n          \"text-embedding-ada-002\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ],\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Connect your own custom n8n tools to this node on the canvas\",\n    \"name\": \"noticeTools\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"hide\": {\n        \"hideTools\": [\n          \"hide\"\n        ]\n      },\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Frequency Penalty\",\n        \"name\": \"frequency_penalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Maximum Number of Tokens\",\n        \"name\": \"maxTokens\",\n        \"default\": 16,\n        \"description\": \"The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).\",\n        \"type\": \"number\",\n        \"typeOptions\": {\n          \"maxValue\": 32768\n        }\n      },\n      {\n        \"displayName\": \"Number of Completions\",\n        \"name\": \"n\",\n        \"default\": 1,\n        \"description\": \"How many completions to generate for each prompt. Note: Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Presence Penalty\",\n        \"name\": \"presence_penalty\",\n        \"default\": 0,\n        \"typeOptions\": {\n          \"maxValue\": 2,\n          \"minValue\": -2,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Temperature)\",\n        \"name\": \"temperature\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Output Randomness (Top P)\",\n        \"name\": \"topP\",\n        \"default\": 1,\n        \"typeOptions\": {\n          \"maxValue\": 1,\n          \"minValue\": 0,\n          \"numberPrecision\": 1\n        },\n        \"description\": \"An alternative to sampling with temperature, controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Max Tool Calls Iterations\",\n        \"name\": \"maxToolsIterations\",\n        \"type\": \"number\",\n        \"default\": 15,\n        \"description\": \"The maximum number of tool iteration cycles the LLM will run before stopping. A single iteration can contain multiple tool calls. Set to 0 for no limit.\",\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.5\n                }\n              }\n            ]\n          }\n        }\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"operation\": [\n          \"message\"\n        ],\n        \"resource\": [\n          \"text\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[\n  {\n    \"operation\": \"create\",\n    \"name\": \"Create an Assistant\",\n    \"description\": \"Create a new assistant\"\n  },\n  {\n    \"operation\": \"deleteAssistant\",\n    \"name\": \"Delete an Assistant\",\n    \"description\": \"Delete an assistant from the account\"\n  },\n  {\n    \"operation\": \"list\",\n    \"name\": \"List Assistants\",\n    \"description\": \"List assistants in the organization\"\n  },\n  {\n    \"operation\": \"message\",\n    \"name\": \"Message an Assistant\",\n    \"description\": \"Send messages to an assistant\"\n  },\n  {\n    \"operation\": \"update\",\n    \"name\": \"Update an Assistant\",\n    \"description\": \"Update an existing assistant\"\n  }\n]",
    "credentials_required": "[\n  {\n    \"name\": \"openAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.openAiAssistant",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "OpenAI Assistant",
    "description": "Utilizes Assistant API from Open AI.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Operation\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"existing\",\n    \"options\": [\n      {\n        \"name\": \"Use New Assistant\",\n        \"value\": \"new\"\n      },\n      {\n        \"name\": \"Use Existing Assistant\",\n        \"value\": \"existing\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"/mode\": [\n          \"new\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Instructions\",\n    \"name\": \"instructions\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"How the Assistant and model should behave or respond\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/mode\": [\n          \"new\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 5\n    }\n  },\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"model\",\n    \"type\": \"options\",\n    \"default\": \"gpt-3.5-turbo-1106\",\n    \"description\": \"The model which will be used to power the assistant. <a href=\\\"https://beta.openai.com/docs/models/overview\\\">Learn more</a>. The Retrieval tool requires gpt-3.5-turbo-1106 and gpt-4-1106-preview models.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"/mode\": [\n          \"new\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"url\": \"={{ $parameter.options?.baseURL?.split(\\\"/\\\").slice(-1).pop() || \\\"v1\\\"  }}/models\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"filter\",\n                \"properties\": {\n                  \"pass\": \"={{ $responseItem.id.startsWith('gpt-') && !$responseItem.id.includes('instruct') }}\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.id}}\",\n                  \"value\": \"={{$responseItem.id}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    }\n  },\n  {\n    \"displayName\": \"Assistant\",\n    \"name\": \"assistantId\",\n    \"type\": \"options\",\n    \"default\": \"\",\n    \"description\": \"The assistant to use. <a href=\\\"https://beta.openai.com/docs/assistants/overview\\\">Learn more</a>.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"/mode\": [\n          \"existing\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptions\": {\n        \"routing\": {\n          \"request\": {\n            \"method\": \"GET\",\n            \"headers\": {\n              \"OpenAI-Beta\": \"assistants=v1\"\n            },\n            \"url\": \"={{ $parameter.options?.baseURL?.split(\\\"/\\\").slice(-1).pop() || \\\"v1\\\"  }}/assistants\"\n          },\n          \"output\": {\n            \"postReceive\": [\n              {\n                \"type\": \"rootProperty\",\n                \"properties\": {\n                  \"property\": \"data\"\n                }\n              },\n              {\n                \"type\": \"setKeyValue\",\n                \"properties\": {\n                  \"name\": \"={{$responseItem.name}}\",\n                  \"value\": \"={{$responseItem.id}}\",\n                  \"description\": \"={{$responseItem.model}}\"\n                }\n              },\n              {\n                \"type\": \"sort\",\n                \"properties\": {\n                  \"key\": \"name\"\n                }\n              }\n            ]\n          }\n        }\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Text\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chat_input }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Text\",\n    \"name\": \"text\",\n    \"type\": \"string\",\n    \"default\": \"={{ $json.chatInput }}\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"OpenAI Tools\",\n    \"name\": \"nativeTools\",\n    \"type\": \"multiOptions\",\n    \"default\": [],\n    \"options\": [\n      {\n        \"name\": \"Code Interpreter\",\n        \"value\": \"code_interpreter\"\n      },\n      {\n        \"name\": \"Knowledge Retrieval\",\n        \"value\": \"retrieval\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Connect your own custom tools to this node on the canvas\",\n    \"name\": \"noticeTools\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Upload files for retrieval using the <a href=\\\"https://platform.openai.com/playground\\\" target=\\\"_blank\\\">OpenAI website<a/>\",\n    \"name\": \"noticeTools\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/nativeTools\": [\n          \"retrieval\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"noticeTheme\": \"info\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Base URL\",\n        \"name\": \"baseURL\",\n        \"default\": \"https://api.openai.com/v1\",\n        \"description\": \"Override the default base URL for the API\",\n        \"type\": \"string\"\n      },\n      {\n        \"displayName\": \"Max Retries\",\n        \"name\": \"maxRetries\",\n        \"default\": 2,\n        \"description\": \"Maximum number of retries to attempt\",\n        \"type\": \"number\"\n      },\n      {\n        \"displayName\": \"Timeout\",\n        \"name\": \"timeout\",\n        \"default\": 10000,\n        \"description\": \"Maximum amount of time a request is allowed to take in milliseconds\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"openAiApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.outputParserAutofixing",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Auto-fixing Output Parser",
    "description": "Deprecated, use structured output parser",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Auto-fixing Output Parser node documentation\ndescription: Learn how to use the Auto-fixing Output Parser node in n8n. Follow technical documentation to integrate Auto-fixing Output Parser node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Auto-fixing Output Parser node\n\nThe Auto-fixing Output Parser node wraps another output parser. If the first one fails, it calls out to another LLM to fix any errors.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'auto-fixing-output-parser') ]]\n\n## Related resources\n\nRefer to [LangChain's output parser documentation](https://js.langchain.com/docs/concepts/output_parsers/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node wraps another output parser. If the first one fails it calls an LLM to fix the format\",\n    \"name\": \"info\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Retry Prompt\",\n        \"name\": \"prompt\",\n        \"type\": \"string\",\n        \"default\": \"Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:\",\n        \"typeOptions\": {\n          \"rows\": 10\n        },\n        \"hint\": \"Should include \\\"{error}\\\", \\\"{instructions}\\\", and \\\"{completion}\\\" placeholders\",\n        \"description\": \"Prompt template used for fixing the output. Uses placeholders: \\\"{instructions}\\\" for parsing rules, \\\"{completion}\\\" for the failed attempt, and \\\"{error}\\\" for the validation error message.\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.outputParserItemList",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Item List Output Parser",
    "description": "Return the results as separate items",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Item List Output Parser node documentation\ndescription: Learn how to use the Item List Output Parser node in n8n. Follow technical documentation to integrate Item List Output Parser node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Item List Output Parser node\n\nUse the Item List Output Parser node to return a list of items with a specific length and separator.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node options\n\n* **Number of Items**: Enter the maximum items to return. Set to `-1` for unlimited items.\n* **Separator**: Select the separator used to split the results into separate items. Defaults to a new line.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'item-list-output-parser') ]]\n\n## Related resources\n\nRefer to [LangChain's output parser documentation](https://js.langchain.com/docs/concepts/output_parsers){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Number Of Items\",\n        \"name\": \"numberOfItems\",\n        \"type\": \"number\",\n        \"default\": -1,\n        \"description\": \"Defines how many items should be returned maximally. If set to -1, there is no limit.\"\n      },\n      {\n        \"displayName\": \"Separator\",\n        \"name\": \"separator\",\n        \"type\": \"string\",\n        \"default\": \"\\\\n\",\n        \"description\": \"Defines the separator that should be used to split the results into separate items. Defaults to a new line but can be changed depending on the data that should be returned.\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.outputParserStructured",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Structured Output Parser",
    "description": "Return data in a defined JSON format",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Structured Output Parser node documentation\ndescription: Learn how to use the Structured Output Parser node in n8n. Follow technical documentation to integrate Structured Output Parser node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Structured Output Parser node\n\nUse the Structured Output Parser node to return fields based on a JSON Schema.\n\nOn this page, you'll find the node parameters for the Structured Output Parser node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Schema Type**: Define the output structure and validation. You have two options to provide the schema:\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-sub-nodes/schema-type-structuring.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'structured-output-parser') ]]\n\n## Related resources\n\nRefer to [LangChain's output parser documentation](https://js.langchain.com/docs/concepts/output_parsers){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.outputparserstructured/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Schema Type\",\n    \"name\": \"schemaType\",\n    \"type\": \"options\",\n    \"default\": \"fromJson\",\n    \"description\": \"How to specify the schema for the function\",\n    \"options\": [\n      {\n        \"name\": \"Generate From JSON Example\",\n        \"value\": \"fromJson\",\n        \"description\": \"Generate a schema from an example JSON object\"\n      },\n      {\n        \"name\": \"Define using JSON Schema\",\n        \"value\": \"manual\",\n        \"description\": \"Define the JSON schema manually\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"JSON Example\",\n    \"name\": \"jsonSchemaExample\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\t\\\"state\\\": \\\"California\\\",\\n\\t\\\"cities\\\": [\\\"Los Angeles\\\", \\\"San Francisco\\\", \\\"San Diego\\\"]\\n}\",\n    \"description\": \"Example JSON object to use to generate the schema\",\n    \"displayOptions\": {\n      \"show\": {\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"All properties will be required. To make them optional, use the 'JSON Schema' schema type instead\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ],\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Schema\",\n    \"name\": \"inputSchema\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\t\\\"type\\\": \\\"object\\\",\\n\\t\\\"properties\\\": {\\n\\t\\t\\\"state\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t},\\n\\t\\t\\\"cities\\\": {\\n\\t\\t\\t\\\"type\\\": \\\"array\\\",\\n\\t\\t\\t\\\"items\\\": {\\n\\t\\t\\t\\t\\\"type\\\": \\\"string\\\"\\n\\t\\t\\t}\\n\\t\\t}\\n\\t}\\n}\",\n    \"description\": \"Schema to use for the function\",\n    \"displayOptions\": {\n      \"show\": {\n        \"schemaType\": [\n          \"manual\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"JSON Schema\",\n    \"name\": \"jsonSchema\",\n    \"type\": \"json\",\n    \"default\": \"{\\n  \\\"type\\\": \\\"object\\\",\\n  \\\"properties\\\": {\\n    \\\"state\\\": {\\n      \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"cities\\\": {\\n      \\\"type\\\": \\\"array\\\",\\n      \\\"items\\\": {\\n        \\\"type\\\": \\\"string\\\"\\n      }\\n    }\\n  }\\n}\",\n    \"description\": \"JSON Schema to structure and validate the output against\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.1\n            }\n          }\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    }\n  },\n  {\n    \"displayName\": \"Auto-Fix Format\",\n    \"name\": \"autoFix\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to automatically fix the output when it is not in the correct format. Will cause another LLM call.\"\n  },\n  {\n    \"displayName\": \"Customize Retry Prompt\",\n    \"name\": \"customizeRetryPrompt\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to customize the prompt used for retrying the output parsing. If disabled, a default prompt will be used.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"autoFix\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Custom Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"Instructions:\\n--------------\\n{instructions}\\n--------------\\nCompletion:\\n--------------\\n{completion}\\n--------------\\n\\nAbove, the Completion did not satisfy the constraints given in the Instructions.\\nError:\\n--------------\\n{error}\\n--------------\\n\\nPlease try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:\",\n    \"description\": \"Prompt template used for fixing the output. Uses placeholders: \\\"{instructions}\\\" for parsing rules, \\\"{completion}\\\" for the failed attempt, and \\\"{error}\\\" for the validation error message.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"autoFix\": [\n          true\n        ],\n        \"customizeRetryPrompt\": [\n          true\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.rerankerCohere",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Reranker Cohere",
    "description": "Use Cohere Reranker to reorder documents after retrieval from a vector store by relevance to the given query.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 1,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Reranker Cohere\ndescription: Learn how to use the Reranker Cohere node in n8n. Follow technical documentation to integrate Cohere reranking into your workflows.\ncontentType: [integration, reference]\n---\n\n# Reranker Cohere\n\nThe Reranker Cohere node allows you to [rerank](/glossary.md#ai-reranking) the resulting chunks from a [vector store](/glossary.md#ai-vector-store). You can connect this node to a vector store.\n\nThe reranker reorders the list of documents retrieved from a vector store for a given query in order of descending relevance.\n\nOn this page, you'll find the node parameters for the Reranker Cohere node, and links to more resources.\n\n///  note  | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/cohere.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Model\n\nChoose the reranking model to use. You can find out more about the available models in [Cohere's model documentation](https://docs.cohere.com/docs/models#rerank).\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Model\",\n    \"name\": \"modelName\",\n    \"type\": \"options\",\n    \"default\": \"rerank-v3.5\",\n    \"description\": \"The model that should be used to rerank the documents. <a href=\\\"https://docs.cohere.com/docs/models\\\">Learn more</a>.\",\n    \"options\": [\n      {\n        \"name\": \"rerank-v3.5\",\n        \"value\": \"rerank-v3.5\"\n      },\n      {\n        \"name\": \"rerank-english-v3.0\",\n        \"value\": \"rerank-english-v3.0\"\n      },\n      {\n        \"name\": \"rerank-multilingual-v3.0\",\n        \"value\": \"rerank-multilingual-v3.0\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"cohereApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.retrieverContextualCompression",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Contextual Compression Retriever",
    "description": "Enhances document similarity search by contextual compression.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Contextual Compression Retriever node documentation\ndescription: Learn how to use the Contextual Compression Retriever node in n8n. Follow technical documentation to integrate Contextual Compression Retriever node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Contextual Compression Retriever node\n\nThe Contextual Compression Retriever node improves the answers returned from [vector store](/glossary.md#ai-vector-store) document similarity searches by taking into account the context from the query.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'contextual-compression-retriever') ]]\n\n## Related resources\n\nRefer to [LangChain's contextual compression retriever documentation](https://js.langchain.com/docs/how_to/contextual_compression/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.retrieverMultiQuery",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "MultiQuery Retriever",
    "description": "Automates prompt tuning, generates diverse queries and expands document pool for enhanced retrieval.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: MultiQuery Retriever node documentation\ndescription: Learn how to use the MultiQuery Retriever node in n8n. Follow technical documentation to integrate MultiQuery Retriever node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# MultiQuery Retriever node\n\nThe MultiQuery Retriever node automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query.\n\nOn this page, you'll find the node parameters for the MultiQuery Retriever node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node options\n\n* **Query Count**: Enter how many different versions of the query to generate.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'multiquery-retriever') ]]\n\n## Related resources\n\nRefer to [LangChain's retriever conceptual documentation](https://js.langchain.com/docs/concepts/retrievers){:target=_blank .external-link} and [LangChain's multiquery retriever API documentation](https://v03.api.js.langchain.com/classes/langchain.retrievers_multi_query.MultiQueryRetriever.html){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Query Count\",\n        \"name\": \"queryCount\",\n        \"default\": 3,\n        \"typeOptions\": {\n          \"minValue\": 1\n        },\n        \"description\": \"Number of different versions of the given question to generate\",\n        \"type\": \"number\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.retrieverVectorStore",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Vector Store Retriever",
    "description": "Use a Vector Store as Retriever",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Vector Store Retriever node documentation\ndescription: Learn how to use the Vector Store Retriever node in n8n. Follow technical documentation to integrate Vector Store Retriever node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Vector Store Retriever node\n\nUse the Vector Store Retriever node to retrieve documents from a [vector store](/glossary.md#ai-vector-store).\n\nOn this page, you'll find the node parameters for the Vector Store Retriever node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Limit**: Enter the maximum number of results to return.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'vector-store-retriever') ]]\n\n## Related resources\n\nRefer to [LangChain's vector store retriever documentation](https://js.langchain.com/docs/how_to/vectorstore_retriever/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"The maximum number of results to return\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.retrieverWorkflow",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Workflow Retriever",
    "description": "Use an n8n Workflow as Retriever",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Workflow Retriever node documentation\ndescription: Learn how to use the Workflow Retriever node in n8n. Follow technical documentation to integrate Workflow Retriever node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Workflow Retriever node\n\nUse the Workflow Retriever node to retrieve data from an n8n workflow for use in a Retrieval QA Chain or another Retriever node.\n\nOn this page, you'll find the node parameters for the Workflow Retriever node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Source\n\nTell n8n which workflow to call. You can choose either:\n\n* **Database** and enter a workflow ID.\n* **Parameter** and copy in a complete [workflow JSON](/workflows/export-import.md).\n\n### Workflow values\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-sub-nodes/workflow-values.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'workflow-retriever') ]]\n\n## Related resources\n\nRefer to [LangChain's general retriever documentation](https://js.langchain.com/docs/concepts/retrievers/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"The workflow will receive \\\"query\\\" as input and the output of the last node will be returned and converted to Documents\",\n    \"name\": \"executeNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Source\",\n    \"name\": \"source\",\n    \"type\": \"options\",\n    \"default\": \"database\",\n    \"description\": \"Where to get the workflow to execute from\",\n    \"options\": [\n      {\n        \"name\": \"Database\",\n        \"value\": \"database\",\n        \"description\": \"Load the workflow from the database by ID\"\n      },\n      {\n        \"name\": \"Parameter\",\n        \"value\": \"parameter\",\n        \"description\": \"Load the workflow from a parameter\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Workflow ID\",\n    \"name\": \"workflowId\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The workflow to execute\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"database\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"eq\": 1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Workflow\",\n    \"name\": \"workflowId\",\n    \"type\": \"workflowSelector\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"database\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Workflow JSON\",\n    \"name\": \"workflowJson\",\n    \"type\": \"json\",\n    \"default\": \"\\n\\n\\n\",\n    \"description\": \"The workflow JSON code to execute\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"parameter\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    }\n  },\n  {\n    \"displayName\": \"Workflow Values\",\n    \"name\": \"fields\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"description\": \"Set the values which should be made available in the workflow\",\n    \"options\": [\n      {\n        \"name\": \"values\",\n        \"displayName\": \"Values\",\n        \"values\": [\n          {\n            \"displayName\": \"Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"placeholder\": \"e.g. fieldName\",\n            \"description\": \"Name of the field to set the value of. Supports dot-notation. Example: data.person[0].name.\",\n            \"requiresDataPath\": \"single\"\n          },\n          {\n            \"displayName\": \"Type\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"description\": \"The field value type\",\n            \"options\": [\n              {\n                \"name\": \"String\",\n                \"value\": \"stringValue\"\n              },\n              {\n                \"name\": \"Number\",\n                \"value\": \"numberValue\"\n              },\n              {\n                \"name\": \"Boolean\",\n                \"value\": \"booleanValue\"\n              },\n              {\n                \"name\": \"Array\",\n                \"value\": \"arrayValue\"\n              },\n              {\n                \"name\": \"Object\",\n                \"value\": \"objectValue\"\n              }\n            ],\n            \"default\": \"stringValue\"\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"stringValue\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"stringValue\"\n                ]\n              }\n            },\n            \"validateType\": \"string\",\n            \"ignoreValidationDuringExecution\": true\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"numberValue\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"numberValue\"\n                ]\n              }\n            },\n            \"validateType\": \"number\",\n            \"ignoreValidationDuringExecution\": true\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"booleanValue\",\n            \"type\": \"options\",\n            \"default\": \"true\",\n            \"options\": [\n              {\n                \"name\": \"True\",\n                \"value\": \"true\"\n              },\n              {\n                \"name\": \"False\",\n                \"value\": \"false\"\n              }\n            ],\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"booleanValue\"\n                ]\n              }\n            },\n            \"validateType\": \"boolean\",\n            \"ignoreValidationDuringExecution\": true\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"arrayValue\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"placeholder\": \"e.g. [ arrayItem1, arrayItem2, arrayItem3 ]\",\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"arrayValue\"\n                ]\n              }\n            },\n            \"validateType\": \"array\",\n            \"ignoreValidationDuringExecution\": true\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"objectValue\",\n            \"type\": \"json\",\n            \"default\": \"={}\",\n            \"typeOptions\": {\n              \"rows\": 2\n            },\n            \"displayOptions\": {\n              \"show\": {\n                \"type\": [\n                  \"objectValue\"\n                ]\n              }\n            },\n            \"validateType\": \"object\",\n            \"ignoreValidationDuringExecution\": true\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true,\n      \"sortable\": true\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.sentimentAnalysis",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Sentiment Analysis",
    "description": "Analyze the sentiment of your text",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Sentiment Analysis node documentation\ndescription: Learn how to use the Sentiment Analysis node in n8n. Follow technical documentation to integrate Sentiment Analysis node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Sentiment Analysis node\n\nUse the Sentiment Analysis node to analyze the sentiment of incoming text data.\n\nThe language model uses the [**Sentiment Categories**](#node-options) in the node options to determine each item's sentiment.\n\n## Node parameters\n\n* **Text to Analyze** defines the input text for sentiment analysis. This is an expression that references a field from the input items. For example, this could be \n`{{ $json.chatInput }}` if the input is from a chat or message source. By default, it expects a `text` field.\n\n## Node options\n\n* **Sentiment Categories**: Define the categories that you want to classify your input as.\n    * By default, these are `Positive, Neutral, Negative`. You can customize these categories to fit your specific use case, such as `Very Positive, Positive, Neutral, Negative, Very Negative` for more granular analysis.\n* **Include Detailed Results**: When turned on, this option includes sentiment strength and confidence scores in the output. Note that these scores are estimates generated by the language model and are rough indicators rather than precise measurements.\n* **System Prompt Template**: Use this option to change the system prompt that's used for the sentiment analysis. It uses the `{categories}` placeholder for the categories.\n* **Enable Auto-Fixing**: When enabled, the node automatically fixes model outputs to ensure they match the expected format. Do this by sending the schema parsing error to the LLM and asking it to fix it.\n\n## Usage Notes\n\n### Model Temperature Setting\n\nIt's strongly advised to set the temperature of the connected language model to 0 or a value close to 0. This helps ensure that the results are as deterministic as possible, providing more consistent and reliable sentiment analysis across multiple runs.\n\n### Language Considerations\n\nThe node's performance may vary depending on the language of the input text.\n\nFor best results, ensure your chosen language model supports the input language.\n\n### Processing Large Volumes\n\nWhen analyzing large amounts of text, consider splitting the input into smaller chunks to optimize processing time and resource usage.\n\n### Iterative Refinement\n\nFor complex sentiment analysis tasks, you may need to iteratively refine the system prompt and categories to achieve the desired results.\n\n## Example Usage\n\n### Basic Sentiment Analysis\n\n1. Connect a data source (for example, RSS Feed, HTTP Request) to the Sentiment Analysis node.\n2. Set the \"Text to Analyze\" field to the relevant item property (for example, `{{ $json.content }}` for blog post content).\n3. Keep the default sentiment categories.\n4. Connect the node's outputs to separate paths for processing positive, neutral, and negative sentiments differently.\n\n### Custom Category Analysis\n\n1. Change the **Sentiment Categories** to `Excited, Happy, Neutral, Disappointed, Angry`.\n2. Adjust your workflow to handle these five output categories.\n3. Use this setup to analyze customer feedback with more nuanced emotional categories.\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Text to Analyze\",\n    \"name\": \"inputText\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Use an expression to reference data in previous nodes or enter static text\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Sentiment scores are LLM-generated estimates, not statistically rigorous measurements. They may be inconsistent across runs and should be used as rough indicators only.\",\n    \"name\": \"detailedResultsNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"/options.includeDetailedResults\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Sentiment Categories\",\n        \"name\": \"categories\",\n        \"type\": \"string\",\n        \"default\": \"Positive, Neutral, Negative\",\n        \"description\": \"A comma-separated list of categories to analyze\",\n        \"noDataExpression\": true,\n        \"typeOptions\": {\n          \"rows\": 2\n        }\n      },\n      {\n        \"displayName\": \"System Prompt Template\",\n        \"name\": \"systemPromptTemplate\",\n        \"type\": \"string\",\n        \"default\": \"You are highly intelligent and accurate sentiment analyzer. Analyze the sentiment of the provided text. Categorize it into one of the following: {categories}. Use the provided formatting instructions. Only output the JSON.\",\n        \"description\": \"String to use directly as the system prompt template\",\n        \"typeOptions\": {\n          \"rows\": 6\n        }\n      },\n      {\n        \"displayName\": \"Include Detailed Results\",\n        \"name\": \"includeDetailedResults\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to include sentiment strength and confidence scores in the output\"\n      },\n      {\n        \"displayName\": \"Enable Auto-Fixing\",\n        \"name\": \"enableAutoFixing\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to enable auto-fixing (may trigger an additional LLM call if output is broken)\"\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 5,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.1\n                }\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.textClassifier",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Text Classifier",
    "description": "Classify your text into distinct categories",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Text to Classify\",\n    \"name\": \"inputText\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Use an expression to reference data in previous nodes or enter static text\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Categories\",\n    \"name\": \"categories\",\n    \"type\": \"fixedCollection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"name\": \"categories\",\n        \"displayName\": \"Categories\",\n        \"values\": [\n          {\n            \"displayName\": \"Category\",\n            \"name\": \"category\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"description\": \"Category to add\",\n            \"required\": true\n          },\n          {\n            \"displayName\": \"Description\",\n            \"name\": \"description\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"description\": \"Describe your category if it's not obvious\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Allow Multiple Classes To Be True\",\n        \"name\": \"multiClass\",\n        \"type\": \"boolean\",\n        \"default\": false\n      },\n      {\n        \"displayName\": \"When No Clear Match\",\n        \"name\": \"fallback\",\n        \"type\": \"options\",\n        \"default\": \"discard\",\n        \"description\": \"What to do with items that don’t match the categories exactly\",\n        \"options\": [\n          {\n            \"name\": \"Discard Item\",\n            \"value\": \"discard\",\n            \"description\": \"Ignore the item and drop it from the output\"\n          },\n          {\n            \"name\": \"Output on Extra, 'Other' Branch\",\n            \"value\": \"other\",\n            \"description\": \"Create a separate output branch called 'Other'\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"System Prompt Template\",\n        \"name\": \"systemPromptTemplate\",\n        \"type\": \"string\",\n        \"default\": \"Please classify the text provided by the user into one of the following categories: {categories}, and use the provided formatting instructions below. Don't explain, and only output the json.\",\n        \"description\": \"String to use directly as the system prompt template\",\n        \"typeOptions\": {\n          \"rows\": 6\n        }\n      },\n      {\n        \"displayName\": \"Enable Auto-Fixing\",\n        \"name\": \"enableAutoFixing\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to enable auto-fixing (may trigger an additional LLM call if output is broken)\"\n      },\n      {\n        \"displayName\": \"Batch Processing\",\n        \"name\": \"batching\",\n        \"type\": \"collection\",\n        \"placeholder\": \"Add Batch Processing Option\",\n        \"description\": \"Batch processing options for rate limiting\",\n        \"default\": {},\n        \"options\": [\n          {\n            \"displayName\": \"Batch Size\",\n            \"name\": \"batchSize\",\n            \"default\": 5,\n            \"type\": \"number\",\n            \"description\": \"How many items to process in parallel. This is useful for rate limiting, but might impact the log output ordering.\"\n          },\n          {\n            \"displayName\": \"Delay Between Batches\",\n            \"name\": \"delayBetweenBatches\",\n            \"default\": 0,\n            \"type\": \"number\",\n            \"description\": \"Delay in milliseconds between batches. This is useful for rate limiting.\"\n          }\n        ],\n        \"displayOptions\": {\n          \"show\": {\n            \"@version\": [\n              {\n                \"_cnd\": {\n                  \"gte\": 1.1\n                }\n              }\n            ]\n          }\n        }\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.textSplitterCharacterTextSplitter",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Character Text Splitter",
    "description": "Split text into chunks by characters",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Character Text Splitter node documentation\ndescription: Learn how to use the Character Text Splitter node in n8n. Follow technical documentation to integrate Character Text Splitter node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Character Text Splitter node\n\nUse the Character Text Splitter node to split document data based on characters.\n\nOn this page, you'll find the node parameters for the Character Text Splitter node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Separator**: Select the separator used to split the document into separate items.\n* **Chunk Size**: Enter the number of characters in each chunk.\n* **Chunk Overlap**: Enter how much overlap to have between chunks.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'character-text-splitter') ]]\n\n## Related resources\n\nRefer to [LangChain's text splitter documentation](https://js.langchain.com/docs/concepts/text_splitters){:target=_blank .external-link} and [LangChain's API documentation for character text splitting](https://v03.api.js.langchain.com/classes/langchain.text_splitter.CharacterTextSplitter.html){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Separator\",\n    \"name\": \"separator\",\n    \"type\": \"string\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Chunk Size\",\n    \"name\": \"chunkSize\",\n    \"type\": \"number\",\n    \"default\": 1000\n  },\n  {\n    \"displayName\": \"Chunk Overlap\",\n    \"name\": \"chunkOverlap\",\n    \"type\": \"number\",\n    \"default\": 0\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.textSplitterRecursiveCharacterTextSplitter",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Recursive Character Text Splitter",
    "description": "Split text into chunks by characters recursively, recommended for most use cases",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Recursive Character Text Splitter node documentation\ndescription: Learn how to use the Recursive Character Text Splitter node in n8n. Follow technical documentation to integrate Recursive Character Text Splitter node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Recursive Character Text Splitter node\n\nThe Recursive Character Text Splitter node splits document data recursively to keep all paragraphs, sentences then words together as long as possible.\n\nOn this page, you'll find the node parameters for the Recursive Character Text Splitter node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Chunk Size**: Enter the number of characters in each chunk.\n* **Chunk Overlap**: Enter how much overlap to have between chunks.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'recursive-character-text-splitter') ]]\n\n## Related resources\n\nRefer to [LangChain's text splitter documentation](https://js.langchain.com/docs/concepts/text_splitters){:target=_blank .external-link} and [LangChain's recursively split by character documentation](https://v03.api.js.langchain.com/classes/langchain.text_splitter.RecursiveCharacterTextSplitter.html){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Chunk Size\",\n    \"name\": \"chunkSize\",\n    \"type\": \"number\",\n    \"default\": 1000\n  },\n  {\n    \"displayName\": \"Chunk Overlap\",\n    \"name\": \"chunkOverlap\",\n    \"type\": \"number\",\n    \"default\": 0\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"description\": \"Additional options to add\",\n    \"options\": [\n      {\n        \"displayName\": \"Split Code\",\n        \"name\": \"splitCode\",\n        \"default\": \"markdown\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"cpp\",\n            \"value\": \"cpp\"\n          },\n          {\n            \"name\": \"go\",\n            \"value\": \"go\"\n          },\n          {\n            \"name\": \"java\",\n            \"value\": \"java\"\n          },\n          {\n            \"name\": \"js\",\n            \"value\": \"js\"\n          },\n          {\n            \"name\": \"php\",\n            \"value\": \"php\"\n          },\n          {\n            \"name\": \"proto\",\n            \"value\": \"proto\"\n          },\n          {\n            \"name\": \"python\",\n            \"value\": \"python\"\n          },\n          {\n            \"name\": \"rst\",\n            \"value\": \"rst\"\n          },\n          {\n            \"name\": \"ruby\",\n            \"value\": \"ruby\"\n          },\n          {\n            \"name\": \"rust\",\n            \"value\": \"rust\"\n          },\n          {\n            \"name\": \"scala\",\n            \"value\": \"scala\"\n          },\n          {\n            \"name\": \"swift\",\n            \"value\": \"swift\"\n          },\n          {\n            \"name\": \"markdown\",\n            \"value\": \"markdown\"\n          },\n          {\n            \"name\": \"latex\",\n            \"value\": \"latex\"\n          },\n          {\n            \"name\": \"html\",\n            \"value\": \"html\"\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.textSplitterTokenSplitter",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Token Splitter",
    "description": "Split text into chunks by tokens",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Token Splitter node documentation\ndescription: Learn how to use the Token Splitter node in n8n. Follow technical documentation to integrate Token Splitter node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Token Splitter node\n\nThe Token Splitter node splits a raw text string by first converting the text into BPE tokens, then splits these tokens into chunks and converts the tokens within a single chunk back into text.\n\nOn this page, you'll find the node parameters for the Token Splitter node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Chunk Size**: Enter the number of characters in each chunk.\n* **Chunk Overlap**: Enter how much overlap to have between chunks.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'token-splitter') ]]\n\n## Related resources\n\nRefer to [LangChain's token documentation](https://js.langchain.com/docs/concepts/tokens/){:target=_blank .external-link} and [LangChain's text splitter documentation](https://js.langchain.com/docs/concepts/text_splitters/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to a document loader. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_document'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Chunk Size\",\n    \"name\": \"chunkSize\",\n    \"type\": \"number\",\n    \"default\": 1000\n  },\n  {\n    \"displayName\": \"Chunk Overlap\",\n    \"name\": \"chunkOverlap\",\n    \"type\": \"number\",\n    \"default\": 0\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolCalculator",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Calculator",
    "description": "Make it easier for AI agents to perform arithmetic",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Calculator node documentation\ndescription: Learn how to use the Calculator node in n8n. Follow technical documentation to integrate Calculator node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Calculator node\n\nThe Calculator node is a [tool](/glossary.md#ai-tool) that allows an [agent](/glossary.md#ai-agent) to run mathematical calculations.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'calculator') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolCode",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Code Tool",
    "description": "Write a tool in JS or Python",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Custom Code Tool node documentation\ndescription: Learn how to use the Custom Code Tool node in n8n. Follow technical documentation to integrate Custom Code Tool node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Custom Code Tool node\n\nUse the Custom Code Tool node to write code that an [agent](/glossary.md#ai-agent) can run.\n\nOn this page, you'll find the node parameters for the Custom Code Tool node and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Description\n\nGive your custom code a description. This tells the agent when to use this tool. For example:\n\n> Call this tool to get a random color. The input should be a string with comma separated names of colors to exclude.\n\n### Language\n\nYou can use JavaScript or Python.\n\n### JavaScript / Python box\n\nWrite the code here.\n\nYou can access the tool input using `query`. For example, to take the input string and lowercase it:\n\n```js\nlet myString = query;\nreturn myString.toLowerCase();\n```\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'code-tool') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"See an example of a conversational agent with custom tool written in JavaScript <a href=\\\"/templates/1963\\\" target=\\\"_blank\\\">here</a>.\",\n    \"name\": \"noticeTemplateExample\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the function to be called, could contain letters, numbers, and underscores only\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1.1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"description\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  },\n  {\n    \"displayName\": \"Language\",\n    \"name\": \"language\",\n    \"type\": \"options\",\n    \"default\": \"javaScript\",\n    \"options\": [\n      {\n        \"name\": \"JavaScript\",\n        \"value\": \"javaScript\"\n      },\n      {\n        \"name\": \"Python (Beta)\",\n        \"value\": \"python\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"JavaScript\",\n    \"name\": \"jsCode\",\n    \"type\": \"string\",\n    \"default\": \"// Example: convert the incoming query to uppercase and return it\\nreturn query.toUpperCase()\",\n    \"description\": \"E.g. Converts any text to uppercase\",\n    \"displayOptions\": {\n      \"show\": {\n        \"language\": [\n          \"javaScript\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"editor\": \"jsEditor\"\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Python\",\n    \"name\": \"pythonCode\",\n    \"type\": \"string\",\n    \"default\": \"# Example: convert the incoming query to uppercase and return it\\nreturn query.upper()\",\n    \"description\": \"E.g. Converts any text to uppercase\",\n    \"displayOptions\": {\n      \"show\": {\n        \"language\": [\n          \"python\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"editor\": \"codeNodeEditor\",\n      \"editorLanguage\": \"python\"\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Specify Input Schema\",\n    \"name\": \"specifyInputSchema\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to specify the schema for the function. This would require the LLM to provide the input in the correct format and would validate it against the schema.\",\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Schema Type\",\n    \"name\": \"schemaType\",\n    \"type\": \"options\",\n    \"default\": \"fromJson\",\n    \"description\": \"How to specify the schema for the function\",\n    \"options\": [\n      {\n        \"name\": \"Generate From JSON Example\",\n        \"value\": \"fromJson\",\n        \"description\": \"Generate a schema from an example JSON object\"\n      },\n      {\n        \"name\": \"Define using JSON Schema\",\n        \"value\": \"manual\",\n        \"description\": \"Define the JSON schema manually\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"specifyInputSchema\": [\n          true\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"JSON Example\",\n    \"name\": \"jsonSchemaExample\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\t\\\"some_input\\\": \\\"some_value\\\"\\n}\",\n    \"description\": \"Example JSON object to use to generate the schema\",\n    \"displayOptions\": {\n      \"show\": {\n        \"specifyInputSchema\": [\n          true\n        ],\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"All properties will be required. To make them optional, use the 'JSON Schema' schema type instead\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"specifyInputSchema\": [\n          true\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.3\n            }\n          }\n        ],\n        \"schemaType\": [\n          \"fromJson\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Input Schema\",\n    \"name\": \"inputSchema\",\n    \"type\": \"json\",\n    \"default\": \"{\\n\\\"type\\\": \\\"object\\\",\\n\\\"properties\\\": {\\n\\t\\\"some_input\\\": {\\n\\t\\t\\\"type\\\": \\\"string\\\",\\n\\t\\t\\\"description\\\": \\\"Some input to the function\\\"\\n\\t\\t}\\n\\t}\\n}\",\n    \"description\": \"Schema to use for the function\",\n    \"displayOptions\": {\n      \"show\": {\n        \"specifyInputSchema\": [\n          true\n        ],\n        \"schemaType\": [\n          \"manual\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    },\n    \"noDataExpression\": true\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolExecutor",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Tool Executor",
    "description": "Node to execute tools without an AI Agent",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Query\",\n    \"name\": \"query\",\n    \"type\": \"json\",\n    \"default\": \"{}\",\n    \"description\": \"Parameters to pass to the tool as JSON or string\"\n  },\n  {\n    \"displayName\": \"Tool Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the tool to execute if the connected tool is a toolkit\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolHttpRequest",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "HTTP Request Tool",
    "description": "Makes an HTTP request and returns the response data",
    "category": "output",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: HTTP Request Tool node documentation\ndescription: Learn how to use the HTTP Request Tool node in n8n. Follow technical documentation to integrate HTTP Request Tool node into your workflows.\nsearch:\n  exclude: true\ncontentType: [integration, reference]\n---\n\n# HTTP Request Tool node\n\n/// warning | Legacy tool version\nNew instances of the HTTP Request tool node that you add to workflows use the standard [HTTP Request](/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/index.md) node as a tool. This page is describes the legacy, standalone HTTP Request tool node.\n\nYou can identify which tool version is in your workflow by checking if the node has an **Add option** property when you open the node on the canvas. If that button is present, you're using the new version, not the one described on this page.\n///\n\nThe HTTP Request tool works just like the [HTTP Request](/integrations/builtin/core-nodes/n8n-nodes-base.httprequest/index.md) node, but it's designed to be used with an [AI agent](/glossary.md#ai-agent) as a tool to collect information from a website or API.\n\nOn this page, you'll find a list of operations the HTTP Request node supports and links to more resources.\n\n/// note | Credentials\nRefer to [HTTP Request credentials](/integrations/builtin/credentials/httprequest.md) for guidance on setting up authentication. \n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'http-request-tool') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to LLM what this tool does, better description would allow LLM to produce expected result\",\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  },\n  {\n    \"displayName\": \"Method\",\n    \"name\": \"method\",\n    \"type\": \"options\",\n    \"default\": \"GET\",\n    \"options\": [\n      {\n        \"name\": \"DELETE\",\n        \"value\": \"DELETE\"\n      },\n      {\n        \"name\": \"GET\",\n        \"value\": \"GET\"\n      },\n      {\n        \"name\": \"PATCH\",\n        \"value\": \"PATCH\"\n      },\n      {\n        \"name\": \"POST\",\n        \"value\": \"POST\"\n      },\n      {\n        \"name\": \"PUT\",\n        \"value\": \"PUT\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Tip: You can use a {placeholder} for any part of the request to be filled by the model. Provide more context about them in the placeholders section\",\n    \"name\": \"placeholderNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"URL\",\n    \"name\": \"url\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Authentication\",\n    \"name\": \"authentication\",\n    \"type\": \"options\",\n    \"default\": \"none\",\n    \"description\": \"Select the type of authentication to use if needed, authentication would be done by n8n and your credentials will not be shared with the LLM\",\n    \"options\": [\n      {\n        \"name\": \"None\",\n        \"value\": \"none\"\n      },\n      {\n        \"name\": \"Predefined Credential Type\",\n        \"value\": \"predefinedCredentialType\",\n        \"description\": \"We've already implemented auth for many services so that you don't have to set it up manually\"\n      },\n      {\n        \"name\": \"Generic Credential Type\",\n        \"value\": \"genericCredentialType\",\n        \"description\": \"Fully customizable. Choose between basic, header, OAuth2, etc.\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Credential Type\",\n    \"name\": \"nodeCredentialType\",\n    \"type\": \"credentialsSelect\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"predefinedCredentialType\"\n        ]\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Make sure you have specified the scope(s) for the Service Account in the credential\",\n    \"name\": \"googleApiWarning\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"nodeCredentialType\": [\n          \"googleApi\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Generic Auth Type\",\n    \"name\": \"genericAuthType\",\n    \"type\": \"credentialsSelect\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"authentication\": [\n          \"genericCredentialType\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Send Query Parameters\",\n    \"name\": \"sendQuery\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether the request has query params or not\",\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Specify Query Parameters\",\n    \"name\": \"specifyQuery\",\n    \"type\": \"options\",\n    \"default\": \"keypair\",\n    \"options\": [\n      {\n        \"name\": \"Using Fields Below\",\n        \"value\": \"keypair\"\n      },\n      {\n        \"name\": \"Using JSON Below\",\n        \"value\": \"json\"\n      },\n      {\n        \"name\": \"Let Model Specify Entire Body\",\n        \"value\": \"model\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendQuery\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Query Parameters\",\n    \"name\": \"parametersQuery\",\n    \"type\": \"fixedCollection\",\n    \"default\": {\n      \"values\": [\n        {\n          \"name\": \"\"\n        }\n      ]\n    },\n    \"options\": [\n      {\n        \"name\": \"values\",\n        \"displayName\": \"Values\",\n        \"values\": [\n          {\n            \"displayName\": \"Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Value Provided\",\n            \"name\": \"valueProvider\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"By Model (and is required)\",\n                \"value\": \"modelRequired\"\n              },\n              {\n                \"name\": \"By Model (but is optional)\",\n                \"value\": \"modelOptional\"\n              },\n              {\n                \"name\": \"Using Field Below\",\n                \"value\": \"fieldValue\"\n              }\n            ],\n            \"default\": \"modelRequired\"\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"value\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"hint\": \"Use a {placeholder} for any data to be filled in by the model\",\n            \"displayOptions\": {\n              \"show\": {\n                \"valueProvider\": [\n                  \"fieldValue\"\n                ]\n              }\n            }\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendQuery\": [\n          true\n        ],\n        \"specifyQuery\": [\n          \"keypair\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"JSON\",\n    \"name\": \"jsonQuery\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sendQuery\": [\n          true\n        ],\n        \"specifyQuery\": [\n          \"json\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 5\n    }\n  },\n  {\n    \"displayName\": \"Send Headers\",\n    \"name\": \"sendHeaders\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether the request has headers or not\",\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Specify Headers\",\n    \"name\": \"specifyHeaders\",\n    \"type\": \"options\",\n    \"default\": \"keypair\",\n    \"options\": [\n      {\n        \"name\": \"Using Fields Below\",\n        \"value\": \"keypair\"\n      },\n      {\n        \"name\": \"Using JSON Below\",\n        \"value\": \"json\"\n      },\n      {\n        \"name\": \"Let Model Specify Entire Body\",\n        \"value\": \"model\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendHeaders\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Header Parameters\",\n    \"name\": \"parametersHeaders\",\n    \"type\": \"fixedCollection\",\n    \"default\": {\n      \"values\": [\n        {\n          \"name\": \"\"\n        }\n      ]\n    },\n    \"options\": [\n      {\n        \"name\": \"values\",\n        \"displayName\": \"Values\",\n        \"values\": [\n          {\n            \"displayName\": \"Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Value Provided\",\n            \"name\": \"valueProvider\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"By Model (and is required)\",\n                \"value\": \"modelRequired\"\n              },\n              {\n                \"name\": \"By Model (but is optional)\",\n                \"value\": \"modelOptional\"\n              },\n              {\n                \"name\": \"Using Field Below\",\n                \"value\": \"fieldValue\"\n              }\n            ],\n            \"default\": \"modelRequired\"\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"value\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"hint\": \"Use a {placeholder} for any data to be filled in by the model\",\n            \"displayOptions\": {\n              \"show\": {\n                \"valueProvider\": [\n                  \"fieldValue\"\n                ]\n              }\n            }\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendHeaders\": [\n          true\n        ],\n        \"specifyHeaders\": [\n          \"keypair\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"JSON\",\n    \"name\": \"jsonHeaders\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sendHeaders\": [\n          true\n        ],\n        \"specifyHeaders\": [\n          \"json\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 5\n    }\n  },\n  {\n    \"displayName\": \"Send Body\",\n    \"name\": \"sendBody\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether the request has body or not\",\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Specify Body\",\n    \"name\": \"specifyBody\",\n    \"type\": \"options\",\n    \"default\": \"keypair\",\n    \"options\": [\n      {\n        \"name\": \"Using Fields Below\",\n        \"value\": \"keypair\"\n      },\n      {\n        \"name\": \"Using JSON Below\",\n        \"value\": \"json\"\n      },\n      {\n        \"name\": \"Let Model Specify Entire Body\",\n        \"value\": \"model\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendBody\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Body Parameters\",\n    \"name\": \"parametersBody\",\n    \"type\": \"fixedCollection\",\n    \"default\": {\n      \"values\": [\n        {\n          \"name\": \"\"\n        }\n      ]\n    },\n    \"options\": [\n      {\n        \"name\": \"values\",\n        \"displayName\": \"Values\",\n        \"values\": [\n          {\n            \"displayName\": \"Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Value Provided\",\n            \"name\": \"valueProvider\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"By Model (and is required)\",\n                \"value\": \"modelRequired\"\n              },\n              {\n                \"name\": \"By Model (but is optional)\",\n                \"value\": \"modelOptional\"\n              },\n              {\n                \"name\": \"Using Field Below\",\n                \"value\": \"fieldValue\"\n              }\n            ],\n            \"default\": \"modelRequired\"\n          },\n          {\n            \"displayName\": \"Value\",\n            \"name\": \"value\",\n            \"type\": \"string\",\n            \"default\": \"\",\n            \"hint\": \"Use a {placeholder} for any data to be filled in by the model\",\n            \"displayOptions\": {\n              \"show\": {\n                \"valueProvider\": [\n                  \"fieldValue\"\n                ]\n              }\n            }\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"sendBody\": [\n          true\n        ],\n        \"specifyBody\": [\n          \"keypair\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"JSON\",\n    \"name\": \"jsonBody\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"sendBody\": [\n          true\n        ],\n        \"specifyBody\": [\n          \"json\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 5\n    }\n  },\n  {\n    \"displayName\": \"Placeholder Definitions\",\n    \"name\": \"placeholderDefinitions\",\n    \"type\": \"fixedCollection\",\n    \"default\": [],\n    \"options\": [\n      {\n        \"name\": \"values\",\n        \"displayName\": \"Values\",\n        \"values\": [\n          {\n            \"displayName\": \"Placeholder Name\",\n            \"name\": \"name\",\n            \"type\": \"string\",\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Description\",\n            \"name\": \"description\",\n            \"type\": \"string\",\n            \"default\": \"\"\n          },\n          {\n            \"displayName\": \"Type\",\n            \"name\": \"type\",\n            \"type\": \"options\",\n            \"options\": [\n              {\n                \"name\": \"Not Specified (Default)\",\n                \"value\": \"not specified\"\n              },\n              {\n                \"name\": \"String\",\n                \"value\": \"string\"\n              },\n              {\n                \"name\": \"Number\",\n                \"value\": \"number\"\n              },\n              {\n                \"name\": \"Boolean\",\n                \"value\": \"boolean\"\n              },\n              {\n                \"name\": \"JSON\",\n                \"value\": \"json\"\n              }\n            ],\n            \"default\": \"not specified\"\n          }\n        ]\n      }\n    ],\n    \"typeOptions\": {\n      \"multipleValues\": true\n    }\n  },\n  {\n    \"displayName\": \"Optimize Response\",\n    \"name\": \"optimizeResponse\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether the optimize the tool response to reduce amount of data passed to the LLM that could lead to better result and reduce cost\",\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Expected Response Type\",\n    \"name\": \"responseType\",\n    \"type\": \"options\",\n    \"default\": \"json\",\n    \"options\": [\n      {\n        \"name\": \"JSON\",\n        \"value\": \"json\"\n      },\n      {\n        \"name\": \"HTML\",\n        \"value\": \"html\"\n      },\n      {\n        \"name\": \"Text\",\n        \"value\": \"text\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Field Containing Data\",\n    \"name\": \"dataField\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Specify the name of the field in the response containing the data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"json\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Fields\",\n    \"name\": \"fieldsToInclude\",\n    \"type\": \"options\",\n    \"default\": \"all\",\n    \"description\": \"What fields response object should include\",\n    \"options\": [\n      {\n        \"name\": \"All\",\n        \"value\": \"all\",\n        \"description\": \"Include all fields\"\n      },\n      {\n        \"name\": \"Selected\",\n        \"value\": \"selected\",\n        \"description\": \"Include only fields specified below\"\n      },\n      {\n        \"name\": \"Except\",\n        \"value\": \"except\",\n        \"description\": \"Exclude fields specified below\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"json\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Fields\",\n    \"name\": \"fields\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Comma-separated list of the field names. Supports dot notation. You can drag the selected fields from the input panel.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"json\"\n        ]\n      },\n      \"hide\": {\n        \"fieldsToInclude\": [\n          \"all\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Selector (CSS)\",\n    \"name\": \"cssSelector\",\n    \"type\": \"string\",\n    \"default\": \"body\",\n    \"description\": \"Select specific element(e.g. body) or multiple elements(e.g. div) of chosen type in the response HTML.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"html\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Return Only Content\",\n    \"name\": \"onlyContent\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to return only content of html elements, stripping html tags and attributes\",\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"html\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Elements To Omit\",\n    \"name\": \"elementsToOmit\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Comma-separated list of selectors that would be excluded when extracting content\",\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"html\"\n        ],\n        \"onlyContent\": [\n          true\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Truncate Response\",\n    \"name\": \"truncateResponse\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"text\",\n          \"html\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Max Response Characters\",\n    \"name\": \"maxLength\",\n    \"type\": \"number\",\n    \"default\": 1000,\n    \"displayOptions\": {\n      \"show\": {\n        \"optimizeResponse\": [\n          true\n        ],\n        \"responseType\": [\n          \"text\",\n          \"html\"\n        ],\n        \"truncateResponse\": [\n          true\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"minValue\": 1\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolSearXng",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "SearXNG",
    "description": "Search in SearXNG",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: SearXNG Tool node documentation\ndescription: Learn how to use the SearXNG Tool node in n8n. Follow technical documentation to integrate SearXNG Tool node into your workflows.\ncontentType: [integration, reference]\n---\n\n# SearXNG Tool node\n\nThe SearXNG Tool node allows you to integrate search capabilities into your workflows using SearXNG. SearXNG aggregates results from multiple search engines without tracking you.\n\nOn this page, you'll find the node options for the SearXNG Tool node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/searxng.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node Options\n\n* **Number of Results**: The number of results to retrieve. The default is 10.\n* **Page Number**: The page number of the search results to retrieve. The default is 1.\n* **Language**: A two-letter [language code](https://en.wikipedia.org/wiki/List_of_ISO_639_language_codes) to filter search results by language. For example: `en` for English, `fr` for French. The default is `en`.\n* **Safe Search**: Enables or disables filtering explicit content in the search results. Can be None, Moderate, or Strict. The default is None.\n\n## Running a SearXNG instance\n\nThis node requires running the SearXNG service on the same network as your n8n instance. Ensure your n8n instance has network access to the SearXNG service.\n\nThis node requires results in JSON format, which isn't enabled in the default SearXNG configuration. To enable JSON output, add `json` to the `search.formats` section of your SearXNG instance's `settings.yml` file:\n\n```yaml\nsearch:\n  # options available for formats: [html, csv, json, rss]\n  formats:\n    - html\n    - json\n```\n\nIf the `formats` section isn't there, add it. The exact location of the `settings.yml` file depends on how you installed SearXNG. You can find more by visiting the [SearXNG configuration documentation](https://docs.searxng.org/admin/installation-searxng.html#configuration).\n\nThe quality and availability of search results depend on the configuration and health of the SearXNG instance you use. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'searxng') ]]\n\n## Related resources\n\nRefer to [SearXNG's documentation](https://docs.searxng.org/) for more information about the service. You can also view [LangChain's documentation on their SearXNG integration](https://python.langchain.com/docs/integrations/tools/searx_search/).\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Number of Results\",\n        \"name\": \"numResults\",\n        \"type\": \"number\",\n        \"default\": 10\n      },\n      {\n        \"displayName\": \"Search Page Number\",\n        \"name\": \"pageNumber\",\n        \"type\": \"number\",\n        \"default\": 1\n      },\n      {\n        \"displayName\": \"Language\",\n        \"name\": \"language\",\n        \"type\": \"string\",\n        \"default\": \"en\",\n        \"description\": \"Defines the language to use. It's a two-letter language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French). Head to <a href=\\\"https://docs.searxng.org/user/search-syntax.html#select-language\\\">SearXNG search syntax page</a> for more info.\"\n      },\n      {\n        \"displayName\": \"Safe Search\",\n        \"name\": \"safesearch\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"None\",\n            \"value\": 0\n          },\n          {\n            \"name\": \"Moderate\",\n            \"value\": 1\n          },\n          {\n            \"name\": \"Strict\",\n            \"value\": 2\n          }\n        ],\n        \"default\": 0,\n        \"description\": \"Filter search results of engines which support safe search\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"searXngApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolSerpApi",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "SerpApi (Google Search)",
    "description": "Search in Google using SerpAPI",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: SerpApi (Google Search) node documentation\ndescription: Learn how to use the SerpApi (Google Search) node in n8n. Follow technical documentation to integrate SerpApi (Google Search) node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# SerpApi (Google Search) node\n\nThe SerpAPI node allows an [agent](/glossary.md#ai-agent) in your workflow to call Google's Search API.\n\nOn this page, you'll find the node parameters for the SerpAPI node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/serp.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node options\n\n* **Country**: Enter the country code you'd like to use. Refer to [Google GL Parameter: Supported Google Countries](https://serpapi.com/google-countries){:target=_blank .external-link} for supported countries and country codes.\n* **Device**: Select the device to use to get the search results.\n* **Explicit Array**: Choose whether to force SerpApi to fetch the Google results even if a cached version is already present (turned on) or not (turned off).\n* **Google Domain**: Enter the Google Domain to use. Refer to [Supported Google Domains](https://serpapi.com/google-domains) for supported domains.\n* **Language**: Enter the language code you'd like to use. Refer to [Google HL Parameter: Supported Google Languages](https://serpapi.com/google-languages){:target=_blank .external-link} for supported languages and language codes.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'serpapi') ]]\n\n## Related resources\n\nRefer to [Serp's documentation](https://serpapi.com/search-api){:target=_blank .external-link} for more information about the service. You can also view [LangChain's documentation on their Serp integration](https://js.langchain.com/docs/integrations/tools/serpapi/){:target=_blank .external-link}.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Country\",\n        \"name\": \"gl\",\n        \"type\": \"string\",\n        \"default\": \"us\",\n        \"description\": \"Defines the country to use for search. Head to <a href=\\\"https://serpapi.com/google-countries\\\">Google countries page</a> for a full list of supported countries.\"\n      },\n      {\n        \"displayName\": \"Device\",\n        \"name\": \"device\",\n        \"type\": \"options\",\n        \"options\": [\n          {\n            \"name\": \"Desktop\",\n            \"value\": \"desktop\"\n          },\n          {\n            \"name\": \"Mobile\",\n            \"value\": \"mobile\"\n          },\n          {\n            \"name\": \"Tablet\",\n            \"value\": \"tablet\"\n          }\n        ],\n        \"default\": \"desktop\",\n        \"description\": \"Device to use to get the results\"\n      },\n      {\n        \"displayName\": \"Explicit Array\",\n        \"name\": \"no_cache\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to force SerpApi to fetch the Google results even if a cached version is already present. Cache expires after 1h. Cached searches are free, and are not counted towards your searches per month.\"\n      },\n      {\n        \"displayName\": \"Google Domain\",\n        \"name\": \"google_domain\",\n        \"type\": \"string\",\n        \"default\": \"google.com\",\n        \"description\": \"Defines the domain to use for search. Head to <a href=\\\"https://serpapi.com/google-domains\\\">Google domains page</a> for a full list of supported domains.\"\n      },\n      {\n        \"displayName\": \"Language\",\n        \"name\": \"hl\",\n        \"type\": \"string\",\n        \"default\": \"en\",\n        \"description\": \"Defines the language to use. It's a two-letter language code. (e.g., `en` for English, `es` for Spanish, or `fr` for French). Head to <a href=\\\"https://serpapi.com/google-languages\\\">Google languages page</a> for a full list of supported languages.\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"serpApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolThink",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Think Tool",
    "description": "Invite the AI agent to do some thinking",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Think Tool node documentation\ndescription: Learn how to use the Think Tool node in n8n. Follow technical documentation to integrate the Tool Think node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Think Tool node\n\nThe Think Tool node is a [tool](/glossary.md#ai-tool) that invites an [agent](/glossary.md#ai-agent) to do some thinking before answering. Essentially, it allows the agent to reflect on the question by talking to itself before providing an answer. In particular, this is useful for complex queries that require a thoughtful response.\n\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'think-tool') ]]\n\n### Prompting example\n\nAs an example, say you ask the agent:\n\n<!-- vale off -->\n> *Jack is looking at Anne. Anne is looking at George. Jack is married, George is not, and we don't know if Anne is married. Is a married person looking at an unmarried person?*\n<!-- vale on -->\n\t\nThe agent will do its thinking with the tool:\n\n<!-- vale off -->\n> *We have the following information: 1. Jack is looking at Anne. 2. Anne is looking at George. 3. Jack is married. 4. George is not married. 5. We don't know if Anne is married or not.*\n\n> *We need to determine if a married person is looking at an unmarried person. Let's consider both cases for Anne:*\n\n> *Case 1: Anne is married.*\n\n> - *If Anne is married, then she (married) is looking at George (unmarried). In this case, a married person (Anne) is looking at an unmarried person (George).*\n\n> *Case 2: Anne is unmarried.*\n\n> - *If Anne is unmarried, then Jack (married) is looking at Anne (unmarried). In this case, a married person (Jack) is looking at an unmarried person (Anne).*\n\n> *In both scenarios, we find that a married person is looking at an unmarried person. Therefore, the answer to the question is yes.*\n<!-- vale on -->\n\t\nand then afterward provide the answer:\n\n<!-- vale off -->\n> *Yes, a married person is looking at an unmarried person. In both possible scenarios regarding Anne's marital status, we find that a married individual (either Jack or Anne) is looking at an unmarried individual (either George or Anne).*\n<!-- vale on -->\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Think Tool Description\",\n    \"name\": \"description\",\n    \"type\": \"string\",\n    \"default\": \"Use the tool to think about something. It will not obtain new information or change the database, but just append the thought to the log. Use it when complex reasoning or some cache memory is needed.\",\n    \"description\": \"The thinking tool's description\",\n    \"required\": true,\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolVectorStore",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Vector Store Question Answer Tool",
    "description": "Answer questions with a vector store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Vector Store Question Answer Tool node documentation\ndescription: Learn how to use the Vector Store Question Answer Tool node in n8n. Follow technical documentation to integrate Vector Store Question Answer Tool node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Vector Store Question Answer Tool node\n\nThe Vector Store Question Answer node is a [tool](/glossary.md#ai-tool) that allows an [agent](/glossary.md#ai-agent) to summarize results and answer questions based on chunks from a [vector store](/glossary.md#ai-vector-store). \n\nOn this page, you'll find the node parameters for the Vector Store Question Answer node, and links to more resources.\n\n/// note | Examples and templates\nFor usage examples and templates to help you get started, refer to n8n's [Vector Store Question Answer Tool integrations](https://n8n.io/integrations/vector-store-tool/){:target=_blank .external-link} page.\n///\t\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Description of Data\n\nEnter a description of the data in the vector store.\n\n### Limit\n\nThe maximum number of results to return.\n\n## How n8n populates the tool description\n\nn8n uses the node name (select the name to edit) and **Description of Data** parameter to populate the tool description for AI agents using the following format:\n\n> Useful for when you need to answer questions about [node name]. Whenever you need information about [Description of Data], you should ALWAYS use this. Input should be a fully formed question.\n\nSpaces in the node name are converted to underscores in the tool description.\n\n/// warning | Avoid special characters in node names\nUsing special characters in the node name will cause errors when the agent runs:\n\n![model errors from special characters](/_images/integrations/builtin/cluster-nodes/toolvectorstore/name-characters-error.png)\n\nUse only alphanumeric characters, spaces, dashes, and underscores in node names.\n///\n\n## Related resources\n\nView [example workflows and related content](https://n8n.io/integrations/vector-store-tool/){:target=_blank .external-link} on n8n's website.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Data Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the data in vector store. This will be used to fill this tool description: Useful for when you need to answer questions about [name]. Whenever you need information about [data description], you should ALWAYS use this. Input should be a fully formed question.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          1\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description of Data\",\n    \"name\": \"description\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Describe the data in vector store. This will be used to fill this tool description: Useful for when you need to answer questions about [name]. Whenever you need information about [data description], you should ALWAYS use this. Input should be a fully formed question.\",\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"The maximum number of results to return\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolWikipedia",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Wikipedia",
    "description": "Search in Wikipedia",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Wikipedia node documentation\ndescription: Learn how to use the Wikipedia node in n8n. Follow technical documentation to integrate Wikipedia node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Wikipedia node\n\nThe Wikipedia node is a [tool](/glossary.md#ai-tool) that allows an [agent](/glossary.md#ai-agent) to search and return information from Wikipedia.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'wikipedia') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolWolframAlpha",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Wolfram|Alpha",
    "description": "Connects to WolframAlpha's computational intelligence engine.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Wolfram|Alpha tool node documentation\ndescription: Learn how to use the Wolfram|Alpha tool node in n8n. Follow technical documentation to integrate Wolfram|Alpha tool node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Wolfram|Alpha tool node\n\nUse the Wolfram|Alpha tool node to connect your [agents](/glossary.md#ai-agent) and [chains](/glossary.md#ai-chain) to Wolfram|Alpha's computational intelligence engine.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/wolframalpha.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'wolframoralpha') ]]\n\n## Related resources\n\nRefer to [Wolfram|Alpha's documentation](https://products.wolframalpha.com/api){:target=_blank .external-link} for more information about the service. You can also view [LangChain's documentation on their WolframAlpha Tool](https://js.langchain.com/docs/integrations/tools/wolframalpha/){:target=_blank .external-link}.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"wolframAlphaApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.toolWorkflow",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Call n8n Sub-Workflow Tool",
    "description": "Uses another n8n workflow as a tool. Allows packaging any n8n node(s) as a tool.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "2.2",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Call n8n Workflow Tool node documentation\ndescription: Learn how to use the Call n8n Workflow Tool node in n8n. Follow technical documentation to integrate Call n8n Workflow Tool node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Call n8n Workflow Tool node\n\nThe Call n8n Workflow Tool node is a [tool](/glossary.md#ai-tool) that allows an [agent](/glossary.md#ai-agent) to run another n8n workflow and fetch its output data. \n\nOn this page, you'll find the node parameters for the Call n8n Workflow Tool node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Description\n\nEnter a custom code a description. This tells the agent when to use this tool. For example:\n\n> Call this tool to get a random color. The input should be a string with comma separated names of colors to exclude.\n\n### Source\n\nTell n8n which workflow to call. You can choose either:\n\n* **Database** to select the workflow from a list or enter a workflow ID.\n* **Define Below** and copy in a complete [workflow JSON](/workflows/export-import.md).\n\n### Workflow Inputs\n\nWhen using **Database** as workflow source, once you choose a sub-workflow (and define the **Workflow Input Schema** in the sub-workflow), you can define the **Workflow Inputs**.\n\nSelect the **Refresh** button to pull in the input fields from the sub-workflow.\n\nYou can define the workflow input values using any combination of the following options:\n\n* providing fixed values\n* using expressions to reference data from the current workflow\n* [letting the AI model specify the parameter](/advanced-ai/examples/using-the-fromai-function.md) by selecting the button AI button on the right side of the field\n* using the [`$fromAI()` function](/advanced-ai/examples/using-the-fromai-function.md#use-the-fromai-function) in expressions to control the way the model fills in data and to mix AI generated input with other custom input\n\nTo reference data from the current workflow, drag fields from the input panel to the field with the Expressions mode selected.\n\nTo get started with the `$fromAI()` function, select the \"Let the model define this parameter\" button on the right side of the field and then use the **X** on the box to revert to user-defined values. The field will change to an expression field pre-populated with the `$fromAI()` expression. From here, you can customize the expression to add other static or dynamic content, or tweak the `$fromAI()` function parameters.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'workflow-tool') ]]\n\n## Related resources\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/tools-link.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"This node must be connected to an AI agent. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"See an example of a workflow to suggest meeting slots using AI <a href=\\\"/templates/1953\\\" target=\\\"_blank\\\">here</a>.\",\n    \"name\": \"noticeTemplateExample\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"name\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the function to be called, could contain letters, numbers, and underscores only\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 2.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"description\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"rows\": 3\n    }\n  },\n  {\n    \"displayName\": \"This tool will call the workflow you define below, and look in the last node for the response. The workflow needs to start with an Execute Workflow trigger\",\n    \"name\": \"executeNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Source\",\n    \"name\": \"source\",\n    \"type\": \"options\",\n    \"default\": \"database\",\n    \"description\": \"Where to get the workflow to execute from\",\n    \"options\": [\n      {\n        \"name\": \"Database\",\n        \"value\": \"database\",\n        \"description\": \"Load the workflow from the database by ID\"\n      },\n      {\n        \"name\": \"Define Below\",\n        \"value\": \"parameter\",\n        \"description\": \"Pass the JSON code of a workflow\"\n      }\n    ]\n  },\n  {\n    \"displayName\": \"Workflow\",\n    \"name\": \"workflowId\",\n    \"type\": \"workflowSelector\",\n    \"default\": \"\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"database\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Workflow Inputs\",\n    \"name\": \"workflowInputs\",\n    \"type\": \"resourceMapper\",\n    \"default\": {\n      \"mappingMode\": \"defineBelow\",\n      \"value\": null\n    },\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"database\"\n        ]\n      },\n      \"hide\": {\n        \"workflowId\": [\n          \"\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"loadOptionsDependsOn\": [\n        \"workflowId.value\"\n      ],\n      \"resourceMapper\": {\n        \"localResourceMapperMethod\": \"loadSubWorkflowInputs\",\n        \"valuesLabel\": \"Workflow Inputs\",\n        \"mode\": \"map\",\n        \"fieldWords\": {\n          \"singular\": \"workflow input\",\n          \"plural\": \"workflow inputs\"\n        },\n        \"addAllFields\": true,\n        \"multiKeyMatch\": false,\n        \"supportAutoMap\": false\n      }\n    },\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"Workflow JSON\",\n    \"name\": \"workflowJson\",\n    \"type\": \"json\",\n    \"default\": \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n    \"description\": \"The workflow JSON code to execute\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"source\": [\n          \"parameter\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 10\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreInMemory",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Simple Vector Store",
    "description": "The easiest way to experiment with vector stores, without external setup.",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Simple Vector Store node documentation\ndescription: Learn how to use the Simple Vector Store node in n8n. Follow technical documentation to integrate Simple Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Simple Vector Store node\n\nUse the Simple Vector Store node to store and retrieve [embeddings](/glossary.md#ai-embedding) in n8n's in-app memory. \n\nOn this page, you'll find the node parameters for the Simple Vector Store node, and links to more resources.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n/// note | This node is different from AI memory nodes\nThe simple vector storage described here is different to the AI memory nodes such as [Simple Memory](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.memorybufferwindow/index.md).\n\nThis node creates a [vector database](/glossary.md#ai-vector-store) in the app memory.\n///\n\n## Data safety limitations\n\nBefore using the Simple Vector Store node, it's important to understand its limitations and how it works.\n\n/// warning\nn8n recommends using Simple Vector store for development use only.\n///\n\n### Vector store data isn't persistent\n\nThis node stores data in memory only. All data is lost when n8n restarts and may also be purged in low-memory conditions.\n\n### All instance users can access vector store data\n\nMemory keys for the Simple Vector Store node are global, not scoped to individual workflows.\n\nThis means that all users of the instance can access vector store data by adding a Simple Vector Store node and selecting the memory key, regardless of the access controls set for the original workflow. Take care not to expose sensitive information when ingesting data with the Simple Vector Store node.\n\n## Node usage patterns\n\nYou can use the Simple Vector Store node in the following patterns.\n\n### Use as a regular node to insert and retrieve documents\n\nYou can use the Simple Vector Store as a regular node to insert or get documents. This pattern places the Simple Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of in step 2 of [this template](https://n8n.io/workflows/2465-building-your-first-whatsapp-chatbot/).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Simple Vector Store node directly to the [tool](/glossary.md#ai-tool) connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Simple Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Simple Vector Store node to fetch documents from the Simple Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Simple Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Simple Vector Store node. Rather than connecting the Simple Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2465-building-your-first-whatsapp-chatbot/) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Simple Vector store.\n\n## Memory Management\n\nThe Simple Vector Store implements memory management to prevent excessive memory usage:\n\n- Automatically cleans up old vector stores when memory pressure increases\n- Removes inactive stores that haven't been accessed for a configurable amount of time\n\n### Configuration Options\n\nYou can control memory usage with these environment variables:\n\n | Variable                      | Type   | Default | Description                                                                         |\n |-------------------------------|--------|---------|-------------------------------------------------------------------------------------|\n | `N8N_VECTOR_STORE_MAX_MEMORY` | Number | -1      | Maximum memory in MB allowed for all vector stores combined (-1 to disable limits). |\n | `N8N_VECTOR_STORE_TTL_HOURS`  | Number | -1      | Hours of inactivity after which a store gets removed (-1 to disable TTL).           |\n\nOn n8n Cloud, these values are preset to 100MB (about 8,000 documents, depending on document size and metadata) and 7 days respectively. For self-hosted instances, both values default to -1(no memory limits or time-based cleanup).\n\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Memory Key**: Select or create the key containing the vector memory you want to query.\n* **Prompt**: Enter the search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n\n### Insert Documents parameters\n\n* **Memory Key**: Select or create the key you want to store the vector memory as.\n* **Clear Store**: Use this parameter to control whether to wipe the vector store for the given memory key for this workflow before inserting data (turned on).\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Memory Key**: Select or create the key containing the vector memory you want to query.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Memory Key**: Select or create the key containing the vector memory you want to query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'in-memory-vector-store') ]]\n\n## Related resources\n\nRefer to [LangChains's Memory Vector Store documentation](https://js.langchain.com/docs/integrations/vectorstores/memory/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Memory Key\",\n    \"name\": \"memoryKey\",\n    \"type\": \"string\",\n    \"default\": \"vector_store_key\",\n    \"description\": \"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions.\",\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Memory Key\",\n    \"name\": \"memoryKey\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"vector_store_key\"\n    },\n    \"description\": \"The key to use to store the vector memory in the workflow data. These keys are shared between workflows.\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.2\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Clear Store\",\n    \"name\": \"clearStore\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to clear the store before inserting new data\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"<strong>For experimental use only</strong>: Data is stored in memory and will be lost if n8n restarts. Data may also be cleared if available memory gets low, and is accessible to all users of this instance. <a href=\\\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/\\\">More info</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"<strong>For experimental use only</strong>: Data is stored in memory and will be lost if n8n restarts. Data may also be cleared if available memory gets low, and is accessible to all users of this instance. <a href=\\\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/\\\">More info</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"<strong>For experimental use only</strong>: Data is stored in memory and will be lost if n8n restarts. Data may also be cleared if available memory gets low, and is accessible to all users of this instance. <a href=\\\"https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.vectorstoreinmemory/\\\">More info</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreInMemoryInsert",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "In Memory Vector Store Insert",
    "description": "Insert data into an in-memory vector store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"The embbded data are stored in the server memory, so they will be lost when the server is restarted. Additionally, if the amount of data is too large, it may cause the server to crash due to insufficient memory.\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Clear Store\",\n    \"name\": \"clearStore\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to clear the store before inserting new data\"\n  },\n  {\n    \"displayName\": \"Memory Key\",\n    \"name\": \"memoryKey\",\n    \"type\": \"string\",\n    \"default\": \"vector_store_key\",\n    \"description\": \"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions.\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreInMemoryLoad",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "In Memory Vector Store Load",
    "description": "Load embedded data from an in-memory vector store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Memory Key\",\n    \"name\": \"memoryKey\",\n    \"type\": \"string\",\n    \"default\": \"vector_store_key\",\n    \"description\": \"The key to use to store the vector memory in the workflow data. The key will be prefixed with the workflow ID to avoid collisions.\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreMilvus",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Milvus Vector Store",
    "description": "Work with your data in Milvus Vector Store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Milvus Vector Store node documentation\ndescription: Learn how to use the Milvus Vector Store node in n8n. Follow technical documentation to integrate Milvus Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Milvus Vector Store node\n\nUse the Milvus node to interact with your Milvus database as [vector store](/glossary.md#ai-vector-store). You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain), or connect directly to an [agent](/glossary.md#ai-agent) as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the Milvus node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/milvus.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node usage patterns\n\nYou can use the Milvus Vector Store node in the following patterns.\n\n### Use as a regular node to insert and retrieve documents\n\nYou can use the Milvus Vector Store as a regular node to insert, or get documents. This pattern places the Milvus Vector Store in the regular connection flow without using an agent.\n\nSee this [example template](https://n8n.io/workflows/3573-create-a-rag-system-with-paul-essays-milvus-and-openai-for-cited-answers/) for how to build a system that stores documents in Milvus and retrieves them to support cited, chat-based answers.\n\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Milvus Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Milvus Vector Store node. See this [example template](https://n8n.io/workflows/3576-paul-graham-essay-search-and-chat-with-milvus-vector-database/) where data is embedded and indexed in Milvus, and the AI Agent uses the vector store as a knowledge tool for question-answering.\n\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Milvus Vector Store node to fetch documents from the Milvus Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nA typical node connection flow looks like this: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Milvus Vector Store.\n\nCheck out this [workflow example](https://n8n.io/workflows/3574-create-a-paul-graham-essay-qanda-system-with-openai-and-milvus-vector-database/) to see how to ingest external data into Milvus and build a chat-based semantic Q&A system.\n\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Milvus Vector Store node. Rather than connecting the Milvus Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe connections flow would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Milvus Vector store.\n\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Milvus Collection**: Select or enter the Milvus Collection to use.\n* **Prompt**: Enter your search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Insert Documents parameters\n\n* **Milvus Collection**: Select or enter the Milvus Collection to use.\n* **Clear Collection**: Specify whether to clear the collection before inserting new documents.\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Milvus collection**: Select or enter the Milvus Collection to use.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Milvus Collection**: Select or enter the Milvus Collection to use.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n### Clear Collection\n\nAvailable in **Insert Documents** mode. Deletes all data from the collection before inserting the new data.\n\n## Related resources\n\nRefer to [LangChain's Milvus documentation](https://js.langchain.com/docs/integrations/vectorstores/milvus/) for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Milvus Collection\",\n    \"name\": \"milvusCollection\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Clear Collection\",\n        \"name\": \"clearCollection\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to clear the collection before inserting new data\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"milvusApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreMongoDBAtlas",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "MongoDB Atlas Vector Store",
    "description": "Work with your data in MongoDB Atlas Vector Store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: MongoDB Atlas Vector Store node documentation\ndescription: Learn how to use the MongoDB Atlas Vector Store node in n8n. Follow technical documentation to integrate MongoDB Atlas Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# MongoDB Atlas Vector Store node\n\nMongoDB Atlas Vector Search is a feature of MongoDB Atlas that enables users to store and query vector embeddings. Use this node to interact with Vector Search indexes in your MongoDB Atlas collections. You can insert documents, retrieve documents, and use the vector store in chains or as a tool for agents.\n\nOn this page, you'll find the node parameters for the MongoDB Atlas Vector Store node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/mongodb.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Prerequisites\n\nBefore using this node, create a [Vector Search index](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-type/) in your MongoDB Atlas collection. Follow these steps to create one:\n\n1. Log in to the [MongoDB Atlas dashboard](https://cloud.mongodb.com/).\n\n3. Select your organization and project.\n4. Find \"Search & Vector Search\" section.\n5. Select your cluster and click \"Go to search\".\n7. Click \"Create Search Index\".\n8. Choose \"Vector Search\" mode and use the visual or JSON editors. For example:\n   ```json\n   {\n     \"fields\": [\n       {\n         \"type\": \"vector\",\n         \"path\": \"<field-name>\",\n         \"numDimensions\": 1536, // any other value\n         \"similarity\": \"<similarity-function>\"\n       }\n     ]\n   }\n   ```\n\n9. Adjust the \"dimensions\" value according to your embedding model (For example, `1536` for OpenAI's `text-embedding-small-3`).\n10. Name your index and create.\n\nMake sure to note the following values which are required when configuring the node:\n\n- Collection name\n- Vector index name \n- Field names for embeddings and metadata\n\n## Node usage patterns\n\nYou can use the MongoDB Atlas Vector Store node in the following patterns:\n\n### Use as a regular node to insert and retrieve documents\n\nYou can use the MongoDB Atlas Vector Store as a regular node to insert or get documents. This pattern places the MongoDB Atlas Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of [this template](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/) (the template uses the Supabase Vector Store, but the pattern is the same).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the MongoDB Atlas Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use the vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> MongoDB Atlas Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the MongoDB Atlas Vector Store node to fetch documents from the MongoDB Atlas Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> MongoDB Atlas Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the MongoDB Atlas Vector Store node. Rather than connecting the MongoDB Atlas Vector Store directly as a tool, this pattern uses a tool specifically designed to summarize data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2465-building-your-first-whatsapp-chatbot/) (the linked example uses the In-Memory Vector Store, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> In-Memory Vector store.\n\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale off -->\n### Get Many parameters\n<!-- vale on -->\n\n- **Mongo Collection**: Enter the name of the MongoDB collection to use.\n- **Vector Index Name**: Enter the name of the Vector Search index in your MongoDB Atlas collection.\n- **Embedding Field**: Enter the field name in your documents that contains the vector embeddings.\n- **Metadata Field**: Enter the field name in your documents that contains the text metadata.\n\n### Insert Documents parameters\n\n- **Mongo Collection**: Enter the name of the MongoDB collection to use.\n- **Vector Index Name**: Enter the name of the Vector Search index in your MongoDB Atlas collection.\n- **Embedding Field**: Enter the field name in your documents that contains the vector embeddings.\n- **Metadata Field**: Enter the field name in your documents that contains the text metadata.\n\n### Retrieve Documents parameters (As Vector Store for Chain/Tool)\n\n- **Mongo Collection**: Enter the name of the MongoDB collection to use.\n- **Vector Index Name**: Enter the name of the Vector Search index in your MongoDB Atlas collection.\n- **Embedding Field**: Enter the field name in your documents that contains the vector embeddings.\n- **Metadata Field**: Enter the field name in your documents that contains the text metadata.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n- **Name**: The name of the vector store.\n- **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n- **Mongo Collection**: Enter the name of the MongoDB collection to use.\n- **Vector Index Name**: Enter the name of the Vector Search index in your MongoDB Atlas collection.\n- **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Options\n\n- **Metadata Filter**: Filters results based on metadata.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'mongodb-atlas-vector-store') ]]\n\n## Related resources\n\nRefer to:\n\n- [LangChain's MongoDB Atlas Vector Search documentation](https://js.langchain.com/docs/integrations/vectorstores/mongodb_atlas){:target=_blank .external-link} for more information about the service.\n- [MongoDB Atlas Vector Search documentation](https://www.mongodb.com/docs/atlas/atlas-vector-search/){:target=_blank .external-link} for more information about MongoDB Atlas Vector Search.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      },\n      {\n        \"name\": \"Update Documents\",\n        \"value\": \"update\",\n        \"description\": \"Update documents in vector store by ID\",\n        \"action\": \"Update vector store documents\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"MongoDB Collection\",\n    \"name\": \"mongoCollection\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding\",\n    \"name\": \"embedding\",\n    \"type\": \"string\",\n    \"default\": \"embedding\",\n    \"description\": \"The field with the embedding array\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Metadata Field\",\n    \"name\": \"metadata_field\",\n    \"type\": \"string\",\n    \"default\": \"text\",\n    \"description\": \"The text field of the raw data\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Vector Index Name\",\n    \"name\": \"vectorIndexName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"The name of the vector index\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Clear Namespace\",\n        \"name\": \"clearNamespace\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to clear documents in the namespace before inserting new data\"\n      },\n      {\n        \"displayName\": \"Namespace\",\n        \"name\": \"namespace\",\n        \"type\": \"string\",\n        \"description\": \"Logical partition for documents. Uses metadata.namespace field for filtering.\",\n        \"default\": \"\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Namespace\",\n        \"name\": \"namespace\",\n        \"type\": \"string\",\n        \"description\": \"Logical partition for documents. Uses metadata.namespace field for filtering.\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Namespace\",\n        \"name\": \"namespace\",\n        \"type\": \"string\",\n        \"description\": \"Logical partition for documents. Uses metadata.namespace field for filtering.\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"mongoDb\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStorePGVector",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Postgres PGVector Store",
    "description": "Work with your data in Postgresql with the PGVector extension",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: PGVector Vector Store node documentation\ndescription: Learn how to use the PGVector Vector Store node in n8n. Follow technical documentation to integrate PGVector Vector Store node into your workflows.\npriority: medium\n---\n\n# PGVector Vector Store node\n\nPGVector is an extension of Postgresql. Use this node to interact with the PGVector tables in your Postgresql database. You can insert documents into a vector table, get documents from a vector table, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain), or connect directly to an [agent](/glossary.md#ai-agent) as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the PGVector node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/postgres.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node usage patterns\n\nYou can use the PGVector Vector Store node in the following patterns.\n\n### Use as a regular node to insert and retrieve documents\n\nYou can use the PGVector Vector Store as a regular node to insert or get documents. This pattern places the PGVector Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of [this template](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/) (the template uses the Supabase Vector Store, but the pattern is the same).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the PGVector Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> PGVector Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the PGVector Vector Store node to fetch documents from the PGVector Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) (the linked example uses Pinecone, but the pattern is the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> PGVector Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the PGVector Vector Store node. Rather than connecting the PGVector Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2465-building-your-first-whatsapp-chatbot/) (the linked example uses the Simple Vector Store, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Simple Vector store.\n\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale off -->\n### Get Many parameters\n<!-- vale on -->\n\n* **Table name**: Enter the name of the table you want to query.\n* **Prompt**: Enter your search query.\n* **Limit**: Enter a number to set how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Insert Documents parameters\n\n* **Table name**: Enter the name of the table you want to query.\n\n### Retrieve Documents parameters (As Vector Store for Chain/Tool)\n\n* **Table name**: Enter the name of the table you want to query.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Table Name**: Enter the PGVector table to use.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Collection\n\nA way to separate datasets in PGVector. This creates a separate table and column to keep track of which collection a vector belongs to.\n\n* **Use Collection**: Select whether to use a collection (turned on) or not (turned off).\n* **Collection Name**: Enter the name of the collection you want to use.\n* **Collection Table Name**: Enter the name of the table to store collection information in.\n\n### Column Names\n\nThe following options specify the names of the columns to store the vectors and corresponding information in:\n\n* **ID Column Name**\n* **Vector Column Name**\n* **Content Column Name**\n* **Metadata Column Name**\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'postgres-pgvector-store') ]]\n\n## Related resources\n\nRefer to [LangChain's PGVector documentation](https://js.langchain.com/docs/integrations/vectorstores/pgvector){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Table Name\",\n    \"name\": \"tableName\",\n    \"type\": \"string\",\n    \"default\": \"n8n_vectors\",\n    \"description\": \"The table name to store the vectors in. If table does not exist, it will be created.\"\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Collection\",\n        \"name\": \"collection\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Collection of vectors\",\n        \"default\": {\n          \"values\": {\n            \"useCollection\": false,\n            \"collectionName\": \"n8n\",\n            \"collectionTable\": \"n8n_vector_collections\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Add Collection Settings\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Collection Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"Use Collection\",\n                \"name\": \"useCollection\",\n                \"type\": \"boolean\",\n                \"default\": false\n              },\n              {\n                \"displayName\": \"Collection Name\",\n                \"name\": \"collectionName\",\n                \"type\": \"string\",\n                \"default\": \"n8n\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              },\n              {\n                \"displayName\": \"Collection Table Name\",\n                \"name\": \"collectionTableName\",\n                \"type\": \"string\",\n                \"default\": \"n8n_vector_collections\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Column Names\",\n        \"name\": \"columnNames\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"The names of the columns in the PGVector table\",\n        \"default\": {\n          \"values\": {\n            \"idColumnName\": \"id\",\n            \"vectorColumnName\": \"embedding\",\n            \"contentColumnName\": \"text\",\n            \"metadataColumnName\": \"metadata\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Set Column Names\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Column Name Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"ID Column Name\",\n                \"name\": \"idColumnName\",\n                \"type\": \"string\",\n                \"default\": \"id\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Vector Column Name\",\n                \"name\": \"vectorColumnName\",\n                \"type\": \"string\",\n                \"default\": \"embedding\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Content Column Name\",\n                \"name\": \"contentColumnName\",\n                \"type\": \"string\",\n                \"default\": \"text\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Metadata Column Name\",\n                \"name\": \"metadataColumnName\",\n                \"type\": \"string\",\n                \"default\": \"metadata\",\n                \"required\": true\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Distance Strategy\",\n        \"name\": \"distanceStrategy\",\n        \"type\": \"options\",\n        \"default\": \"cosine\",\n        \"description\": \"The method to calculate the distance between two vectors\",\n        \"options\": [\n          {\n            \"name\": \"Cosine\",\n            \"value\": \"cosine\"\n          },\n          {\n            \"name\": \"Inner Product\",\n            \"value\": \"innerProduct\"\n          },\n          {\n            \"name\": \"Euclidean\",\n            \"value\": \"euclidean\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Collection\",\n        \"name\": \"collection\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Collection of vectors\",\n        \"default\": {\n          \"values\": {\n            \"useCollection\": false,\n            \"collectionName\": \"n8n\",\n            \"collectionTable\": \"n8n_vector_collections\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Add Collection Settings\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Collection Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"Use Collection\",\n                \"name\": \"useCollection\",\n                \"type\": \"boolean\",\n                \"default\": false\n              },\n              {\n                \"displayName\": \"Collection Name\",\n                \"name\": \"collectionName\",\n                \"type\": \"string\",\n                \"default\": \"n8n\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              },\n              {\n                \"displayName\": \"Collection Table Name\",\n                \"name\": \"collectionTableName\",\n                \"type\": \"string\",\n                \"default\": \"n8n_vector_collections\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Column Names\",\n        \"name\": \"columnNames\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"The names of the columns in the PGVector table\",\n        \"default\": {\n          \"values\": {\n            \"idColumnName\": \"id\",\n            \"vectorColumnName\": \"embedding\",\n            \"contentColumnName\": \"text\",\n            \"metadataColumnName\": \"metadata\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Set Column Names\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Column Name Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"ID Column Name\",\n                \"name\": \"idColumnName\",\n                \"type\": \"string\",\n                \"default\": \"id\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Vector Column Name\",\n                \"name\": \"vectorColumnName\",\n                \"type\": \"string\",\n                \"default\": \"embedding\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Content Column Name\",\n                \"name\": \"contentColumnName\",\n                \"type\": \"string\",\n                \"default\": \"text\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Metadata Column Name\",\n                \"name\": \"metadataColumnName\",\n                \"type\": \"string\",\n                \"default\": \"metadata\",\n                \"required\": true\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Distance Strategy\",\n        \"name\": \"distanceStrategy\",\n        \"type\": \"options\",\n        \"default\": \"cosine\",\n        \"description\": \"The method to calculate the distance between two vectors\",\n        \"options\": [\n          {\n            \"name\": \"Cosine\",\n            \"value\": \"cosine\"\n          },\n          {\n            \"name\": \"Inner Product\",\n            \"value\": \"innerProduct\"\n          },\n          {\n            \"name\": \"Euclidean\",\n            \"value\": \"euclidean\"\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Collection\",\n        \"name\": \"collection\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Collection of vectors\",\n        \"default\": {\n          \"values\": {\n            \"useCollection\": false,\n            \"collectionName\": \"n8n\",\n            \"collectionTable\": \"n8n_vector_collections\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Add Collection Settings\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Collection Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"Use Collection\",\n                \"name\": \"useCollection\",\n                \"type\": \"boolean\",\n                \"default\": false\n              },\n              {\n                \"displayName\": \"Collection Name\",\n                \"name\": \"collectionName\",\n                \"type\": \"string\",\n                \"default\": \"n8n\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              },\n              {\n                \"displayName\": \"Collection Table Name\",\n                \"name\": \"collectionTableName\",\n                \"type\": \"string\",\n                \"default\": \"n8n_vector_collections\",\n                \"required\": true,\n                \"displayOptions\": {\n                  \"show\": {\n                    \"useCollection\": [\n                      true\n                    ]\n                  }\n                }\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Column Names\",\n        \"name\": \"columnNames\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"The names of the columns in the PGVector table\",\n        \"default\": {\n          \"values\": {\n            \"idColumnName\": \"id\",\n            \"vectorColumnName\": \"embedding\",\n            \"contentColumnName\": \"text\",\n            \"metadataColumnName\": \"metadata\"\n          }\n        },\n        \"typeOptions\": {},\n        \"placeholder\": \"Set Column Names\",\n        \"options\": [\n          {\n            \"name\": \"values\",\n            \"displayName\": \"Column Name Settings\",\n            \"values\": [\n              {\n                \"displayName\": \"ID Column Name\",\n                \"name\": \"idColumnName\",\n                \"type\": \"string\",\n                \"default\": \"id\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Vector Column Name\",\n                \"name\": \"vectorColumnName\",\n                \"type\": \"string\",\n                \"default\": \"embedding\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Content Column Name\",\n                \"name\": \"contentColumnName\",\n                \"type\": \"string\",\n                \"default\": \"text\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Metadata Column Name\",\n                \"name\": \"metadataColumnName\",\n                \"type\": \"string\",\n                \"default\": \"metadata\",\n                \"required\": true\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"postgres\",\n    \"required\": true,\n    \"testedBy\": \"postgresConnectionTest\"\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStorePinecone",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Pinecone Vector Store",
    "description": "Work with your data in Pinecone Vector Store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Pinecone Vector Store node documentation\ndescription: Learn how to use the Pinecone Vector Store node in n8n. Follow technical documentation to integrate Pinecone Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Pinecone Vector Store node\n\nUse the Pinecone node to interact with your Pinecone database as [vector store](/glossary.md#ai-vector-store). You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain), or connect directly to an [agent](/glossary.md#ai-agent) as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the Pinecone node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/pinecone.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node usage patterns\n\nYou can use the Pinecone Vector Store node in the following patterns.\n\n### Use as a regular node to insert, update, and retrieve documents\n\nYou can use the Pinecone Vector Store as a regular node to insert, update, or get documents. This pattern places the Pinecone Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of [this template](https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Pinecone Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Pinecone Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Pinecone Vector Store node to fetch documents from the Pinecone Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Pinecone Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Pinecone Vector Store node. Rather than connecting the Pinecone Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2705-chat-with-github-api-documentation-rag-powered-chatbot-with-pinecone-and-openai/) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Pinecone Vector store.\n\t\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode-with-update.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Pinecone Index**: Select or enter the Pinecone Index to use.\n* **Prompt**: Enter your search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Insert Documents parameters\n\n* **Pinecone Index**: Select or enter the Pinecone Index to use.\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Pinecone Index**: Select or enter the Pinecone Index to use.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Pinecone Index**: Select or enter the Pinecone Index to use.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Pinecone Namespace \n\nAnother segregation option for how to store your data within the index.\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n### Clear Namespace\n\nAvailable in **Insert Documents** mode. Deletes all data from the namespace before inserting the new data.\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'pinecone-vector-store') ]]\n\n## Related resources\n\nRefer to [LangChain's Pinecone documentation](https://js.langchain.com/docs/integrations/vectorstores/pinecone/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n### Find your Pinecone index and namespace\n\nYour Pinecone index and namespace are available in your Pinecone account.\n\n![Screenshot of a Pinecone account, with the Pinecone index labelled](/_images/integrations/builtin/cluster-nodes/vectorstorepinecone/pinecone-index-namespace.png)\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      },\n      {\n        \"name\": \"Update Documents\",\n        \"value\": \"update\",\n        \"description\": \"Update documents in vector store by ID\",\n        \"action\": \"Update vector store documents\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Pinecone Index\",\n    \"name\": \"pineconeIndex\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Clear Namespace\",\n        \"name\": \"clearNamespace\",\n        \"type\": \"boolean\",\n        \"default\": false,\n        \"description\": \"Whether to clear the namespace before inserting new data\"\n      },\n      {\n        \"displayName\": \"Pinecone Namespace\",\n        \"name\": \"pineconeNamespace\",\n        \"type\": \"string\",\n        \"description\": \"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.\",\n        \"default\": \"\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Pinecone Namespace\",\n        \"name\": \"pineconeNamespace\",\n        \"type\": \"string\",\n        \"description\": \"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Pinecone Namespace\",\n        \"name\": \"pineconeNamespace\",\n        \"type\": \"string\",\n        \"description\": \"Partition the records in an index into namespaces. Queries and other operations are then limited to one namespace, so different requests can search different subsets of your index.\",\n        \"default\": \"\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"pineconeApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStorePineconeInsert",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Pinecone: Insert",
    "description": "Insert data into Pinecone Vector Store index",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Pinecone Index\",\n    \"name\": \"pineconeIndex\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Pinecone Namespace\",\n    \"name\": \"pineconeNamespace\",\n    \"type\": \"string\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Specify the document to load in the document loader sub-node\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Clear Namespace\",\n    \"name\": \"clearNamespace\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether to clear the namespace before inserting new data\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"pineconeApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStorePineconeLoad",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Pinecone: Load",
    "description": "Load data from Pinecone Vector Store index",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Pinecone Index\",\n    \"name\": \"pineconeIndex\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Pinecone Namespace\",\n    \"name\": \"pineconeNamespace\",\n    \"type\": \"string\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"pineconeApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreQdrant",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Qdrant Vector Store",
    "description": "Work with your data in a Qdrant collection",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Qdrant Vector Store node documentation\ndescription: Learn how to use the Qdrant Vector Store node in n8n. Follow technical documentation to integrate Qdrant Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Qdrant Vector Store node\n\nUse the Qdrant node to interact with your Qdrant collection as a [vector store](/glossary.md#ai-vector-store). You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain) or connect it directly to an [agent](/glossary.md#ai-agent) to use as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the Qdrant node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/qdrant.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node usage patterns\n\nYou can use the Qdrant Vector Store node in the following patterns.\n\n### Use as a regular node to insert and retrieve documents\n\nYou can use the Qdrant Vector Store as a regular node to insert or get documents. This pattern places the Qdrant Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in the first part of [this template](https://n8n.io/workflows/2440-building-rag-chatbot-for-movie-recommendations-with-qdrant-and-open-ai/).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Qdrant Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Qdrant Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Qdrant Vector Store node to fetch documents from the Qdrant Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/2183-ai-crew-to-automate-fundamental-stock-analysis-qanda-workflow/) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Qdrant Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Qdrant Vector Store node. Rather than connecting the Qdrant Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2464-scale-deal-flow-with-a-pitch-deck-ai-vision-chatbot-and-qdrant-vector-store/) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Qdrant Vector store.\n\t\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Qdrant collection name**: Enter the name of the Qdrant collection to use.\n* **Prompt**: Enter the search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\nThis Operation Mode includes one **Node option**, the [Metadata Filter](#metadata-filter).\n\n### Insert Documents parameters\n\n* **Qdrant collection name**: Enter the name of the Qdrant collection to use.\n\nThis Operation Mode includes one **Node option**:\n\n* **Collection Config**: Enter JSON options for creating a Qdrant collection creation configuration. Refer to the Qdrant [Collections](https://qdrant.tech/documentation/concepts/collections/){:target=_blank .external-link} documentation for more information.\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Qdrant Collection**: Enter the name of the Qdrant collection to use.\n\nThis Operation Mode includes one **Node option**, the [Metadata Filter](#metadata-filter).\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Qdrant Collection**: Enter the name of the Qdrant collection to use.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'qdrant-vector-store') ]]\n\n## Related resources\n\nRefer to [LangChain's Qdrant documentation](https://js.langchain.com/docs/integrations/vectorstores/qdrant){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Qdrant Collection\",\n    \"name\": \"qdrantCollection\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Collection Config\",\n        \"name\": \"collectionConfig\",\n        \"type\": \"json\",\n        \"default\": \"\",\n        \"description\": \"JSON options for creating a collection. <a href=\\\"https://qdrant.tech/documentation/concepts/collections\\\">Learn more</a>.\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Search Filter\",\n        \"name\": \"searchFilterJson\",\n        \"type\": \"json\",\n        \"typeOptions\": {\n          \"rows\": 5\n        },\n        \"default\": \"{\\n  \\\"should\\\": [\\n    {\\n      \\\"key\\\": \\\"metadata.batch\\\",\\n      \\\"match\\\": {\\n        \\\"value\\\": 12345\\n      }\\n    }\\n  ]\\n}\",\n        \"validateType\": \"object\",\n        \"description\": \"Filter pageContent or metadata using this <a href=\\\"https://qdrant.tech/documentation/concepts/filtering/\\\" target=\\\"_blank\\\">filtering syntax</a>\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Search Filter\",\n        \"name\": \"searchFilterJson\",\n        \"type\": \"json\",\n        \"typeOptions\": {\n          \"rows\": 5\n        },\n        \"default\": \"{\\n  \\\"should\\\": [\\n    {\\n      \\\"key\\\": \\\"metadata.batch\\\",\\n      \\\"match\\\": {\\n        \\\"value\\\": 12345\\n      }\\n    }\\n  ]\\n}\",\n        \"validateType\": \"object\",\n        \"description\": \"Filter pageContent or metadata using this <a href=\\\"https://qdrant.tech/documentation/concepts/filtering/\\\" target=\\\"_blank\\\">filtering syntax</a>\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"qdrantApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreSupabase",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Supabase Vector Store",
    "description": "Work with your data in Supabase Vector Store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Supabase Vector Store node documentation\ndescription: Learn how to use the Supabase Vector Store node in n8n. Follow technical documentation to integrate Supabase Vector Store node into your workflows.\ncontentType: [integration, reference]\npriority: medium\n---\n\n# Supabase Vector Store node\n\nUse the Supabase Vector Store to interact with your Supabase database as [vector store](/glossary.md#ai-vector-store). You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain), or connect it directly to an [agent](/glossary.md#ai-agent) to use as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the Supabase node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/supabase.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\t\nSupabase provides a [quickstart for setting up your vector store](https://supabase.com/docs/guides/ai/langchain?database-method=sql){:target=_blank .external-link}. If you use settings other than the defaults in the quickstart, this may affect parameter settings in n8n. Make sure you understand what you're doing.\n\n## Node usage patterns\n\nYou can use the Supabase Vector Store node in the following patterns.\n\n### Use as a regular node to insert, update, and retrieve documents\n\nYou can use the Supabase Vector Store as a regular node to insert, update, or get documents. This pattern places the Supabase Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of [this template](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Supabase Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Supabase Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Supabase Vector Store node to fetch documents from the Supabase Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Supabase Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Supabase Vector Store node. Rather than connecting the Supabase Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Supabase Vector store.\n\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode-with-update.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Table Name**: Enter the Supabase table to use.\n* **Prompt**: Enter the search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Insert Documents parameters\n\n* **Table Name**: Enter the Supabase table to use.\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Table Name**: Enter the Supabase table to use.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Table Name**: Enter the Supabase table to use.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Update Documents\n\n* **Table Name**: Enter the Supabase table to use.\n* **ID**: The ID of an embedding entry.\n\n## Node options\n\n### Query Name\n\nThe name of the matching function you set up in Supabase. If you follow the [Supabase quickstart](https://supabase.com/docs/guides/ai/langchain?database-method=sql){:target=_blank .external-link}, this will be `match_documents`.\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'supabase-vector-store') ]]\n\n## Related resources\n\nRefer to [LangChain's Supabase documentation](https://js.langchain.com/docs/integrations/vectorstores/supabase/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      },\n      {\n        \"name\": \"Update Documents\",\n        \"value\": \"update\",\n        \"description\": \"Update documents in vector store by ID\",\n        \"action\": \"Update vector store documents\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Table Name\",\n    \"name\": \"tableName\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Query Name\",\n        \"name\": \"queryName\",\n        \"type\": \"string\",\n        \"default\": \"match_documents\",\n        \"description\": \"Name of the query to use for matching documents\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Query Name\",\n        \"name\": \"queryName\",\n        \"type\": \"string\",\n        \"default\": \"match_documents\",\n        \"description\": \"Name of the query to use for matching documents\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Query Name\",\n        \"name\": \"queryName\",\n        \"type\": \"string\",\n        \"default\": \"match_documents\",\n        \"description\": \"Name of the query to use for matching documents\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Query Name\",\n        \"name\": \"queryName\",\n        \"type\": \"string\",\n        \"default\": \"match_documents\",\n        \"description\": \"Name of the query to use for matching documents\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"supabaseApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreSupabaseInsert",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Supabase: Insert",
    "description": "Insert data into Supabase Vector Store index [https://supabase.com/docs/guides/ai/langchain]",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Please refer to the <a href=\\\"https://supabase.com/docs/guides/ai/langchain\\\" target=\\\"_blank\\\">Supabase documentation</a> for more information on how to setup your database as a Vector Store.\",\n    \"name\": \"setupNotice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Table Name\",\n    \"name\": \"tableName\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Query Name\",\n    \"name\": \"queryName\",\n    \"type\": \"string\",\n    \"default\": \"match_documents\",\n    \"description\": \"Name of the query to use for matching documents\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Specify the document to load in the document loader sub-node\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"supabaseApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreSupabaseLoad",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Supabase: Load",
    "description": "Load data from Supabase Vector Store index",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Table Name\",\n    \"name\": \"tableName\",\n    \"type\": \"resourceLocator\",\n    \"default\": {\n      \"mode\": \"list\",\n      \"value\": \"\"\n    },\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Query Name\",\n    \"name\": \"queryName\",\n    \"type\": \"string\",\n    \"default\": \"match_documents\",\n    \"description\": \"Name of the query to use for matching documents\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"supabaseApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreZep",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Zep Vector Store",
    "description": "Work with your data in Zep Vector Store",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 1,
    "version": "1.3",
    "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Zep Vector Store node documentation\ndescription: Learn how to use the Zep Vector Store node in n8n. Follow technical documentation to integrate Zep Vector Store node into your workflows.\ncontentType: [integration, reference]\n---\n\n# Zep Vector Store node\n\nUse the Zep Vector Store to interact with Zep vector databases. You can insert documents into a vector database, get documents from a vector database, retrieve documents to provide them to a retriever connected to a [chain](/glossary.md#ai-chain), or connect it directly to an [agent](/glossary.md#ai-agent) to use as a [tool](/glossary.md#ai-tool).\n\nOn this page, you'll find the node parameters for the Zep Vector Store node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/zep.md).\n///\n\n/// note | Examples and templates\nFor usage examples and templates to help you get started, refer to n8n's [Zep Vector Store integrations](https://n8n.io/integrations/zep-vector-store/){:target=_blank .external-link} page.\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node usage patterns\n\nYou can use the Zep Vector Store node in the following patterns.\n\n### Use as a regular node to insert, update, and retrieve documents\n\nYou can use the Zep Vector Store as a regular node to insert or get documents. This pattern places the Zep Vector Store in the regular connection flow without using an agent.\n\nYou can see an example of this in scenario 1 of [this template](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/) (the example uses Supabase, but the pattern is the same).\n\n### Connect directly to an AI agent as a tool\n\nYou can connect the Zep Vector Store node directly to the tool connector of an [AI agent](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/index.md) to use a vector store as a resource when answering queries.\n\nHere, the connection would be: AI agent (tools connector) -> Zep Vector Store node.\n\n### Use a retriever to fetch documents\n\nYou can use the [Vector Store Retriever](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.retrievervectorstore.md) node with the Zep Vector Store node to fetch documents from the Zep Vector Store node. This is often used with the [Question and Answer Chain](/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.chainretrievalqa/index.md) node to fetch documents from the vector store that match the given chat input.\n\nAn [example of the connection flow](https://n8n.io/workflows/1960-ask-questions-about-a-pdf-using-ai/) (the example uses Pinecone, but the pattern in the same) would be: Question and Answer Chain (Retriever connector) -> Vector Store Retriever (Vector Store connector) -> Zep Vector Store.\n\n### Use the Vector Store Question Answer Tool to answer questions\n\nAnother pattern uses the [Vector Store Question Answer Tool](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.toolvectorstore.md) to summarize results and answer questions from the Zep Vector Store node. Rather than connecting the Zep Vector Store directly as a tool, this pattern uses a tool specifically designed to summarizes data in the vector store.\n\nThe [connections flow](https://n8n.io/workflows/2621-ai-agent-to-chat-with-files-in-supabase-storage/) (this example uses Supabase, but the pattern is the same) in this case would look like this: AI agent (tools connector) -> Vector Store Question Answer Tool (Vector Store connector) -> Zep Vector store.\n\t\n## Node parameters\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-mode.md\"\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/vector-store-rerank-results.md\"\n\n### Insert Documents parameters\n\n* **Collection Name**: Enter the collection name to store the data in.\n\n<!-- vale from-write-good.Weasel = NO -->\n### Get Many parameters\n<!-- vale from-write-good.Weasel = YES -->\n\n* **Collection Name**: Enter the collection name to retrieve the data from.\n* **Prompt**: Enter the search query.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n### Retrieve Documents (As Vector Store for Chain/Tool) parameters\n\n* **Collection Name**: Enter the collection name to retrieve the data from.\n\n### Retrieve Documents (As Tool for AI Agent) parameters\n\n* **Name**: The name of the vector store.\n* **Description**: Explain to the LLM what this tool does. A good, specific description allows LLMs to produce expected results more often.\n* **Collection Name**: Enter the collection name to retrieve the data from.\n* **Limit**: Enter how many results to retrieve from the vector store. For example, set this to `10` to get the ten best results.\n\n## Node options\n\n### Embedding Dimensions\n\nMust be the same when embedding the data and when querying it.\n\nThis sets the size of the array of floats used to represent the semantic meaning of a text document.\n\nRead more about Zep embeddings in [Zep's embeddings documentation](https://docs.getzep.com/deployment/embeddings/){:target=_blank .external-link}.\n\n### Is Auto Embedded\n\nAvailable in the **Insert Documents** Operation Mode, enabled by default.\n\nDisable this to configure your embeddings in Zep instead of in n8n.\n\n### Metadata Filter\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-root-nodes/vector-store-metadata-filter.md\"\n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'zep-vector-store') ]]\n\n## Related resources\n\nRefer to [LangChain's Zep documentation](https://js.langchain.com/docs/integrations/vectorstores/zep/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n--8<-- \"_glossary/ai-glossary.md\"\n",
    "properties_schema": "[\n  {\n    \"displayName\": \"Tip: Get a feel for vector stores in n8n with our\",\n    \"name\": \"ragStarterCallout\",\n    \"type\": \"callout\",\n    \"default\": \"\",\n    \"typeOptions\": {\n      \"calloutAction\": {\n        \"label\": \"RAG starter template\",\n        \"type\": \"openRagStarterTemplate\"\n      }\n    }\n  },\n  {\n    \"displayName\": \"Operation Mode\",\n    \"name\": \"mode\",\n    \"type\": \"options\",\n    \"default\": \"retrieve\",\n    \"options\": [\n      {\n        \"name\": \"Get Many\",\n        \"value\": \"load\",\n        \"description\": \"Get many ranked documents from vector store for query\",\n        \"action\": \"Get ranked documents from vector store\"\n      },\n      {\n        \"name\": \"Insert Documents\",\n        \"value\": \"insert\",\n        \"description\": \"Insert documents into vector store\",\n        \"action\": \"Add documents to vector store\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Vector Store for Chain/Tool)\",\n        \"value\": \"retrieve\",\n        \"description\": \"Retrieve documents from vector store to be used as vector store with AI nodes\",\n        \"action\": \"Retrieve documents for Chain/Tool as Vector Store\",\n        \"outputConnectionType\": \"ai_vectorStore\"\n      },\n      {\n        \"name\": \"Retrieve Documents (As Tool for AI Agent)\",\n        \"value\": \"retrieve-as-tool\",\n        \"description\": \"Retrieve documents from vector store to be used as tool with AI nodes\",\n        \"action\": \"Retrieve documents for AI Agent as Tool\",\n        \"outputConnectionType\": \"ai_tool\"\n      }\n    ],\n    \"noDataExpression\": true\n  },\n  {\n    \"displayName\": \"This node must be connected to a vector store retriever. <a data-action='openSelectiveNodeCreator' data-action-parameter-connectiontype='ai_retriever'>Insert one</a>\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"containerClass\": \"ndv-connection-hint-notice\"\n    }\n  },\n  {\n    \"displayName\": \"Name\",\n    \"name\": \"toolName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Name of the vector store\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"lte\": 1.2\n            }\n          }\n        ],\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Description\",\n    \"name\": \"toolDescription\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Explain to the LLM what this tool does, a good, specific description would allow LLMs to produce expected results much more often\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve-as-tool\"\n        ]\n      }\n    },\n    \"typeOptions\": {\n      \"rows\": 2\n    }\n  },\n  {\n    \"displayName\": \"Collection Name\",\n    \"name\": \"collectionName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Embedding Batch Size\",\n    \"name\": \"embeddingBatchSize\",\n    \"type\": \"number\",\n    \"default\": 200,\n    \"description\": \"Number of documents to embed in a single batch\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ],\n        \"@version\": [\n          {\n            \"_cnd\": {\n              \"gte\": 1.1\n            }\n          }\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Embedding Dimensions\",\n        \"name\": \"embeddingDimensions\",\n        \"type\": \"number\",\n        \"default\": 1536,\n        \"description\": \"Whether to allow using characters from the Unicode surrogate blocks\"\n      },\n      {\n        \"displayName\": \"Is Auto Embedded\",\n        \"name\": \"isAutoEmbedded\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to automatically embed documents when they are added\"\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"insert\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Prompt\",\n    \"name\": \"prompt\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"Search prompt to retrieve matching documents from the vector store using similarity-based ranking\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Limit\",\n    \"name\": \"topK\",\n    \"type\": \"number\",\n    \"default\": 4,\n    \"description\": \"Number of top results to fetch from vector store\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Include Metadata\",\n    \"name\": \"includeDocumentMetadata\",\n    \"type\": \"boolean\",\n    \"default\": true,\n    \"description\": \"Whether or not to include document metadata\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Rerank Results\",\n    \"name\": \"useReranker\",\n    \"type\": \"boolean\",\n    \"default\": false,\n    \"description\": \"Whether or not to rerank results\",\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"ID\",\n    \"name\": \"id\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"description\": \"ID of an embedding entry\",\n    \"required\": true,\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"update\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Embedding Dimensions\",\n        \"name\": \"embeddingDimensions\",\n        \"type\": \"number\",\n        \"default\": 1536,\n        \"description\": \"Whether to allow using characters from the Unicode surrogate blocks\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"load\",\n          \"retrieve-as-tool\"\n        ]\n      }\n    }\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Embedding Dimensions\",\n        \"name\": \"embeddingDimensions\",\n        \"type\": \"number\",\n        \"default\": 1536,\n        \"description\": \"Whether to allow using characters from the Unicode surrogate blocks\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ],\n    \"displayOptions\": {\n      \"show\": {\n        \"mode\": [\n          \"retrieve\"\n        ]\n      }\n    }\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"zepApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreZepInsert",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Zep Vector Store: Insert",
    "description": "Insert data into Zep Vector Store index",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Collection Name\",\n    \"name\": \"collectionName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Specify the document to load in the document loader sub-node\",\n    \"name\": \"notice\",\n    \"type\": \"notice\",\n    \"default\": \"\"\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Embedding Dimensions\",\n        \"name\": \"embeddingDimensions\",\n        \"type\": \"number\",\n        \"default\": 1536,\n        \"description\": \"Whether to allow using characters from the Unicode surrogate blocks\"\n      },\n      {\n        \"displayName\": \"Is Auto Embedded\",\n        \"name\": \"isAutoEmbedded\",\n        \"type\": \"boolean\",\n        \"default\": true,\n        \"description\": \"Whether to automatically embed documents when they are added\"\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"zepApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  },
  {
    "node_type": "nodes-langchain.vectorStoreZepLoad",
    "package_name": "@n8n/n8n-nodes-langchain",
    "display_name": "Zep Vector Store: Load",
    "description": "Load data from Zep Vector Store index",
    "category": "transform",
    "development_style": "programmatic",
    "is_ai_tool": 0,
    "is_trigger": 0,
    "is_webhook": 0,
    "is_versioned": 0,
    "version": "1",
    "documentation": null,
    "properties_schema": "[\n  {\n    \"displayName\": \"Collection Name\",\n    \"name\": \"collectionName\",\n    \"type\": \"string\",\n    \"default\": \"\",\n    \"required\": true\n  },\n  {\n    \"displayName\": \"Options\",\n    \"name\": \"options\",\n    \"type\": \"collection\",\n    \"default\": {},\n    \"options\": [\n      {\n        \"displayName\": \"Embedding Dimensions\",\n        \"name\": \"embeddingDimensions\",\n        \"type\": \"number\",\n        \"default\": 1536,\n        \"description\": \"Whether to allow using characters from the Unicode surrogate blocks\"\n      },\n      {\n        \"displayName\": \"Metadata Filter\",\n        \"name\": \"metadata\",\n        \"type\": \"fixedCollection\",\n        \"description\": \"Metadata to filter the document by\",\n        \"typeOptions\": {\n          \"multipleValues\": true\n        },\n        \"default\": {},\n        \"placeholder\": \"Add filter field\",\n        \"options\": [\n          {\n            \"name\": \"metadataValues\",\n            \"displayName\": \"Fields to Set\",\n            \"values\": [\n              {\n                \"displayName\": \"Name\",\n                \"name\": \"name\",\n                \"type\": \"string\",\n                \"default\": \"\",\n                \"required\": true\n              },\n              {\n                \"displayName\": \"Value\",\n                \"name\": \"value\",\n                \"type\": \"string\",\n                \"default\": \"\"\n              }\n            ]\n          }\n        ]\n      }\n    ]\n  }\n]",
    "operations": "[]",
    "credentials_required": "[\n  {\n    \"name\": \"zepApi\",\n    \"required\": true\n  }\n]",
    "updated_at": "2025-07-06 13:14:51"
  }
]