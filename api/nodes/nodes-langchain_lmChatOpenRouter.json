{
  "node_type": "nodes-langchain.lmChatOpenRouter",
  "display_name": "OpenRouter Chat Model",
  "description": "For advanced usage with an AI chain",
  "category": "transform",
  "development_style": "programmatic",
  "package_name": "@n8n/n8n-nodes-langchain",
  "version": "1",
  "is_ai_tool": false,
  "is_trigger": false,
  "is_webhook": false,
  "is_versioned": true,
  "has_documentation": true,
  "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: OpenRouter Chat Model node documentation\ndescription: Learn how to use the OpenRouter Chat Model node in n8n. Follow technical documentation to integrate OpenRouter Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# OpenRouter Chat Model node\n\nUse the OpenRouter Chat Model node to use OpenRouter's chat models with conversational agents.\n\nOn this page, you'll find the node parameters for the OpenRouter Chat Model node and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/openrouter.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n### Model\n\nSelect the model to use to generate the completion.\n\nn8n dynamically loads models from OpenRouter and you'll only see the models available to your account.\n\n## Node options\n\nUse these options to further refine the node's behavior.\n\n### Frequency Penalty\n\nUse this option to control the chances of the model repeating itself. Higher values reduce the chance of the model repeating itself.\n\n### Maximum Number of Tokens\n\nEnter the maximum number of tokens used, which sets the completion length.\n\n### Response Format\n\nChoose **Text** or **JSON**. **JSON** ensures the model returns valid JSON.\n\n### Presence Penalty\n\nUse this option to control the chances of the model talking about new topics. Higher values increase the chance of the model talking about new topics.\n\n### Sampling Temperature\n\nUse this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n\n### Timeout\n\nEnter the maximum request time in milliseconds.\n\n### Max Retries\n\nEnter the maximum number of times to retry a request.\n\n### Top P\n\nUse this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'openrouter-chat-model') ]]\n\n## Related resources\n\nAs OpenRouter is API-compatible with OpenAI, you can refer to [LangChains's OpenAI documentation](https://js.langchain.com/docs/integrations/chat/openai/) for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n--8<-- \"_glossary/ai-glossary.md\"\n",
  "properties_schema": [
    {
      "displayName": "This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>",
      "name": "notice",
      "type": "notice",
      "default": "",
      "typeOptions": {
        "containerClass": "ndv-connection-hint-notice"
      }
    },
    {
      "displayName": "If using JSON response format, you must include word \"json\" in the prompt in your chain or agent. Also, make sure to select latest models released post November 2023.",
      "name": "notice",
      "type": "notice",
      "default": "",
      "displayOptions": {
        "show": {
          "/options.responseFormat": [
            "json_object"
          ]
        }
      }
    },
    {
      "displayName": "Model",
      "name": "model",
      "type": "options",
      "default": "openai/gpt-4.1-mini",
      "description": "The model which will generate the completion. <a href=\"https://openrouter.ai/docs/models\">Learn more</a>.",
      "typeOptions": {
        "loadOptions": {
          "routing": {
            "request": {
              "method": "GET",
              "url": "/models"
            },
            "output": {
              "postReceive": [
                {
                  "type": "rootProperty",
                  "properties": {
                    "property": "data"
                  }
                },
                {
                  "type": "setKeyValue",
                  "properties": {
                    "name": "={{$responseItem.id}}",
                    "value": "={{$responseItem.id}}"
                  }
                },
                {
                  "type": "sort",
                  "properties": {
                    "key": "name"
                  }
                }
              ]
            }
          }
        }
      }
    },
    {
      "displayName": "Options",
      "name": "options",
      "type": "collection",
      "default": {},
      "description": "Additional options to add",
      "options": [
        {
          "displayName": "Frequency Penalty",
          "name": "frequencyPenalty",
          "default": 0,
          "typeOptions": {
            "maxValue": 2,
            "minValue": -2,
            "numberPrecision": 1
          },
          "description": "Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim",
          "type": "number"
        },
        {
          "displayName": "Maximum Number of Tokens",
          "name": "maxTokens",
          "default": -1,
          "description": "The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 32,768).",
          "type": "number",
          "typeOptions": {
            "maxValue": 32768
          }
        },
        {
          "displayName": "Response Format",
          "name": "responseFormat",
          "default": "text",
          "type": "options",
          "options": [
            {
              "name": "Text",
              "value": "text",
              "description": "Regular text response"
            },
            {
              "name": "JSON",
              "value": "json_object",
              "description": "Enables JSON mode, which should guarantee the message the model generates is valid JSON"
            }
          ]
        },
        {
          "displayName": "Presence Penalty",
          "name": "presencePenalty",
          "default": 0,
          "typeOptions": {
            "maxValue": 2,
            "minValue": -2,
            "numberPrecision": 1
          },
          "description": "Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics",
          "type": "number"
        },
        {
          "displayName": "Sampling Temperature",
          "name": "temperature",
          "default": 0.7,
          "typeOptions": {
            "maxValue": 2,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive.",
          "type": "number"
        },
        {
          "displayName": "Timeout",
          "name": "timeout",
          "default": 360000,
          "description": "Maximum amount of time a request is allowed to take in milliseconds",
          "type": "number"
        },
        {
          "displayName": "Max Retries",
          "name": "maxRetries",
          "default": 2,
          "description": "Maximum number of retries to attempt",
          "type": "number"
        },
        {
          "displayName": "Top P",
          "name": "topP",
          "default": 1,
          "typeOptions": {
            "maxValue": 1,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered. We generally recommend altering this or temperature but not both.",
          "type": "number"
        }
      ]
    }
  ],
  "operations": [],
  "credentials_required": [
    {
      "name": "openRouterApi",
      "required": true
    }
  ],
  "generated_at": "2025-07-23T02:24:22.265Z",
  "api_version": "1.0.0"
}