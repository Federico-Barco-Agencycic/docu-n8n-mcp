{
  "node_type": "nodes-langchain.lmChatOllama",
  "display_name": "Ollama Chat Model",
  "description": "Language Model Ollama",
  "category": "transform",
  "development_style": "programmatic",
  "package_name": "@n8n/n8n-nodes-langchain",
  "version": "1",
  "is_ai_tool": false,
  "is_trigger": false,
  "is_webhook": false,
  "is_versioned": false,
  "has_documentation": true,
  "documentation": "---\n#https://www.notion.so/n8n/Frontmatter-432c2b8dff1f43d4b1c8d20075510fe4\ntitle: Ollama Chat Model node documentation\ndescription: Learn how to use the Ollama Chat Model node in n8n. Follow technical documentation to integrate Ollama Chat Model node into your workflows.\ncontentType: [integration, reference]\npriority: high\n---\n\n# Ollama Chat Model node\n\nThe Ollama Chat Model node allows you use local Llama 2 models with conversational [agents](/glossary.md#ai-agent).\n\nOn this page, you'll find the node parameters for the Ollama Chat Model node, and links to more resources.\n\n/// note | Credentials\nYou can find authentication information for this node [here](/integrations/builtin/credentials/ollama.md).\n///\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/sub-node-expression-resolution.md\"\n\n## Node parameters\n\n* **Model**: Select the model that generates the completion. Choose from:\n\t* **Llama2**\n\t* **Llama2 13B**\n\t* **Llama2 70B**\n\t* **Llama2 Uncensored**\n\nRefer to the Ollama [Models Library documentation](https://ollama.com/library){:target=_blank .external-link} for more information about available models.\n\n## Node options\n\n* **Sampling Temperature**: Use this option to control the randomness of the sampling process. A higher temperature creates more diverse sampling, but increases the risk of hallucinations.\n* **Top K**: Enter the number of token choices the model uses to generate the next token.\n* **Top P**: Use this option to set the probability the completion should use. Use a lower value to ignore less probable options. \n\n## Templates and examples\n\n<!-- see https://www.notion.so/n8n/Pull-in-templates-for-the-integrations-pages-37c716837b804d30a33b47475f6e3780 -->\n[[ templatesWidget(page.title, 'ollama-chat-model') ]]\n\n## Related resources\n\nRefer to [LangChains's Ollama Chat Model documentation](https://js.langchain.com/docs/integrations/chat/ollama/){:target=_blank .external-link} for more information about the service.\n\n--8<-- \"_snippets/integrations/builtin/cluster-nodes/langchain-overview-link.md\"\n\n## Common issues\n\nFor common questions or issues and suggested solutions, refer to [Common issues](/integrations/builtin/cluster-nodes/sub-nodes/n8n-nodes-langchain.lmchatollama/common-issues.md).\n\n--8<-- \"_glossary/ai-glossary.md\"\n\n--8<-- \"_snippets/self-hosting/starter-kits/self-hosted-ai-starter-kit.md\"\n",
  "properties_schema": [
    {
      "displayName": "This node must be connected to an AI chain. <a data-action='openSelectiveNodeCreator' data-action-parameter-creatorview='AI'>Insert one</a>",
      "name": "notice",
      "type": "notice",
      "default": "",
      "typeOptions": {
        "containerClass": "ndv-connection-hint-notice"
      }
    },
    {
      "displayName": "Model",
      "name": "model",
      "type": "options",
      "default": "llama3.2",
      "description": "The model which will generate the completion. To download models, visit <a href=\"https://ollama.ai/library\">Ollama Models Library</a>.",
      "required": true,
      "typeOptions": {
        "loadOptions": {
          "routing": {
            "request": {
              "method": "GET",
              "url": "/api/tags"
            },
            "output": {
              "postReceive": [
                {
                  "type": "rootProperty",
                  "properties": {
                    "property": "models"
                  }
                },
                {
                  "type": "setKeyValue",
                  "properties": {
                    "name": "={{$responseItem.name}}",
                    "value": "={{$responseItem.name}}"
                  }
                },
                {
                  "type": "sort",
                  "properties": {
                    "key": "name"
                  }
                }
              ]
            }
          }
        }
      }
    },
    {
      "displayName": "Options",
      "name": "options",
      "type": "collection",
      "default": {},
      "description": "Additional options to add",
      "options": [
        {
          "displayName": "Sampling Temperature",
          "name": "temperature",
          "default": 0.7,
          "typeOptions": {
            "maxValue": 1,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Controls the randomness of the generated text. Lower values make the output more focused and deterministic, while higher values make it more diverse and random.",
          "type": "number"
        },
        {
          "displayName": "Top K",
          "name": "topK",
          "default": -1,
          "typeOptions": {
            "maxValue": 100,
            "minValue": -1,
            "numberPrecision": 1
          },
          "description": "Limits the number of highest probability vocabulary tokens to consider at each step. A higher value increases diversity but may reduce coherence. Set to -1 to disable.",
          "type": "number"
        },
        {
          "displayName": "Top P",
          "name": "topP",
          "default": 1,
          "typeOptions": {
            "maxValue": 1,
            "minValue": 0,
            "numberPrecision": 1
          },
          "description": "Chooses from the smallest possible set of tokens whose cumulative probability exceeds the probability top_p. Helps generate more human-like text by reducing repetitions.",
          "type": "number"
        },
        {
          "displayName": "Frequency Penalty",
          "name": "frequencyPenalty",
          "type": "number",
          "default": 0,
          "typeOptions": {
            "minValue": 0
          },
          "description": "Adjusts the penalty for tokens that have already appeared in the generated text. Higher values discourage repetition."
        },
        {
          "displayName": "Keep Alive",
          "name": "keepAlive",
          "type": "string",
          "default": "5m",
          "description": "Specifies the duration to keep the loaded model in memory after use. Useful for frequently used models. Format: 1h30m (1 hour 30 minutes)."
        },
        {
          "displayName": "Low VRAM Mode",
          "name": "lowVram",
          "type": "boolean",
          "default": false,
          "description": "Whether to Activate low VRAM mode, which reduces memory usage at the cost of slower generation speed. Useful for GPUs with limited memory."
        },
        {
          "displayName": "Main GPU ID",
          "name": "mainGpu",
          "type": "number",
          "default": 0,
          "description": "Specifies the ID of the GPU to use for the main computation. Only change this if you have multiple GPUs."
        },
        {
          "displayName": "Context Batch Size",
          "name": "numBatch",
          "type": "number",
          "default": 512,
          "description": "Sets the batch size for prompt processing. Larger batch sizes may improve generation speed but increase memory usage."
        },
        {
          "displayName": "Context Length",
          "name": "numCtx",
          "type": "number",
          "default": 2048,
          "description": "The maximum number of tokens to use as context for generating the next token. Smaller values reduce memory usage, while larger values provide more context to the model."
        },
        {
          "displayName": "Number of GPUs",
          "name": "numGpu",
          "type": "number",
          "default": -1,
          "description": "Specifies the number of GPUs to use for parallel processing. Set to -1 for auto-detection."
        },
        {
          "displayName": "Max Tokens to Generate",
          "name": "numPredict",
          "type": "number",
          "default": -1,
          "description": "The maximum number of tokens to generate. Set to -1 for no limit. Be cautious when setting this to a large value, as it can lead to very long outputs."
        },
        {
          "displayName": "Number of CPU Threads",
          "name": "numThread",
          "type": "number",
          "default": 0,
          "description": "Specifies the number of CPU threads to use for processing. Set to 0 for auto-detection."
        },
        {
          "displayName": "Penalize Newlines",
          "name": "penalizeNewline",
          "type": "boolean",
          "default": true,
          "description": "Whether the model will be less likely to generate newline characters, encouraging longer continuous sequences of text"
        },
        {
          "displayName": "Presence Penalty",
          "name": "presencePenalty",
          "type": "number",
          "default": 0,
          "description": "Adjusts the penalty for tokens based on their presence in the generated text so far. Positive values penalize tokens that have already appeared, encouraging diversity."
        },
        {
          "displayName": "Repetition Penalty",
          "name": "repeatPenalty",
          "type": "number",
          "default": 1,
          "description": "Adjusts the penalty factor for repeated tokens. Higher values more strongly discourage repetition. Set to 1.0 to disable repetition penalty."
        },
        {
          "displayName": "Use Memory Locking",
          "name": "useMLock",
          "type": "boolean",
          "default": false,
          "description": "Whether to lock the model in memory to prevent swapping. This can improve performance but requires sufficient available memory."
        },
        {
          "displayName": "Use Memory Mapping",
          "name": "useMMap",
          "type": "boolean",
          "default": true,
          "description": "Whether to use memory mapping for loading the model. This can reduce memory usage but may impact performance. Recommended to keep enabled."
        },
        {
          "displayName": "Load Vocabulary Only",
          "name": "vocabOnly",
          "type": "boolean",
          "default": false,
          "description": "Whether to only load the model vocabulary without the weights. Useful for quickly testing tokenization."
        },
        {
          "displayName": "Output Format",
          "name": "format",
          "type": "options",
          "options": [
            {
              "name": "Default",
              "value": "default"
            },
            {
              "name": "JSON",
              "value": "json"
            }
          ],
          "default": "default",
          "description": "Specifies the format of the API response"
        }
      ]
    }
  ],
  "operations": [],
  "credentials_required": [
    {
      "name": "ollamaApi",
      "required": true
    }
  ],
  "generated_at": "2025-07-23T02:24:22.264Z",
  "api_version": "1.0.0"
}